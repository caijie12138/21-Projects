{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于Seq2Seq生成对联"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#处理数据\n",
    "source = open('source.txt','w')\n",
    "target = open('target.txt','w')\n",
    "\n",
    "with open('对联.txt','r') as f:\n",
    "    for line in f.readlines():\n",
    "        up_down = line.strip().split(' ')\n",
    "        source.write(up_down[0]+'\\n')\n",
    "        target.write(up_down[1]+'\\n')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('source.txt','r') as f:\n",
    "    source_data = f.read()\n",
    "with open('target.txt','r') as f:\n",
    "    target_data = f.read()\n",
    "    \n",
    "#将上下联分开\n",
    "source = source_data.split('\\n')\n",
    "target = target_data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq2seq模型中不能输入字符，输入的应该是数字 就是使用数字表示文本\n",
    "#首先建立字符到数字和数字到字符的字典\n",
    "def extract_character_vocab(data):\n",
    "    '''\n",
    "    参数:data\n",
    "    返回:voc_int int_voc\n",
    "    '''\n",
    "    special_words = ['<PAD>','<UNK>','<GO>','<EOS>']\n",
    "    #<PAD>用于字符补全，'<UNK>','<GO>'用于Decoder端序列中<UNK>代替一些未出现的词或者低频词\n",
    "    set_words = list(set([character for line in data for character in line]))\n",
    "    int_to_voc = {idx:value for idx,value in enumerate(special_words+set_words)}\n",
    "    voc_to_int = {word:idx for idx,word in int_to_voc.items()}\n",
    "    return voc_to_int,int_to_voc\n",
    "\n",
    "source_letter_to_int,source_int_to_letter = extract_character_vocab(source+target)\n",
    "target_letter_to_int,target_int_to_letter = extract_character_vocab(source+target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dict的get方法定义了两个参数（a,b） 如果dict中存在key-a则返回dict[a]否则返回b\n",
    "#将字符用数字表示\n",
    "source_int = [[source_letter_to_int.get(letter,source_letter_to_int['<UNK>']) for letter in line]for line in source]\n",
    "\n",
    "target_int = [[target_letter_to_int.get(letter,target_letter_to_int['<UNK>']) for letter in line]for line in target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 按步骤建立Encoder和Decoder模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1、首先建立encoder层\n",
    "def get_encoder_layer(input_data,rnn_size,num_layers,source_sequence_length,source_vocab_size,encoding_embedding_size):\n",
    "    \n",
    "    #tf.contrib.layers.embed_sequence 将文章的的每一个字使用embedding表示\n",
    "    encoder_embed_input = tf.contrib.layers.embed_sequence(input_data,source_voc_size,embed_dim)\n",
    "    \n",
    "    def get_LSTMCell(rnn_size):\n",
    "        return tf.contrib.rnn.BasicLSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([get_LSTMCell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    encoder_output,encoder_state = tf.nn.dynamic_rnn(cell,encoder_embed_input,sequence_length=source_sequence_length,dtype = tf.float32)\n",
    "    \n",
    "    return encoder_output,encoder_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2、其次建立decoder层\n",
    "#decoder层包含两个阶段 training和predict两个阶段共享参数\n",
    "#需要在target句子前加一个<go>表示句子的开头，还需要将target中的最后一个字符去掉\n",
    "def process_decoder_input(data,voc_to_int,batch_size):\n",
    "    \n",
    "    ending = tf.strided_slice(data,[0,0],[batch_size,-1],[1,1])#将target中的最后一个字符去掉\n",
    "    #fill参数表示的（形状,填充的数字）\n",
    "    decoder_input = tf.concat([tf.fill([batch_size,1],voc_to_int['<GO>']),ending],1)#target句子前加一个<go>表示句子的开头\n",
    "    \n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_decoder_layer(target_letter_to_int,decoding_embedding_size,num_layers,rnn_size,\n",
    "                   target_sequence_length,max_target_sequence_length,encoder_state,decoder_input):\n",
    "    #1、embedding\n",
    "    target_vocab_size = len(target_letter_to_int)\n",
    "    decoder_embeddings = tf.Variable(tf.random_uniform([target_vocab_size,decoder_embedding_size]))\n",
    "    decoder_embed_input = tf.nn.embedding_lookup(decoder_embeddings,decoder_input)\n",
    "    #2、构造Decoder中的RNNCell单元\n",
    "    def get_decode_cell(rnn_size):\n",
    "        return tf.contrib.rnn.BasicLSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([get_decoder_layer(rnn_size) for _ in range(num_layers)])\n",
    "    #3、Output全连接层\n",
    "    output_layer = Dense(target_vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.1,stddev=0.1))\n",
    "    #4、Training decoder\n",
    "    with tf.Variable_scope('decode'):\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embed_input,\n",
    "                                                            sequence_length = target_sequence_length,\n",
    "                                                           time_major = False)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(cell,training_helper,encoder_state,output_layer)\n",
    "        training_decoder_output,_,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,impute_finished=True,\n",
    "                                                                        maximum_iterations=max_target_sequence_length)\n",
    "    #5、Predicting decoder\n",
    "    #与前一个decoder共享参数\n",
    "    with tf.Variable_scope('decode',reuse = True):\n",
    "        start_token = tf.tile(tf.constant([target_letter_to_int['GO']],dtype = tf.int32),[batch_size],name='start_token')\n",
    "        predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings,start_tokens,\n",
    "                                                                     target_letter_to_int['<EOS>'])\n",
    "        predicting_decoder = tf.contrib.seq2seq.BasicDecoder(cell,predicting_helper,encoder_state,output_layer)\n",
    "        predicting_decoder_output,_,_ = tf.contrib.seq2seq.dynamic_decode(pre,impute_finished=True,\n",
    "                                                                          maximum_iterations=max_target_sequence_length)\n",
    "        return training_decoder_output,predicting_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#上面已经把Encoder和Decoder建立完毕，之后使用seq2seq模型将其链接起来\n",
    "def seq2seq_model(input_data,targets,lr,target_sequence_length,\n",
    "                  max_target_sequence_length,source_sequence_length,\n",
    "                  source_vocab_size,target_vocab_size,encoder_embedding_size,decoder_embedding_size,rnn_size,num_layers):\n",
    "    _,encoder_state = get_encoder_layer(input_data,\n",
    "                                        rnn_size,\n",
    "                                        num_layers,\n",
    "                                        source_sequence_length,\n",
    "                                        source_vocab_size,\n",
    "                                        encoding_embedding_size)\n",
    "    decoder_input = process_decoder_input(target,target_letter_to_int,batch_size)\n",
    "    training_decoder_output,predicting_decoder_output = get_decoder_layer(target_letter_to_int,decoding_embedding_size,\n",
    "                                                                          num_layers,rnn_size,\n",
    "                   target_sequence_length,max_target_sequence_length,encoder_state,decoder_input)\n",
    "    return training_decoder_output,predicting_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义输入数据函数\n",
    "def get_inputs():\n",
    "    inputs = tf.placehloder(tf.int32,[None,None],name = 'inputs')\n",
    "    targets = tf.placeholder(tf.int32,[None,None],name = 'targets')\n",
    "    learning_rate = tf.placeholder(tf.float32,name = 'learning_rate')\n",
    "    \n",
    "    target_sequence_length = tf.placeholder(tf.int32,(None,),name = 'target_sequence_length')\n",
    "    max_target_sequence_length = tf.reduce_max(target_sequence_length,name='max_target_len')\n",
    "    source_sequence_length = tf.placeholder(tf.int32,(None,),name = 'source_sequence_length')\n",
    "    return inputs,targets,learning_rate,target_sequence_length,max_target_sequence_length,source_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义loss和optimizer\n",
    "train_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    inputs,targets,lr,target_sequence_length,max_target_sequence_length,source_sequence_length = get_inputs()\n",
    "    training_decoder_output,predicting_decoder_output = seq2seq_model(  inputs,\n",
    "                                                                        targets,\n",
    "                                                                        lr,\n",
    "                                                                        target_sequence_length,\n",
    "                                                                        max_target_sequence_length,\n",
    "                                                                        source_sequence_length,\n",
    "                                                                        len(source_letter_to_int),\n",
    "                                                                        len(target_letter_to_int),\n",
    "                                                                        encoding_embedding_size,\n",
    "                                                                        decoding_embedding_size,\n",
    "                                                                        rnn_size,\n",
    "                                                                        num_layers)\n",
    "    train_logits = tf.identity(training_decoder_output,rnn_output,'logits')\n",
    "    predict_logits = tf.identity(predicting_decoder_output.sample_id,'predictions')\n",
    "    masks = tf.sequence_mask(target_sequence_length,max_target_sequence_length,dtype = tf.float32,name='masks')\n",
    "    with tf.name_scope('optimization'):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
