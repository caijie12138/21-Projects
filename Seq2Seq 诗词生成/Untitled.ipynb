{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于Seq2Seq生成对联"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 超参数\n",
    "# Number of Epochs\n",
    "epochs = 3000\n",
    "# Batch Size\n",
    "batch_size = 50\n",
    "# RNN Size\n",
    "rnn_size = 50\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 15\n",
    "decoding_embedding_size = 15\n",
    "# Learning Rate\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#处理数据\n",
    "source = open('source.txt','w')\n",
    "target = open('target.txt','w')\n",
    "\n",
    "with open('对联.txt','r') as f:\n",
    "    for line in f.readlines():\n",
    "        up_down = line.strip().split(' ')\n",
    "        source.write(up_down[0]+'\\n')\n",
    "        target.write(up_down[1]+'\\n')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: ResourceWarning: unclosed file <_io.TextIOWrapper name='source.txt' mode='w' encoding='UTF-8'>\n",
      "  import sys\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: ResourceWarning: unclosed file <_io.TextIOWrapper name='target.txt' mode='w' encoding='UTF-8'>\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "with open('source.txt','r') as f:\n",
    "    source_data = f.read()\n",
    "with open('target.txt','r') as f:\n",
    "    target_data = f.read()\n",
    "    \n",
    "#将上下联分开\n",
    "source = source_data.split('\\n')\n",
    "target = target_data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seq2seq模型中不能输入字符，输入的应该是数字 就是使用数字表示文本\n",
    "#首先建立字符到数字和数字到字符的字典\n",
    "def extract_character_vocab(data):\n",
    "    '''\n",
    "    参数:data\n",
    "    返回:voc_int int_voc\n",
    "    '''\n",
    "    special_words = ['<PAD>','<UNK>','<GO>','<EOS>']\n",
    "    #<PAD>用于字符补全，'<UNK>','<GO>'用于Decoder端序列中<UNK>代替一些未出现的词或者低频词\n",
    "    set_words = list(set([character for line in data for character in line]))\n",
    "    int_to_voc = {idx:value for idx,value in enumerate(special_words+set_words)}\n",
    "    voc_to_int = {word:idx for idx,word in int_to_voc.items()}\n",
    "    return voc_to_int,int_to_voc\n",
    "\n",
    "source_letter_to_int,source_int_to_letter = extract_character_vocab(source+target)\n",
    "target_letter_to_int,target_int_to_letter = extract_character_vocab(source+target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dict的get方法定义了两个参数（a,b） 如果dict中存在key-a则返回dict[a]否则返回b\n",
    "#将字符用数字表示\n",
    "source_int = [[source_letter_to_int.get(letter,source_letter_to_int['<UNK>']) for letter in line]for line in source]\n",
    "\n",
    "target_int = [[target_letter_to_int.get(letter,target_letter_to_int['<UNK>']) for letter in line]for line in target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 按步骤建立Encoder和Decoder模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1、首先建立encoder层\n",
    "def get_encoder_layer(input_data,rnn_size,num_layers,source_sequence_length,source_voc_size,encoding_embedding_size):\n",
    "    \n",
    "    #tf.contrib.layers.embed_sequence 将文章的的每一个字使用embedding表示\n",
    "    encoder_embed_input = tf.contrib.layers.embed_sequence(input_data,source_voc_size,encoding_embedding_size)\n",
    "    \n",
    "    def get_LSTMCell(rnn_size):\n",
    "        return tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([get_LSTMCell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    encoder_output,encoder_state = tf.nn.dynamic_rnn(cell,encoder_embed_input,sequence_length=source_sequence_length,dtype = tf.float32)\n",
    "    \n",
    "    return encoder_output,encoder_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2、其次建立decoder层\n",
    "#decoder层包含两个阶段 training和predict两个阶段共享参数\n",
    "#需要在target句子前加一个<go>表示句子的开头，还需要将target中的最后一个字符去掉\n",
    "def process_decoder_input(data,voc_to_int,batch_size):\n",
    "    \n",
    "    ending = tf.strided_slice(data,[0,0],[batch_size,-1],[1,1])#将target中的最后一个字符去掉\n",
    "    #fill参数表示的（形状,填充的数字）\n",
    "    decoder_input = tf.concat([tf.fill([batch_size,1],voc_to_int['<GO>']),ending],1)#target句子前加一个<go>表示句子的开头\n",
    "    \n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_decoder_layer(target_letter_to_int,decoding_embedding_size,num_layers,rnn_size,\n",
    "                   target_sequence_length,max_target_sequence_length,encoder_state,decoder_input):\n",
    "    #1、embedding\n",
    "    target_vocab_size = len(target_letter_to_int)\n",
    "    decoder_embeddings = tf.Variable(tf.random_uniform([target_vocab_size,decoding_embedding_size]))\n",
    "    decoder_embed_input = tf.nn.embedding_lookup(decoder_embeddings,decoder_input)\n",
    "    #2、构造Decoder中的RNNCell单元\n",
    "    def get_decode_cell(rnn_size):\n",
    "        return tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([get_decode_cell(rnn_size) for _ in range(num_layers)])\n",
    "    #3、Output全连接层\n",
    "    output_layer = Dense(target_vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.1,stddev=0.1))\n",
    "    #4、Training decoder\n",
    "    with tf.variable_scope('decode'):\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embed_input,\n",
    "                                                            sequence_length = target_sequence_length,\n",
    "                                                           time_major = False)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(cell,training_helper,encoder_state,output_layer)\n",
    "        training_decoder_output,_,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,impute_finished=True,\n",
    "                                                                        maximum_iterations=max_target_sequence_length)\n",
    "    #5、Predicting decoder\n",
    "    #与前一个decoder共享参数\n",
    "    with tf.variable_scope('decode',reuse = True):\n",
    "        start_tokens = tf.tile(tf.constant([target_letter_to_int['<GO>']],dtype = tf.int32),[batch_size],name='start_token')\n",
    "        predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings,start_tokens,\n",
    "                                                                     target_letter_to_int['<EOS>'])\n",
    "        predicting_decoder = tf.contrib.seq2seq.BasicDecoder(cell,predicting_helper,encoder_state,output_layer)\n",
    "        predicting_decoder_output,_,_ = tf.contrib.seq2seq.dynamic_decode(predicting_decoder,impute_finished=True,\n",
    "                                                                          maximum_iterations=max_target_sequence_length)\n",
    "        return training_decoder_output,predicting_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#上面已经把Encoder和Decoder建立完毕，之后使用seq2seq模型将其链接起来\n",
    "def seq2seq_model(input_data,targets,lr,target_sequence_length,\n",
    "                  max_target_sequence_length,source_sequence_length,\n",
    "                  source_voc_size,target_vocab_size,encoder_embedding_size,decoder_embedding_size,rnn_size,num_layers):\n",
    "    _,encoder_state = get_encoder_layer(input_data,\n",
    "                                        rnn_size,\n",
    "                                        num_layers,\n",
    "                                        source_sequence_length,\n",
    "                                        source_voc_size,\n",
    "                                        encoding_embedding_size)\n",
    "    decoder_input = process_decoder_input(targets,target_letter_to_int,batch_size)\n",
    "    training_decoder_output,predicting_decoder_output = get_decoder_layer(target_letter_to_int,decoding_embedding_size,\n",
    "                                                                          num_layers,rnn_size,\n",
    "                   target_sequence_length,max_target_sequence_length,encoder_state,decoder_input)\n",
    "    return training_decoder_output,predicting_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义输入数据函数\n",
    "def get_inputs():\n",
    "    inputs = tf.placeholder(tf.int32,[None,None],name = 'inputs')\n",
    "    targets = tf.placeholder(tf.int32,[None,None],name = 'targets')\n",
    "    learning_rate = tf.placeholder(tf.float32,name = 'learning_rate')\n",
    "    \n",
    "    target_sequence_length = tf.placeholder(tf.int32,(None,),name = 'target_sequence_length')\n",
    "    max_target_sequence_length = tf.reduce_max(target_sequence_length,name='max_target_len')\n",
    "    source_sequence_length = tf.placeholder(tf.int32,(None,),name = 'source_sequence_length')\n",
    "    return inputs,targets,learning_rate,target_sequence_length,max_target_sequence_length,source_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    }
   ],
   "source": [
    "#定义loss和optimizer\n",
    "train_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    inputs,targets,lr,target_sequence_length,max_target_sequence_length,source_sequence_length = get_inputs()\n",
    "    training_decoder_output,predicting_decoder_output = seq2seq_model(  inputs,\n",
    "                                                                        targets,\n",
    "                                                                        lr,\n",
    "                                                                        target_sequence_length,\n",
    "                                                                        max_target_sequence_length,\n",
    "                                                                        source_sequence_length,\n",
    "                                                                        len(source_letter_to_int),\n",
    "                                                                        len(target_letter_to_int),\n",
    "                                                                        encoding_embedding_size,\n",
    "                                                                        decoding_embedding_size,\n",
    "                                                                        rnn_size,\n",
    "                                                                        num_layers)\n",
    "    train_logits = tf.identity(training_decoder_output.rnn_output,'logits')\n",
    "    predict_logits = tf.identity(predicting_decoder_output.sample_id,'predictions')\n",
    "    masks = tf.sequence_mask(target_sequence_length,max_target_sequence_length,dtype = tf.float32,name='masks')\n",
    "    with tf.name_scope('optimization'):\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "        train_logits,\n",
    "        targets,    \n",
    "        masks)\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch,pad_int):\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence+[pad_int] * (max_sentence-len(sentence))for sentence in sentence_batch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(targets,source,batch_size,source_pad_int,target_pad_int):\n",
    "    for batch_i in range(0,len(source)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        source_batch = sources[start_i:start_i + batch_size]\n",
    "        target_batch = sources[start_i:start_i + batch_size]\n",
    "        \n",
    "        pad_source_batch = pad_sentence_batch(source_batch,source_pad_int)\n",
    "        pad_target_batch = pad_sentence_batch(target_batch,source_pad_int)\n",
    "        \n",
    "        targets_length = []\n",
    "        for target in target_batch:\n",
    "            targets_length.append(len(target))\n",
    "        source_length = []\n",
    "        for source in source_length:\n",
    "            source_length.append(len(source))\n",
    "        yield pad_target_batch,pad_source_batch,targets_length,source_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-d58056695346>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m (valid_targets_batch, valid_sources_batch, valid_targets_lengths, valid_sources_lengths) = next(get_batches(valid_target, valid_source, batch_size,\n\u001b[1;32m     10\u001b[0m                            \u001b[0msource_letter_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<PAD>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                            target_letter_to_int['<PAD>']))\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "train_source = source_int[batch_size:]\n",
    "train_target = target_int[batch_size:]\n",
    "\n",
    "# 留出一个batch进行验证\n",
    "valid_source = source_int[:batch_size]\n",
    "valid_target = target_int[:batch_size]\n",
    "\n",
    "(valid_targets_batch, valid_sources_batch, valid_targets_lengths, valid_sources_lengths) = next(get_batches(valid_target, valid_source, batch_size,\n",
    "                           source_letter_to_int['<PAD>'],\n",
    "                           target_letter_to_int['<PAD>']))\n",
    "\n",
    "display_step = 50\n",
    "\n",
    "checkpoint = \"data/trained_model.ckpt\"\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print()\n",
    "    for epoch_i in range(1,epochs+1):\n",
    "        for batch_i,(targets_batch, sources_batch, targets_lengths, sources_lengths) in enumerate(get_batches(\n",
    "            train_target,train_source,batch_size,source_letter_to_int['<PAD>'],\n",
    "                           target_letter_to_int['<PAD>']\n",
    "        )):\n",
    "            _,loss = sess.run([train_op,cost],feed_dict={\n",
    "                input_data:sources_batch,\n",
    "                targets:targets_batch,\n",
    "                lr:learning_rate,\n",
    "                target_sequence_length:targets_lengths,\n",
    "                source_sequence_length:sources_lengths\n",
    "            })\n",
    "\n",
    "            if batch_i % display_step == 0:\n",
    "                # 计算validation loss\n",
    "                validation_loss = sess.run(\n",
    "                    [cost],\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     targets: valid_targets_batch,\n",
    "                     lr: learning_rate,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     source_sequence_length: valid_sources_lengths})\n",
    "\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Training Loss: {:>6.3f}  - Validation loss: {:>6.3f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs,\n",
    "                              batch_i,\n",
    "                              len(train_source) // batch_size,\n",
    "                              loss,\n",
    "                              validation_loss[0]))\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, checkpoint)\n",
    "    print('Model Trained and Saved')\n",
    "\n",
    "\n",
    "# 预测\n",
    "def source_to_seq(text):\n",
    "    sequence_length = 7\n",
    "    return [source_letter_to_int.get(word,source_letter_to_int['<UNK>']) for word in text] + [source_letter_to_int['<PAD>']] * (sequence_length - len(text))\n",
    "\n",
    "\n",
    "input_word = '戌岁祝福万事顺'\n",
    "text = source_to_seq(input_word)\n",
    "\n",
    "checkpoint = \"data/trained_model.ckpt\"\n",
    "loaded_graph = tf.Graph()\n",
    "\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph(checkpoint+'.meta')\n",
    "    loader.restore(sess,checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "\n",
    "    answer_logits = sess.run(logits, {input_data: [text] * batch_size,\n",
    "                                      target_sequence_length: [len(input_word)] * batch_size,\n",
    "                                      source_sequence_length: [len(input_word)] * batch_size})[0]\n",
    "\n",
    "    pad = source_letter_to_int[\"<PAD>\"]\n",
    "\n",
    "    print('原始输入:', input_word)\n",
    "\n",
    "    print('\\nSource')\n",
    "    print('  Word 编号:    {}'.format([i for i in text]))\n",
    "    print('  Input Words: {}'.format(\" \".join([source_int_to_letter[i] for i in text])))\n",
    "\n",
    "    print('\\nTarget')\n",
    "    print('  Word 编号:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "    print('  Response Words: {}'.format(\" \".join([target_int_to_letter[i] for i in answer_logits if i != pad])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['承上下求索志', '除旧岁破旧俗', '处处春光济美', '处处欢歌遍地', '处处明山秀水', '窗外红梅最艳', '创造万千气象', '春到碧桃树上', '为江山添秀色', '爆竹一声除旧']\n",
      "['绘春秋振兴图', '迎新年树新风', '年年人物风流', '家家喜笑连天', '家家笑语欢歌', '心头美景尤佳', '建设两个文明', '莺歌绿柳楼前', '与日月争光辉', '桃符万户更新']\n",
      "[[279, 206, 521, 263, 580, 22], [21, 408, 43, 161, 408, 462], [200, 200, 653, 455, 223, 597], [200, 200, 542, 726, 118, 398], [200, 200, 356, 490, 243, 723], [260, 51, 492, 300, 216, 187], [291, 464, 525, 33, 447, 673], [653, 561, 96, 459, 126, 206], [205, 475, 490, 25, 243, 306], [289, 456, 647, 457, 21, 408], [352, 435, 374, 158, 436, 644], [352, 435, 33, 695, 327, 436], [660, 224, 503, 653, 552, 600], [435, 176, 455, 639, 43, 505], [169, 159, 707, 452, 732, 176], [480, 467, 282, 314, 626, 27], [583, 705, 44, 66, 33, 43], [583, 69, 492, 300, 375, 625], [569, 669, 299, 337, 595, 187], [569, 669, 299, 337, 595, 187], [569, 669, 358, 653, 120, 187], [569, 123, 635, 653, 120, 187], [569, 478, 44, 726, 352, 435], [653, 561, 670, 446, 547, 106], [653, 314, 215, 118, 608, 558], [653, 314, 653, 224, 653, 669], [653, 314, 653, 224, 653, 306], [653, 314, 669, 337, 231, 539], [653, 455, 460, 79, 567, 398], [653, 463, 231, 539, 669, 337], [653, 230, 62, 62, 340, 514], [653, 696, 83, 79, 567, 398], [653, 444, 569, 669, 489, 264], [653, 121, 33, 490, 17, 338], [608, 466, 654, 653, 597, 160], [128, 58, 702, 608, 20, 111], [32, 244, 75, 605, 653, 573], [32, 291, 33, 695, 567, 436], [572, 677, 239, 548, 543, 286], [230, 230, 299, 34, 468, 240], [230, 520, 314, 678, 224, 622], [505, 429, 715, 213, 190, 547], [505, 356, 652, 559, 164, 306], [69, 219, 300, 669, 584, 187], [314, 376, 492, 383, 147, 131], [314, 160, 627, 343, 384, 470], [314, 598, 492, 383, 147, 131], [314, 671, 475, 490, 595, 243], [180, 147, 653, 669, 493, 264], [67, 425, 259, 383, 219, 230], [67, 7, 33, 695, 162, 352], [67, 7, 625, 32, 547, 700], [67, 7, 114, 114, 230, 206], [340, 84, 616, 356, 203, 207], [340, 360, 653, 494, 567, 398], [340, 625, 198, 494, 567, 398], [28, 419, 653, 455, 331, 40], [229, 379, 79, 545, 595, 243], [512, 21, 412, 274, 683, 447], [158, 585, 551, 234, 67, 425, 25, 625], [380, 285, 277, 388, 718, 253, 523, 420], [684, 339, 684, 77, 630, 630, 184, 370], [525, 396, 33, 492, 569, 669, 623, 264], [707, 645, 211, 568, 630, 630, 725, 527], [416, 180, 109, 374, 184, 590, 528, 334], [653, 520, 314, 250, 669, 337, 231, 539], [67, 7, 84, 562, 653, 314, 405, 696], [669, 337, 231, 539, 62, 99, 653, 175], [664, 54, 731, 680, 299, 140, 497, 714], [366, 577, 653, 306, 612, 726, 353, 632], [567, 398, 494, 653, 490, 302, 644, 573], [310, 556, 616, 321, 188, 665, 120, 534], [194, 312, 389, 596, 367, 378, 79, 696], [733, 362, 62, 250, 569, 728, 438, 179], [647, 570, 299, 56, 198, 653, 468, 361], [567, 398, 542, 85, 653, 494, 468, 696], [465, 230, 88, 487, 33, 61, 9, 352], [670, 446, 547, 106, 653, 314, 431, 520], [128, 232, 275, 656, 204, 143, 67, 425], [569, 525, 588, 656, 704, 658, 554, 100], [11, 547, 84, 562, 114, 114, 230, 206], [230, 520, 345, 577, 455, 117, 525, 106], [670, 446, 547, 106, 33, 144, 9, 243], [67, 163, 592, 178, 508, 109, 28, 657], [536, 240, 417, 280, 212, 386, 309, 79], [516, 437, 397, 669, 715, 32, 128, 341], [33, 106, 648, 698, 601, 145, 536, 659], [453, 245, 664, 54, 62, 374, 381, 306], [192, 504, 468, 294, 299, 696, 179, 657], [705, 354, 473, 76, 569, 436, 179, 278], [664, 54, 731, 680, 731, 680, 468, 10], [634, 120, 455, 356, 653, 377, 692, 619], [96, 379, 583, 238, 654, 653, 473, 131], [111, 387, 41, 238, 654, 481, 32, 165], [376, 272, 27, 294, 685, 492, 230, 520], [392, 22, 503, 209, 284, 18, 610, 409], [305, 379, 110, 16, 238, 386, 391, 447], [660, 224, 653, 314, 652, 238, 613, 471], [431, 648, 630, 653, 669, 124, 583, 69], [525, 673, 584, 621, 490, 91, 723, 243], [647, 570, 299, 56, 198, 653, 219, 230], [602, 249, 12, 716, 326, 314, 28, 448], [653, 79, 62, 546, 569, 669, 595, 187], [272, 27, 358, 653, 653, 299, 310, 318], [301, 408, 43, 427, 21, 408, 691, 155], [705, 598, 158, 585, 551, 234, 217, 439], [17, 338, 440, 132, 33, 506, 9, 151], [644, 22, 620, 487, 492, 180, 64, 265], [526, 210, 374, 44, 180, 729, 67, 7], [515, 319, 652, 348, 428, 57, 382, 290], [629, 64, 435, 606, 469, 209, 204, 143], [525, 673, 584, 621, 544, 345, 322, 705], [575, 307, 199, 561, 188, 227, 79, 696], [525, 396, 33, 492, 569, 669, 178, 187], [589, 329, 653, 608, 569, 669, 623, 264], [461, 76, 174, 347, 201, 697, 159, 323], [366, 577, 230, 505, 32, 653, 160], [315, 202, 410, 457, 197, 41, 360], [654, 724, 456, 52, 588, 687, 131], [148, 483, 468, 685, 368, 75, 226], [608, 522, 521, 295, 653, 659, 496], [308, 630, 680, 666, 62, 542, 310], [483, 169, 190, 61, 61, 468, 625], [483, 169, 61, 545, 346, 74, 543], [483, 210, 186, 190, 608, 521, 399], [483, 618, 308, 630, 190, 190, 680], [483, 131, 492, 300, 555, 652, 111], [483, 609, 687, 676, 653, 29, 29], [483, 609, 687, 676, 653, 29, 640], [483, 472, 637, 706, 358, 652, 111], [483, 404, 225, 208, 685, 398, 520], [483, 404, 208, 440, 685, 398, 520], [483, 556, 61, 355, 374, 136, 165], [483, 388, 169, 202, 355, 715, 165], [230, 621, 505, 129, 588, 687, 292], [230, 621, 505, 129, 687, 375, 373], [505, 356, 359, 521, 137, 233, 530], [505, 356, 53, 521, 482, 591, 530], [505, 129, 230, 621, 687, 375, 373], [314, 701, 172, 119, 185, 316, 202], [294, 579, 456, 52, 167, 687, 486], [67, 163, 325, 158, 281, 483, 474], [531, 363, 639, 730, 358, 621, 43], [517, 483, 719, 542, 358, 35, 528], [517, 43, 369, 308, 33, 422, 79], [517, 660, 720, 126, 196, 669, 202], [517, 43, 369, 308, 569, 699, 622], [503, 661, 687, 676, 715, 692, 632], [179, 689, 503, 392, 668, 485, 450], [235, 483, 110, 417, 287, 423, 184], [630, 342, 567, 468, 173, 238, 644], [375, 373, 500, 103, 301, 408, 43], [687, 457, 708, 5, 88, 487, 29], [687, 292, 55, 246, 499, 625, 47], [687, 528, 699, 190, 190, 514], [687, 676, 608, 206, 276, 320, 207], [687, 676, 608, 206, 276, 320, 207], [547, 680, 374, 76, 26, 497, 427], [354, 705, 19, 135, 687, 375, 373], [522, 205, 423, 184, 287, 660, 125], [522, 463, 680, 8, 261, 463, 399], [522, 556, 61, 355, 625, 189, 262], [522, 152, 172, 61, 625, 189, 262], [653, 314, 332, 366, 579, 316, 632], [653, 455, 356, 430, 353, 479, 632], [345, 483, 411, 608, 358, 465, 230], [345, 483, 556, 61, 61, 355, 165], [53, 400, 675, 479, 33, 106, 243], [625, 444, 653, 314, 421, 305, 53], [588, 483, 712, 388, 518, 405, 284], [301, 408, 628, 687, 726, 230, 573], [17, 687, 375, 107, 550, 274, 663], [149, 53, 676, 479, 33, 507, 530], [79, 61, 234, 82, 653, 669, 264], [72, 43, 274, 687, 167, 527, 236], [391, 69, 157, 157, 308, 77, 160], [391, 69, 157, 157, 369, 72, 160], [621, 630, 432, 71, 490, 142, 243], [621, 653, 603, 483, 61, 440, 404], [92, 676, 359, 717, 62, 643, 657], [434, 519, 212, 667, 237, 138, 487], [588, 687, 726, 352, 435], [353, 632, 475, 490, 243], [353, 632, 653, 455, 573], [501, 24, 148, 725, 75], [653, 401, 33, 490, 48], [675, 687, 62, 692, 632], [387, 92, 44, 653, 230], [492, 687, 418, 202, 373], [687, 74, 637, 103, 505], [687, 457, 260, 440, 505], [687, 457, 608, 521, 164], [687, 457, 557, 373, 65], [687, 457, 574, 451, 373], [492, 687, 418, 202, 373], [501, 444, 725, 81, 240], [687, 676, 608, 264, 373], [687, 457, 260, 440, 505], [687, 457, 557, 373, 65], [687, 457, 608, 521, 164], [274, 687, 230, 384, 392], [274, 687, 358, 164, 306], [675, 687, 542, 692, 632], [685, 249, 649, 652, 36], [687, 676, 653, 680, 399], [687, 676, 647, 230, 406], [687, 676, 567, 398, 48], [687, 676, 525, 355, 373], []]\n",
      "[[454, 653, 695, 115, 179, 473, 3], [358, 621, 630, 126, 621, 314, 3], [630, 630, 62, 184, 314, 701, 3], [190, 190, 625, 310, 350, 608, 3], [190, 190, 310, 539, 542, 726, 3], [180, 271, 597, 160, 86, 272, 3], [84, 562, 28, 247, 616, 356, 3], [612, 726, 305, 53, 59, 440, 3], [390, 230, 505, 178, 455, 117, 3], [459, 407, 525, 355, 584, 621, 3], [345, 577, 284, 193, 531, 719, 3], [345, 577, 525, 106, 653, 314, 3], [470, 669, 672, 660, 364, 32, 3], [267, 639, 17, 338, 302, 490, 3], [297, 136, 210, 220, 328, 285, 3], [492, 300, 727, 456, 91, 359, 3], [492, 180, 340, 219, 653, 695, 3], [281, 612, 396, 353, 358, 653, 3], [67, 7, 193, 648, 197, 426, 3], [67, 7, 193, 648, 178, 653, 3], [188, 710, 205, 547, 178, 455, 3], [188, 665, 205, 547, 178, 455, 3], [366, 577, 340, 112, 374, 725, 3], [111, 269, 694, 53, 61, 440, 3], [198, 455, 331, 40, 62, 546, 3], [621, 630, 621, 43, 621, 160, 3], [621, 43, 621, 630, 621, 314, 3], [202, 505, 414, 5, 141, 457, 3], [82, 532, 219, 118, 345, 577, 3], [607, 292, 490, 356, 723, 243, 3], [475, 490, 200, 200, 242, 653, 3], [198, 455, 331, 40, 62, 546, 3], [111, 561, 62, 566, 630, 308, 3], [383, 197, 525, 106, 588, 314, 3], [62, 540, 67, 7, 330, 473, 3], [541, 641, 583, 515, 692, 190, 3], [427, 621, 144, 206, 669, 154, 3], [454, 287, 67, 7, 330, 473, 3], [358, 463, 460, 133, 314, 701, 3], [653, 653, 170, 111, 189, 558, 3], [190, 250, 62, 566, 630, 308, 3], [669, 470, 488, 293, 90, 105, 3], [60, 79, 654, 475, 653, 455, 3], [653, 78, 53, 306, 86, 91, 3], [653, 463, 305, 723, 110, 445, 3], [475, 490, 110, 101, 724, 195, 3], [653, 463, 305, 723, 110, 336, 3], [180, 623, 547, 700, 254, 278, 3], [34, 110, 391, 69, 655, 646, 3], [652, 559, 38, 560, 557, 653, 3], [366, 608, 525, 673, 584, 621, 3], [33, 294, 340, 360, 190, 88, 3], [652, 436, 85, 85, 64, 154, 3], [44, 179, 17, 338, 267, 639, 3], [44, 726, 625, 561, 62, 546, 3], [44, 726, 653, 561, 62, 546, 3], [33, 190, 391, 447, 386, 61, 3], [569, 669, 118, 398, 288, 337, 3], [126, 392, 495, 36, 272, 314, 3], [733, 168, 134, 180, 652, 284, 342, 653, 3], [30, 544, 473, 395, 115, 179, 45, 601, 3], [724, 664, 724, 405, 190, 680, 62, 99, 3], [654, 475, 67, 425, 652, 507, 308, 276, 3], [143, 298, 315, 15, 608, 608, 64, 206, 3], [453, 183, 174, 130, 374, 201, 711, 528, 3], [374, 680, 547, 76, 62, 566, 630, 308, 3], [569, 669, 623, 264, 257, 321, 299, 653, 3], [314, 250, 160, 356, 374, 514, 630, 308, 3], [99, 642, 245, 190, 4, 441, 308, 666, 3], [67, 425, 535, 132, 284, 193, 531, 719, 3], [198, 455, 331, 40, 603, 563, 335, 326, 3], [625, 679, 257, 46, 569, 231, 178, 676, 3], [502, 215, 102, 10, 273, 581, 134, 180, 3], [653, 520, 669, 32, 33, 490, 403, 154, 3], [611, 608, 129, 82, 228, 447, 719, 703, 3], [440, 132, 529, 251, 230, 614, 189, 256, 3], [653, 314, 7, 224, 525, 184, 178, 154, 3], [136, 535, 75, 206, 38, 560, 557, 535, 3], [688, 39, 290, 62, 234, 587, 652, 442, 3], [564, 371, 62, 374, 599, 592, 44, 180, 3], [508, 109, 299, 339, 85, 85, 64, 154, 3], [653, 494, 567, 398, 447, 673, 647, 621, 3], [258, 726, 457, 267, 525, 648, 476, 719, 3], [492, 383, 282, 420, 206, 521, 623, 180, 3], [584, 601, 463, 630, 190, 680, 62, 666, 3], [340, 339, 621, 314, 49, 254, 128, 615, 3], [647, 457, 284, 68, 182, 181, 110, 314, 3], [126, 392, 189, 339, 208, 636, 621, 314, 3], [575, 307, 199, 561, 218, 549, 386, 61, 3], [158, 585, 551, 234, 420, 547, 403, 88, 3], [99, 642, 705, 190, 705, 190, 455, 154, 3], [93, 104, 644, 537, 238, 248, 415, 485, 3], [274, 303, 554, 648, 525, 106, 535, 605, 3], [721, 608, 633, 230, 525, 673, 584, 621, 3], [484, 198, 653, 488, 533, 265, 50, 349, 3], [263, 685, 536, 538, 238, 270, 498, 550, 3], [492, 459, 147, 720, 501, 252, 653, 314, 3], [721, 608, 633, 230, 569, 619, 280, 198, 3], [358, 238, 43, 625, 108, 97, 492, 300, 3], [652, 238, 613, 391, 230, 573, 653, 639, 3], [79, 608, 322, 82, 228, 447, 719, 487, 3], [685, 232, 356, 154, 671, 447, 79, 70, 3], [111, 269, 148, 241, 67, 163, 715, 158, 3], [308, 77, 375, 625, 625, 206, 511, 191, 3], [358, 621, 653, 540, 454, 621, 336, 473, 3], [443, 287, 192, 223, 678, 413, 277, 307, 3], [136, 535, 75, 206, 525, 648, 476, 719, 3], [653, 314, 431, 520, 391, 447, 386, 61, 3], [374, 594, 526, 682, 624, 453, 547, 709, 3], [703, 209, 67, 7, 227, 22, 287, 351, 3], [361, 472, 333, 398, 214, 433, 651, 614, 3], [569, 669, 623, 264, 653, 79, 62, 546, 3], [524, 484, 250, 714, 218, 549, 134, 180, 3], [652, 559, 67, 425, 647, 146, 44, 653, 3], [62, 546, 597, 160, 67, 7, 330, 473, 3], [311, 245, 62, 374, 374, 201, 23, 733, 3], [67, 425, 571, 726, 89, 522, 630, 3], [522, 630, 289, 456, 375, 325, 158, 3], [652, 111, 300, 669, 697, 483, 540, 3], [567, 485, 598, 113, 166, 608, 398, 3], [34, 345, 578, 666, 625, 386, 61, 3], [352, 435, 325, 158, 483, 128, 313, 3], [458, 674, 42, 296, 296, 299, 669, 3], [648, 610, 251, 75, 128, 301, 650, 3], [619, 42, 150, 248, 435, 546, 385, 3], [687, 676, 352, 435, 200, 200, 653, 3], [687, 139, 727, 456, 375, 654, 724, 3], [479, 726, 353, 632, 230, 662, 662, 3], [479, 726, 353, 632, 160, 177, 221, 3], [687, 276, 69, 393, 375, 654, 724, 3], [108, 276, 300, 510, 375, 653, 621, 3], [108, 276, 300, 206, 451, 653, 356, 3], [159, 395, 547, 190, 435, 49, 653, 3], [62, 424, 402, 50, 608, 128, 156, 3], [547, 165, 374, 158, 603, 483, 463, 3], [43, 41, 630, 88, 522, 556, 61, 3], [230, 40, 487, 267, 687, 483, 509, 3], [230, 40, 62, 546, 687, 483, 509, 3], [630, 88, 43, 41, 483, 32, 61, 3], [447, 673, 33, 586, 127, 483, 630, 3], [372, 357, 300, 669, 222, 483, 394, 3], [366, 577, 17, 338, 603, 531, 646, 3], [447, 415, 487, 608, 354, 483, 630, 3], [631, 687, 692, 632, 360, 154, 78, 3], [522, 630, 112, 111, 67, 660, 158, 3], [522, 43, 308, 4, 666, 441, 62, 3], [522, 630, 112, 111, 67, 660, 266, 3], [536, 205, 522, 37, 565, 73, 158, 3], [375, 547, 672, 618, 483, 648, 664, 3], [136, 482, 582, 94, 523, 272, 6, 3], [547, 663, 148, 725, 687, 483, 266, 3], [677, 317, 169, 202, 14, 621, 87, 3], [483, 324, 300, 669, 391, 69, 646, 3], [483, 463, 567, 398, 375, 653, 696, 3], [483, 43, 705, 34, 355, 355, 542, 3], [483, 609, 62, 546, 127, 553, 490, 3], [483, 127, 487, 267, 401, 553, 490, 3], [687, 676, 483, 609, 375, 713, 325, 3], [325, 158, 346, 722, 483, 709, 617, 3], [108, 375, 111, 491, 376, 567, 630, 3], [92, 632, 566, 359, 353, 632, 653, 3], [62, 604, 304, 80, 43, 468, 365, 3], [62, 99, 67, 163, 360, 468, 365, 3], [447, 673, 525, 33, 514, 522, 630, 3], [174, 347, 616, 356, 687, 483, 266, 3], [588, 687, 513, 406, 451, 308, 630, 3], [330, 410, 153, 435, 435, 314, 326, 3], [653, 283, 404, 483, 525, 190, 158, 3], [310, 358, 603, 483, 431, 274, 687, 3], [281, 593, 582, 171, 638, 287, 274, 3], [358, 621, 391, 483, 375, 630, 308, 3], [281, 483, 358, 653, 193, 603, 208, 3], [621, 653, 404, 483, 172, 268, 158, 3], [420, 435, 335, 356, 603, 483, 463, 3], [621, 653, 697, 483, 169, 639, 61, 3], [483, 394, 63, 63, 375, 653, 669, 3], [483, 394, 63, 63, 375, 653, 669, 3], [255, 483, 477, 269, 230, 505, 136, 3], [639, 730, 274, 531, 608, 51, 646, 3], [483, 169, 61, 545, 174, 576, 158, 3], [483, 205, 435, 266, 264, 18, 13, 3], [289, 456, 95, 258, 726, 3], [687, 676, 230, 505, 621, 3], [687, 676, 547, 306, 195, 3], [687, 555, 567, 468, 630, 3], [687, 676, 525, 355, 32, 3], [264, 690, 435, 250, 325, 3], [687, 238, 567, 111, 630, 3], [281, 483, 609, 630, 308, 3], [62, 358, 435, 176, 653, 3], [62, 310, 111, 106, 653, 3], [653, 696, 425, 93, 60, 3], [231, 539, 401, 653, 686, 3], [669, 505, 171, 681, 653, 3], [281, 483, 609, 630, 308, 3], [687, 358, 17, 338, 653, 3], [733, 497, 398, 494, 653, 3], [62, 310, 111, 106, 653, 3], [231, 539, 401, 653, 686, 3], [653, 696, 425, 93, 60, 3], [396, 353, 653, 98, 646, 3], [603, 344, 116, 653, 455, 3], [193, 648, 625, 358, 653, 3], [375, 122, 449, 654, 676, 3], [108, 375, 43, 325, 158, 3], [300, 31, 569, 669, 440, 3], [108, 375, 79, 693, 653, 3], [353, 632, 67, 660, 653, 3], [3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   1/2000 Batch    0/1 - Training Loss:  6.599  - Validation loss:  6.597\n",
      "Epoch   2/2000 Batch    0/1 - Training Loss:  6.595  - Validation loss:  6.594\n",
      "Epoch   3/2000 Batch    0/1 - Training Loss:  6.592  - Validation loss:  6.591\n",
      "Epoch   4/2000 Batch    0/1 - Training Loss:  6.588  - Validation loss:  6.588\n",
      "Epoch   5/2000 Batch    0/1 - Training Loss:  6.583  - Validation loss:  6.584\n",
      "Epoch   6/2000 Batch    0/1 - Training Loss:  6.578  - Validation loss:  6.579\n",
      "Epoch   7/2000 Batch    0/1 - Training Loss:  6.571  - Validation loss:  6.573\n",
      "Epoch   8/2000 Batch    0/1 - Training Loss:  6.563  - Validation loss:  6.565\n",
      "Epoch   9/2000 Batch    0/1 - Training Loss:  6.552  - Validation loss:  6.555\n",
      "Epoch  10/2000 Batch    0/1 - Training Loss:  6.538  - Validation loss:  6.543\n",
      "Epoch  11/2000 Batch    0/1 - Training Loss:  6.521  - Validation loss:  6.526\n",
      "Epoch  12/2000 Batch    0/1 - Training Loss:  6.498  - Validation loss:  6.506\n",
      "Epoch  13/2000 Batch    0/1 - Training Loss:  6.469  - Validation loss:  6.480\n",
      "Epoch  14/2000 Batch    0/1 - Training Loss:  6.433  - Validation loss:  6.449\n",
      "Epoch  15/2000 Batch    0/1 - Training Loss:  6.388  - Validation loss:  6.413\n",
      "Epoch  16/2000 Batch    0/1 - Training Loss:  6.334  - Validation loss:  6.375\n",
      "Epoch  17/2000 Batch    0/1 - Training Loss:  6.273  - Validation loss:  6.335\n",
      "Epoch  18/2000 Batch    0/1 - Training Loss:  6.206  - Validation loss:  6.297\n",
      "Epoch  19/2000 Batch    0/1 - Training Loss:  6.137  - Validation loss:  6.261\n",
      "Epoch  20/2000 Batch    0/1 - Training Loss:  6.066  - Validation loss:  6.230\n",
      "Epoch  21/2000 Batch    0/1 - Training Loss:  5.996  - Validation loss:  6.203\n",
      "Epoch  22/2000 Batch    0/1 - Training Loss:  5.928  - Validation loss:  6.180\n",
      "Epoch  23/2000 Batch    0/1 - Training Loss:  5.863  - Validation loss:  6.161\n",
      "Epoch  24/2000 Batch    0/1 - Training Loss:  5.801  - Validation loss:  6.145\n",
      "Epoch  25/2000 Batch    0/1 - Training Loss:  5.741  - Validation loss:  6.133\n",
      "Epoch  26/2000 Batch    0/1 - Training Loss:  5.684  - Validation loss:  6.123\n",
      "Epoch  27/2000 Batch    0/1 - Training Loss:  5.628  - Validation loss:  6.117\n",
      "Epoch  28/2000 Batch    0/1 - Training Loss:  5.575  - Validation loss:  6.113\n",
      "Epoch  29/2000 Batch    0/1 - Training Loss:  5.524  - Validation loss:  6.110\n",
      "Epoch  30/2000 Batch    0/1 - Training Loss:  5.474  - Validation loss:  6.109\n",
      "Epoch  31/2000 Batch    0/1 - Training Loss:  5.427  - Validation loss:  6.109\n",
      "Epoch  32/2000 Batch    0/1 - Training Loss:  5.382  - Validation loss:  6.110\n",
      "Epoch  33/2000 Batch    0/1 - Training Loss:  5.339  - Validation loss:  6.112\n",
      "Epoch  34/2000 Batch    0/1 - Training Loss:  5.299  - Validation loss:  6.114\n",
      "Epoch  35/2000 Batch    0/1 - Training Loss:  5.262  - Validation loss:  6.117\n",
      "Epoch  36/2000 Batch    0/1 - Training Loss:  5.227  - Validation loss:  6.121\n",
      "Epoch  37/2000 Batch    0/1 - Training Loss:  5.195  - Validation loss:  6.126\n",
      "Epoch  38/2000 Batch    0/1 - Training Loss:  5.165  - Validation loss:  6.131\n",
      "Epoch  39/2000 Batch    0/1 - Training Loss:  5.137  - Validation loss:  6.136\n",
      "Epoch  40/2000 Batch    0/1 - Training Loss:  5.111  - Validation loss:  6.142\n",
      "Epoch  41/2000 Batch    0/1 - Training Loss:  5.087  - Validation loss:  6.150\n",
      "Epoch  42/2000 Batch    0/1 - Training Loss:  5.067  - Validation loss:  6.160\n",
      "Epoch  43/2000 Batch    0/1 - Training Loss:  5.048  - Validation loss:  6.170\n",
      "Epoch  44/2000 Batch    0/1 - Training Loss:  5.030  - Validation loss:  6.182\n",
      "Epoch  45/2000 Batch    0/1 - Training Loss:  5.014  - Validation loss:  6.194\n",
      "Epoch  46/2000 Batch    0/1 - Training Loss:  4.999  - Validation loss:  6.205\n",
      "Epoch  47/2000 Batch    0/1 - Training Loss:  4.985  - Validation loss:  6.216\n",
      "Epoch  48/2000 Batch    0/1 - Training Loss:  4.972  - Validation loss:  6.227\n",
      "Epoch  49/2000 Batch    0/1 - Training Loss:  4.960  - Validation loss:  6.236\n",
      "Epoch  50/2000 Batch    0/1 - Training Loss:  4.948  - Validation loss:  6.245\n",
      "Epoch  51/2000 Batch    0/1 - Training Loss:  4.937  - Validation loss:  6.254\n",
      "Epoch  52/2000 Batch    0/1 - Training Loss:  4.927  - Validation loss:  6.263\n",
      "Epoch  53/2000 Batch    0/1 - Training Loss:  4.916  - Validation loss:  6.273\n",
      "Epoch  54/2000 Batch    0/1 - Training Loss:  4.907  - Validation loss:  6.282\n",
      "Epoch  55/2000 Batch    0/1 - Training Loss:  4.898  - Validation loss:  6.291\n",
      "Epoch  56/2000 Batch    0/1 - Training Loss:  4.889  - Validation loss:  6.299\n",
      "Epoch  57/2000 Batch    0/1 - Training Loss:  4.880  - Validation loss:  6.309\n",
      "Epoch  58/2000 Batch    0/1 - Training Loss:  4.872  - Validation loss:  6.317\n",
      "Epoch  59/2000 Batch    0/1 - Training Loss:  4.863  - Validation loss:  6.326\n",
      "Epoch  60/2000 Batch    0/1 - Training Loss:  4.855  - Validation loss:  6.333\n",
      "Epoch  61/2000 Batch    0/1 - Training Loss:  4.848  - Validation loss:  6.340\n",
      "Epoch  62/2000 Batch    0/1 - Training Loss:  4.840  - Validation loss:  6.346\n",
      "Epoch  63/2000 Batch    0/1 - Training Loss:  4.832  - Validation loss:  6.352\n",
      "Epoch  64/2000 Batch    0/1 - Training Loss:  4.825  - Validation loss:  6.358\n",
      "Epoch  65/2000 Batch    0/1 - Training Loss:  4.818  - Validation loss:  6.364\n",
      "Epoch  66/2000 Batch    0/1 - Training Loss:  4.810  - Validation loss:  6.370\n",
      "Epoch  67/2000 Batch    0/1 - Training Loss:  4.803  - Validation loss:  6.377\n",
      "Epoch  68/2000 Batch    0/1 - Training Loss:  4.796  - Validation loss:  6.383\n",
      "Epoch  69/2000 Batch    0/1 - Training Loss:  4.789  - Validation loss:  6.389\n",
      "Epoch  70/2000 Batch    0/1 - Training Loss:  4.783  - Validation loss:  6.394\n",
      "Epoch  71/2000 Batch    0/1 - Training Loss:  4.776  - Validation loss:  6.397\n",
      "Epoch  72/2000 Batch    0/1 - Training Loss:  4.769  - Validation loss:  6.399\n",
      "Epoch  73/2000 Batch    0/1 - Training Loss:  4.763  - Validation loss:  6.401\n",
      "Epoch  74/2000 Batch    0/1 - Training Loss:  4.756  - Validation loss:  6.404\n",
      "Epoch  75/2000 Batch    0/1 - Training Loss:  4.750  - Validation loss:  6.407\n",
      "Epoch  76/2000 Batch    0/1 - Training Loss:  4.744  - Validation loss:  6.412\n",
      "Epoch  77/2000 Batch    0/1 - Training Loss:  4.738  - Validation loss:  6.417\n",
      "Epoch  78/2000 Batch    0/1 - Training Loss:  4.732  - Validation loss:  6.422\n",
      "Epoch  79/2000 Batch    0/1 - Training Loss:  4.726  - Validation loss:  6.427\n",
      "Epoch  80/2000 Batch    0/1 - Training Loss:  4.720  - Validation loss:  6.431\n",
      "Epoch  81/2000 Batch    0/1 - Training Loss:  4.714  - Validation loss:  6.435\n",
      "Epoch  82/2000 Batch    0/1 - Training Loss:  4.708  - Validation loss:  6.437\n",
      "Epoch  83/2000 Batch    0/1 - Training Loss:  4.703  - Validation loss:  6.440\n",
      "Epoch  84/2000 Batch    0/1 - Training Loss:  4.697  - Validation loss:  6.445\n",
      "Epoch  85/2000 Batch    0/1 - Training Loss:  4.691  - Validation loss:  6.450\n",
      "Epoch  86/2000 Batch    0/1 - Training Loss:  4.686  - Validation loss:  6.456\n",
      "Epoch  87/2000 Batch    0/1 - Training Loss:  4.680  - Validation loss:  6.462\n",
      "Epoch  88/2000 Batch    0/1 - Training Loss:  4.675  - Validation loss:  6.468\n",
      "Epoch  89/2000 Batch    0/1 - Training Loss:  4.670  - Validation loss:  6.474\n",
      "Epoch  90/2000 Batch    0/1 - Training Loss:  4.664  - Validation loss:  6.480\n",
      "Epoch  91/2000 Batch    0/1 - Training Loss:  4.659  - Validation loss:  6.486\n",
      "Epoch  92/2000 Batch    0/1 - Training Loss:  4.654  - Validation loss:  6.491\n",
      "Epoch  93/2000 Batch    0/1 - Training Loss:  4.649  - Validation loss:  6.496\n",
      "Epoch  94/2000 Batch    0/1 - Training Loss:  4.644  - Validation loss:  6.501\n",
      "Epoch  95/2000 Batch    0/1 - Training Loss:  4.639  - Validation loss:  6.507\n",
      "Epoch  96/2000 Batch    0/1 - Training Loss:  4.634  - Validation loss:  6.512\n",
      "Epoch  97/2000 Batch    0/1 - Training Loss:  4.629  - Validation loss:  6.518\n",
      "Epoch  98/2000 Batch    0/1 - Training Loss:  4.624  - Validation loss:  6.523\n",
      "Epoch  99/2000 Batch    0/1 - Training Loss:  4.619  - Validation loss:  6.528\n",
      "Epoch 100/2000 Batch    0/1 - Training Loss:  4.614  - Validation loss:  6.533\n",
      "Epoch 101/2000 Batch    0/1 - Training Loss:  4.609  - Validation loss:  6.538\n",
      "Epoch 102/2000 Batch    0/1 - Training Loss:  4.605  - Validation loss:  6.542\n",
      "Epoch 103/2000 Batch    0/1 - Training Loss:  4.600  - Validation loss:  6.546\n",
      "Epoch 104/2000 Batch    0/1 - Training Loss:  4.596  - Validation loss:  6.550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/2000 Batch    0/1 - Training Loss:  4.591  - Validation loss:  6.555\n",
      "Epoch 106/2000 Batch    0/1 - Training Loss:  4.587  - Validation loss:  6.559\n",
      "Epoch 107/2000 Batch    0/1 - Training Loss:  4.582  - Validation loss:  6.564\n",
      "Epoch 108/2000 Batch    0/1 - Training Loss:  4.578  - Validation loss:  6.569\n",
      "Epoch 109/2000 Batch    0/1 - Training Loss:  4.573  - Validation loss:  6.573\n",
      "Epoch 110/2000 Batch    0/1 - Training Loss:  4.569  - Validation loss:  6.578\n",
      "Epoch 111/2000 Batch    0/1 - Training Loss:  4.564  - Validation loss:  6.583\n",
      "Epoch 112/2000 Batch    0/1 - Training Loss:  4.560  - Validation loss:  6.588\n",
      "Epoch 113/2000 Batch    0/1 - Training Loss:  4.556  - Validation loss:  6.594\n",
      "Epoch 114/2000 Batch    0/1 - Training Loss:  4.551  - Validation loss:  6.598\n",
      "Epoch 115/2000 Batch    0/1 - Training Loss:  4.547  - Validation loss:  6.603\n",
      "Epoch 116/2000 Batch    0/1 - Training Loss:  4.543  - Validation loss:  6.608\n",
      "Epoch 117/2000 Batch    0/1 - Training Loss:  4.538  - Validation loss:  6.612\n",
      "Epoch 118/2000 Batch    0/1 - Training Loss:  4.534  - Validation loss:  6.616\n",
      "Epoch 119/2000 Batch    0/1 - Training Loss:  4.530  - Validation loss:  6.620\n",
      "Epoch 120/2000 Batch    0/1 - Training Loss:  4.526  - Validation loss:  6.624\n",
      "Epoch 121/2000 Batch    0/1 - Training Loss:  4.522  - Validation loss:  6.628\n",
      "Epoch 122/2000 Batch    0/1 - Training Loss:  4.518  - Validation loss:  6.633\n",
      "Epoch 123/2000 Batch    0/1 - Training Loss:  4.513  - Validation loss:  6.636\n",
      "Epoch 124/2000 Batch    0/1 - Training Loss:  4.509  - Validation loss:  6.642\n",
      "Epoch 125/2000 Batch    0/1 - Training Loss:  4.505  - Validation loss:  6.642\n",
      "Epoch 126/2000 Batch    0/1 - Training Loss:  4.501  - Validation loss:  6.652\n",
      "Epoch 127/2000 Batch    0/1 - Training Loss:  4.497  - Validation loss:  6.653\n",
      "Epoch 128/2000 Batch    0/1 - Training Loss:  4.493  - Validation loss:  6.657\n",
      "Epoch 129/2000 Batch    0/1 - Training Loss:  4.489  - Validation loss:  6.667\n",
      "Epoch 130/2000 Batch    0/1 - Training Loss:  4.485  - Validation loss:  6.669\n",
      "Epoch 131/2000 Batch    0/1 - Training Loss:  4.481  - Validation loss:  6.673\n",
      "Epoch 132/2000 Batch    0/1 - Training Loss:  4.477  - Validation loss:  6.682\n",
      "Epoch 133/2000 Batch    0/1 - Training Loss:  4.473  - Validation loss:  6.684\n",
      "Epoch 134/2000 Batch    0/1 - Training Loss:  4.469  - Validation loss:  6.687\n",
      "Epoch 135/2000 Batch    0/1 - Training Loss:  4.465  - Validation loss:  6.696\n",
      "Epoch 136/2000 Batch    0/1 - Training Loss:  4.461  - Validation loss:  6.699\n",
      "Epoch 137/2000 Batch    0/1 - Training Loss:  4.457  - Validation loss:  6.703\n",
      "Epoch 138/2000 Batch    0/1 - Training Loss:  4.454  - Validation loss:  6.712\n",
      "Epoch 139/2000 Batch    0/1 - Training Loss:  4.450  - Validation loss:  6.714\n",
      "Epoch 140/2000 Batch    0/1 - Training Loss:  4.446  - Validation loss:  6.718\n",
      "Epoch 141/2000 Batch    0/1 - Training Loss:  4.442  - Validation loss:  6.726\n",
      "Epoch 142/2000 Batch    0/1 - Training Loss:  4.439  - Validation loss:  6.728\n",
      "Epoch 143/2000 Batch    0/1 - Training Loss:  4.435  - Validation loss:  6.734\n",
      "Epoch 144/2000 Batch    0/1 - Training Loss:  4.431  - Validation loss:  6.742\n",
      "Epoch 145/2000 Batch    0/1 - Training Loss:  4.427  - Validation loss:  6.744\n",
      "Epoch 146/2000 Batch    0/1 - Training Loss:  4.424  - Validation loss:  6.751\n",
      "Epoch 147/2000 Batch    0/1 - Training Loss:  4.420  - Validation loss:  6.757\n",
      "Epoch 148/2000 Batch    0/1 - Training Loss:  4.416  - Validation loss:  6.761\n",
      "Epoch 149/2000 Batch    0/1 - Training Loss:  4.413  - Validation loss:  6.769\n",
      "Epoch 150/2000 Batch    0/1 - Training Loss:  4.409  - Validation loss:  6.772\n",
      "Epoch 151/2000 Batch    0/1 - Training Loss:  4.405  - Validation loss:  6.779\n",
      "Epoch 152/2000 Batch    0/1 - Training Loss:  4.402  - Validation loss:  6.784\n",
      "Epoch 153/2000 Batch    0/1 - Training Loss:  4.398  - Validation loss:  6.788\n",
      "Epoch 154/2000 Batch    0/1 - Training Loss:  4.395  - Validation loss:  6.796\n",
      "Epoch 155/2000 Batch    0/1 - Training Loss:  4.391  - Validation loss:  6.797\n",
      "Epoch 156/2000 Batch    0/1 - Training Loss:  4.388  - Validation loss:  6.808\n",
      "Epoch 157/2000 Batch    0/1 - Training Loss:  4.384  - Validation loss:  6.805\n",
      "Epoch 158/2000 Batch    0/1 - Training Loss:  4.381  - Validation loss:  6.818\n",
      "Epoch 159/2000 Batch    0/1 - Training Loss:  4.377  - Validation loss:  6.818\n",
      "Epoch 160/2000 Batch    0/1 - Training Loss:  4.373  - Validation loss:  6.823\n",
      "Epoch 161/2000 Batch    0/1 - Training Loss:  4.369  - Validation loss:  6.832\n",
      "Epoch 162/2000 Batch    0/1 - Training Loss:  4.366  - Validation loss:  6.831\n",
      "Epoch 163/2000 Batch    0/1 - Training Loss:  4.362  - Validation loss:  6.839\n",
      "Epoch 164/2000 Batch    0/1 - Training Loss:  4.358  - Validation loss:  6.844\n",
      "Epoch 165/2000 Batch    0/1 - Training Loss:  4.355  - Validation loss:  6.844\n",
      "Epoch 166/2000 Batch    0/1 - Training Loss:  4.352  - Validation loss:  6.853\n",
      "Epoch 167/2000 Batch    0/1 - Training Loss:  4.348  - Validation loss:  6.855\n",
      "Epoch 168/2000 Batch    0/1 - Training Loss:  4.344  - Validation loss:  6.857\n",
      "Epoch 169/2000 Batch    0/1 - Training Loss:  4.340  - Validation loss:  6.865\n",
      "Epoch 170/2000 Batch    0/1 - Training Loss:  4.337  - Validation loss:  6.865\n",
      "Epoch 171/2000 Batch    0/1 - Training Loss:  4.333  - Validation loss:  6.870\n",
      "Epoch 172/2000 Batch    0/1 - Training Loss:  4.329  - Validation loss:  6.875\n",
      "Epoch 173/2000 Batch    0/1 - Training Loss:  4.326  - Validation loss:  6.875\n",
      "Epoch 174/2000 Batch    0/1 - Training Loss:  4.322  - Validation loss:  6.880\n",
      "Epoch 175/2000 Batch    0/1 - Training Loss:  4.318  - Validation loss:  6.882\n",
      "Epoch 176/2000 Batch    0/1 - Training Loss:  4.314  - Validation loss:  6.882\n",
      "Epoch 177/2000 Batch    0/1 - Training Loss:  4.310  - Validation loss:  6.886\n",
      "Epoch 178/2000 Batch    0/1 - Training Loss:  4.306  - Validation loss:  6.884\n",
      "Epoch 179/2000 Batch    0/1 - Training Loss:  4.301  - Validation loss:  6.886\n",
      "Epoch 180/2000 Batch    0/1 - Training Loss:  4.297  - Validation loss:  6.885\n",
      "Epoch 181/2000 Batch    0/1 - Training Loss:  4.292  - Validation loss:  6.883\n",
      "Epoch 182/2000 Batch    0/1 - Training Loss:  4.288  - Validation loss:  6.883\n",
      "Epoch 183/2000 Batch    0/1 - Training Loss:  4.283  - Validation loss:  6.880\n",
      "Epoch 184/2000 Batch    0/1 - Training Loss:  4.279  - Validation loss:  6.884\n",
      "Epoch 185/2000 Batch    0/1 - Training Loss:  4.275  - Validation loss:  6.884\n",
      "Epoch 186/2000 Batch    0/1 - Training Loss:  4.270  - Validation loss:  6.888\n",
      "Epoch 187/2000 Batch    0/1 - Training Loss:  4.266  - Validation loss:  6.891\n",
      "Epoch 188/2000 Batch    0/1 - Training Loss:  4.261  - Validation loss:  6.895\n",
      "Epoch 189/2000 Batch    0/1 - Training Loss:  4.257  - Validation loss:  6.898\n",
      "Epoch 190/2000 Batch    0/1 - Training Loss:  4.253  - Validation loss:  6.898\n",
      "Epoch 191/2000 Batch    0/1 - Training Loss:  4.248  - Validation loss:  6.902\n",
      "Epoch 192/2000 Batch    0/1 - Training Loss:  4.244  - Validation loss:  6.898\n",
      "Epoch 193/2000 Batch    0/1 - Training Loss:  4.239  - Validation loss:  6.900\n",
      "Epoch 194/2000 Batch    0/1 - Training Loss:  4.235  - Validation loss:  6.901\n",
      "Epoch 195/2000 Batch    0/1 - Training Loss:  4.230  - Validation loss:  6.900\n",
      "Epoch 196/2000 Batch    0/1 - Training Loss:  4.225  - Validation loss:  6.903\n",
      "Epoch 197/2000 Batch    0/1 - Training Loss:  4.220  - Validation loss:  6.902\n",
      "Epoch 198/2000 Batch    0/1 - Training Loss:  4.215  - Validation loss:  6.902\n",
      "Epoch 199/2000 Batch    0/1 - Training Loss:  4.210  - Validation loss:  6.902\n",
      "Epoch 200/2000 Batch    0/1 - Training Loss:  4.205  - Validation loss:  6.893\n",
      "Epoch 201/2000 Batch    0/1 - Training Loss:  4.201  - Validation loss:  6.900\n",
      "Epoch 202/2000 Batch    0/1 - Training Loss:  4.196  - Validation loss:  6.884\n",
      "Epoch 203/2000 Batch    0/1 - Training Loss:  4.192  - Validation loss:  6.888\n",
      "Epoch 204/2000 Batch    0/1 - Training Loss:  4.186  - Validation loss:  6.885\n",
      "Epoch 205/2000 Batch    0/1 - Training Loss:  4.180  - Validation loss:  6.879\n",
      "Epoch 206/2000 Batch    0/1 - Training Loss:  4.175  - Validation loss:  6.886\n",
      "Epoch 207/2000 Batch    0/1 - Training Loss:  4.171  - Validation loss:  6.883\n",
      "Epoch 208/2000 Batch    0/1 - Training Loss:  4.164  - Validation loss:  6.884\n",
      "Epoch 209/2000 Batch    0/1 - Training Loss:  4.159  - Validation loss:  6.888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210/2000 Batch    0/1 - Training Loss:  4.155  - Validation loss:  6.883\n",
      "Epoch 211/2000 Batch    0/1 - Training Loss:  4.149  - Validation loss:  6.887\n",
      "Epoch 212/2000 Batch    0/1 - Training Loss:  4.143  - Validation loss:  6.891\n",
      "Epoch 213/2000 Batch    0/1 - Training Loss:  4.138  - Validation loss:  6.886\n",
      "Epoch 214/2000 Batch    0/1 - Training Loss:  4.133  - Validation loss:  6.892\n",
      "Epoch 215/2000 Batch    0/1 - Training Loss:  4.127  - Validation loss:  6.897\n",
      "Epoch 216/2000 Batch    0/1 - Training Loss:  4.122  - Validation loss:  6.893\n",
      "Epoch 217/2000 Batch    0/1 - Training Loss:  4.117  - Validation loss:  6.900\n",
      "Epoch 218/2000 Batch    0/1 - Training Loss:  4.111  - Validation loss:  6.905\n",
      "Epoch 219/2000 Batch    0/1 - Training Loss:  4.106  - Validation loss:  6.901\n",
      "Epoch 220/2000 Batch    0/1 - Training Loss:  4.101  - Validation loss:  6.907\n",
      "Epoch 221/2000 Batch    0/1 - Training Loss:  4.095  - Validation loss:  6.910\n",
      "Epoch 222/2000 Batch    0/1 - Training Loss:  4.090  - Validation loss:  6.909\n",
      "Epoch 223/2000 Batch    0/1 - Training Loss:  4.084  - Validation loss:  6.915\n",
      "Epoch 224/2000 Batch    0/1 - Training Loss:  4.079  - Validation loss:  6.915\n",
      "Epoch 225/2000 Batch    0/1 - Training Loss:  4.074  - Validation loss:  6.921\n",
      "Epoch 226/2000 Batch    0/1 - Training Loss:  4.068  - Validation loss:  6.922\n",
      "Epoch 227/2000 Batch    0/1 - Training Loss:  4.063  - Validation loss:  6.925\n",
      "Epoch 228/2000 Batch    0/1 - Training Loss:  4.058  - Validation loss:  6.933\n",
      "Epoch 229/2000 Batch    0/1 - Training Loss:  4.053  - Validation loss:  6.929\n",
      "Epoch 230/2000 Batch    0/1 - Training Loss:  4.048  - Validation loss:  6.941\n",
      "Epoch 231/2000 Batch    0/1 - Training Loss:  4.043  - Validation loss:  6.938\n",
      "Epoch 232/2000 Batch    0/1 - Training Loss:  4.037  - Validation loss:  6.944\n",
      "Epoch 233/2000 Batch    0/1 - Training Loss:  4.031  - Validation loss:  6.949\n",
      "Epoch 234/2000 Batch    0/1 - Training Loss:  4.026  - Validation loss:  6.949\n",
      "Epoch 235/2000 Batch    0/1 - Training Loss:  4.021  - Validation loss:  6.958\n",
      "Epoch 236/2000 Batch    0/1 - Training Loss:  4.016  - Validation loss:  6.955\n",
      "Epoch 237/2000 Batch    0/1 - Training Loss:  4.011  - Validation loss:  6.963\n",
      "Epoch 238/2000 Batch    0/1 - Training Loss:  4.005  - Validation loss:  6.966\n",
      "Epoch 239/2000 Batch    0/1 - Training Loss:  4.000  - Validation loss:  6.965\n",
      "Epoch 240/2000 Batch    0/1 - Training Loss:  3.995  - Validation loss:  6.974\n",
      "Epoch 241/2000 Batch    0/1 - Training Loss:  3.990  - Validation loss:  6.973\n",
      "Epoch 242/2000 Batch    0/1 - Training Loss:  3.984  - Validation loss:  6.978\n",
      "Epoch 243/2000 Batch    0/1 - Training Loss:  3.978  - Validation loss:  6.982\n",
      "Epoch 244/2000 Batch    0/1 - Training Loss:  3.973  - Validation loss:  6.981\n",
      "Epoch 245/2000 Batch    0/1 - Training Loss:  3.968  - Validation loss:  6.989\n",
      "Epoch 246/2000 Batch    0/1 - Training Loss:  3.963  - Validation loss:  6.982\n",
      "Epoch 247/2000 Batch    0/1 - Training Loss:  3.959  - Validation loss:  6.996\n",
      "Epoch 248/2000 Batch    0/1 - Training Loss:  3.953  - Validation loss:  6.991\n",
      "Epoch 249/2000 Batch    0/1 - Training Loss:  3.947  - Validation loss:  6.994\n",
      "Epoch 250/2000 Batch    0/1 - Training Loss:  3.941  - Validation loss:  7.003\n",
      "Epoch 251/2000 Batch    0/1 - Training Loss:  3.937  - Validation loss:  6.996\n",
      "Epoch 252/2000 Batch    0/1 - Training Loss:  3.932  - Validation loss:  7.005\n",
      "Epoch 253/2000 Batch    0/1 - Training Loss:  3.926  - Validation loss:  7.004\n",
      "Epoch 254/2000 Batch    0/1 - Training Loss:  3.919  - Validation loss:  7.007\n",
      "Epoch 255/2000 Batch    0/1 - Training Loss:  3.913  - Validation loss:  7.011\n",
      "Epoch 256/2000 Batch    0/1 - Training Loss:  3.909  - Validation loss:  7.004\n",
      "Epoch 257/2000 Batch    0/1 - Training Loss:  3.906  - Validation loss:  7.018\n",
      "Epoch 258/2000 Batch    0/1 - Training Loss:  3.898  - Validation loss:  7.017\n",
      "Epoch 259/2000 Batch    0/1 - Training Loss:  3.891  - Validation loss:  7.015\n",
      "Epoch 260/2000 Batch    0/1 - Training Loss:  3.885  - Validation loss:  7.021\n",
      "Epoch 261/2000 Batch    0/1 - Training Loss:  3.881  - Validation loss:  7.019\n",
      "Epoch 262/2000 Batch    0/1 - Training Loss:  3.875  - Validation loss:  7.024\n",
      "Epoch 263/2000 Batch    0/1 - Training Loss:  3.867  - Validation loss:  7.024\n",
      "Epoch 264/2000 Batch    0/1 - Training Loss:  3.861  - Validation loss:  7.025\n",
      "Epoch 265/2000 Batch    0/1 - Training Loss:  3.856  - Validation loss:  7.032\n",
      "Epoch 266/2000 Batch    0/1 - Training Loss:  3.852  - Validation loss:  7.023\n",
      "Epoch 267/2000 Batch    0/1 - Training Loss:  3.848  - Validation loss:  7.037\n",
      "Epoch 268/2000 Batch    0/1 - Training Loss:  3.838  - Validation loss:  7.040\n",
      "Epoch 269/2000 Batch    0/1 - Training Loss:  3.831  - Validation loss:  7.036\n",
      "Epoch 270/2000 Batch    0/1 - Training Loss:  3.826  - Validation loss:  7.042\n",
      "Epoch 271/2000 Batch    0/1 - Training Loss:  3.820  - Validation loss:  7.046\n",
      "Epoch 272/2000 Batch    0/1 - Training Loss:  3.813  - Validation loss:  7.049\n",
      "Epoch 273/2000 Batch    0/1 - Training Loss:  3.806  - Validation loss:  7.050\n",
      "Epoch 274/2000 Batch    0/1 - Training Loss:  3.801  - Validation loss:  7.051\n",
      "Epoch 275/2000 Batch    0/1 - Training Loss:  3.796  - Validation loss:  7.063\n",
      "Epoch 276/2000 Batch    0/1 - Training Loss:  3.790  - Validation loss:  7.058\n",
      "Epoch 277/2000 Batch    0/1 - Training Loss:  3.784  - Validation loss:  7.065\n",
      "Epoch 278/2000 Batch    0/1 - Training Loss:  3.776  - Validation loss:  7.070\n",
      "Epoch 279/2000 Batch    0/1 - Training Loss:  3.769  - Validation loss:  7.068\n",
      "Epoch 280/2000 Batch    0/1 - Training Loss:  3.764  - Validation loss:  7.072\n",
      "Epoch 281/2000 Batch    0/1 - Training Loss:  3.759  - Validation loss:  7.068\n",
      "Epoch 282/2000 Batch    0/1 - Training Loss:  3.755  - Validation loss:  7.078\n",
      "Epoch 283/2000 Batch    0/1 - Training Loss:  3.746  - Validation loss:  7.078\n",
      "Epoch 284/2000 Batch    0/1 - Training Loss:  3.738  - Validation loss:  7.077\n",
      "Epoch 285/2000 Batch    0/1 - Training Loss:  3.732  - Validation loss:  7.083\n",
      "Epoch 286/2000 Batch    0/1 - Training Loss:  3.727  - Validation loss:  7.080\n",
      "Epoch 287/2000 Batch    0/1 - Training Loss:  3.721  - Validation loss:  7.085\n",
      "Epoch 288/2000 Batch    0/1 - Training Loss:  3.712  - Validation loss:  7.088\n",
      "Epoch 289/2000 Batch    0/1 - Training Loss:  3.706  - Validation loss:  7.086\n",
      "Epoch 290/2000 Batch    0/1 - Training Loss:  3.701  - Validation loss:  7.091\n",
      "Epoch 291/2000 Batch    0/1 - Training Loss:  3.694  - Validation loss:  7.091\n",
      "Epoch 292/2000 Batch    0/1 - Training Loss:  3.687  - Validation loss:  7.096\n",
      "Epoch 293/2000 Batch    0/1 - Training Loss:  3.678  - Validation loss:  7.097\n",
      "Epoch 294/2000 Batch    0/1 - Training Loss:  3.672  - Validation loss:  7.096\n",
      "Epoch 295/2000 Batch    0/1 - Training Loss:  3.667  - Validation loss:  7.102\n",
      "Epoch 296/2000 Batch    0/1 - Training Loss:  3.659  - Validation loss:  7.101\n",
      "Epoch 297/2000 Batch    0/1 - Training Loss:  3.651  - Validation loss:  7.104\n",
      "Epoch 298/2000 Batch    0/1 - Training Loss:  3.643  - Validation loss:  7.109\n",
      "Epoch 299/2000 Batch    0/1 - Training Loss:  3.636  - Validation loss:  7.108\n",
      "Epoch 300/2000 Batch    0/1 - Training Loss:  3.630  - Validation loss:  7.109\n",
      "Epoch 301/2000 Batch    0/1 - Training Loss:  3.624  - Validation loss:  7.114\n",
      "Epoch 302/2000 Batch    0/1 - Training Loss:  3.617  - Validation loss:  7.118\n",
      "Epoch 303/2000 Batch    0/1 - Training Loss:  3.609  - Validation loss:  7.118\n",
      "Epoch 304/2000 Batch    0/1 - Training Loss:  3.600  - Validation loss:  7.123\n",
      "Epoch 305/2000 Batch    0/1 - Training Loss:  3.594  - Validation loss:  7.127\n",
      "Epoch 306/2000 Batch    0/1 - Training Loss:  3.588  - Validation loss:  7.127\n",
      "Epoch 307/2000 Batch    0/1 - Training Loss:  3.582  - Validation loss:  7.131\n",
      "Epoch 308/2000 Batch    0/1 - Training Loss:  3.574  - Validation loss:  7.137\n",
      "Epoch 309/2000 Batch    0/1 - Training Loss:  3.566  - Validation loss:  7.135\n",
      "Epoch 310/2000 Batch    0/1 - Training Loss:  3.559  - Validation loss:  7.138\n",
      "Epoch 311/2000 Batch    0/1 - Training Loss:  3.553  - Validation loss:  7.143\n",
      "Epoch 312/2000 Batch    0/1 - Training Loss:  3.549  - Validation loss:  7.140\n",
      "Epoch 313/2000 Batch    0/1 - Training Loss:  3.546  - Validation loss:  7.146\n",
      "Epoch 314/2000 Batch    0/1 - Training Loss:  3.539  - Validation loss:  7.147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 315/2000 Batch    0/1 - Training Loss:  3.526  - Validation loss:  7.151\n",
      "Epoch 316/2000 Batch    0/1 - Training Loss:  3.524  - Validation loss:  7.157\n",
      "Epoch 317/2000 Batch    0/1 - Training Loss:  3.519  - Validation loss:  7.154\n",
      "Epoch 318/2000 Batch    0/1 - Training Loss:  3.506  - Validation loss:  7.163\n",
      "Epoch 319/2000 Batch    0/1 - Training Loss:  3.506  - Validation loss:  7.169\n",
      "Epoch 320/2000 Batch    0/1 - Training Loss:  3.498  - Validation loss:  7.170\n",
      "Epoch 321/2000 Batch    0/1 - Training Loss:  3.486  - Validation loss:  7.175\n",
      "Epoch 322/2000 Batch    0/1 - Training Loss:  3.485  - Validation loss:  7.183\n",
      "Epoch 323/2000 Batch    0/1 - Training Loss:  3.478  - Validation loss:  7.189\n",
      "Epoch 324/2000 Batch    0/1 - Training Loss:  3.466  - Validation loss:  7.186\n",
      "Epoch 325/2000 Batch    0/1 - Training Loss:  3.465  - Validation loss:  7.198\n",
      "Epoch 326/2000 Batch    0/1 - Training Loss:  3.456  - Validation loss:  7.204\n",
      "Epoch 327/2000 Batch    0/1 - Training Loss:  3.446  - Validation loss:  7.200\n",
      "Epoch 328/2000 Batch    0/1 - Training Loss:  3.445  - Validation loss:  7.210\n",
      "Epoch 329/2000 Batch    0/1 - Training Loss:  3.435  - Validation loss:  7.217\n",
      "Epoch 330/2000 Batch    0/1 - Training Loss:  3.427  - Validation loss:  7.217\n",
      "Epoch 331/2000 Batch    0/1 - Training Loss:  3.425  - Validation loss:  7.222\n",
      "Epoch 332/2000 Batch    0/1 - Training Loss:  3.413  - Validation loss:  7.227\n",
      "Epoch 333/2000 Batch    0/1 - Training Loss:  3.408  - Validation loss:  7.229\n",
      "Epoch 334/2000 Batch    0/1 - Training Loss:  3.402  - Validation loss:  7.234\n",
      "Epoch 335/2000 Batch    0/1 - Training Loss:  3.393  - Validation loss:  7.241\n",
      "Epoch 336/2000 Batch    0/1 - Training Loss:  3.389  - Validation loss:  7.245\n",
      "Epoch 337/2000 Batch    0/1 - Training Loss:  3.381  - Validation loss:  7.252\n",
      "Epoch 338/2000 Batch    0/1 - Training Loss:  3.373  - Validation loss:  7.261\n",
      "Epoch 339/2000 Batch    0/1 - Training Loss:  3.368  - Validation loss:  7.262\n",
      "Epoch 340/2000 Batch    0/1 - Training Loss:  3.361  - Validation loss:  7.268\n",
      "Epoch 341/2000 Batch    0/1 - Training Loss:  3.354  - Validation loss:  7.277\n",
      "Epoch 342/2000 Batch    0/1 - Training Loss:  3.349  - Validation loss:  7.277\n",
      "Epoch 343/2000 Batch    0/1 - Training Loss:  3.341  - Validation loss:  7.282\n",
      "Epoch 344/2000 Batch    0/1 - Training Loss:  3.335  - Validation loss:  7.294\n",
      "Epoch 345/2000 Batch    0/1 - Training Loss:  3.329  - Validation loss:  7.295\n",
      "Epoch 346/2000 Batch    0/1 - Training Loss:  3.322  - Validation loss:  7.298\n",
      "Epoch 347/2000 Batch    0/1 - Training Loss:  3.316  - Validation loss:  7.309\n",
      "Epoch 348/2000 Batch    0/1 - Training Loss:  3.310  - Validation loss:  7.309\n",
      "Epoch 349/2000 Batch    0/1 - Training Loss:  3.303  - Validation loss:  7.309\n",
      "Epoch 350/2000 Batch    0/1 - Training Loss:  3.297  - Validation loss:  7.318\n",
      "Epoch 351/2000 Batch    0/1 - Training Loss:  3.290  - Validation loss:  7.321\n",
      "Epoch 352/2000 Batch    0/1 - Training Loss:  3.284  - Validation loss:  7.322\n",
      "Epoch 353/2000 Batch    0/1 - Training Loss:  3.278  - Validation loss:  7.332\n",
      "Epoch 354/2000 Batch    0/1 - Training Loss:  3.271  - Validation loss:  7.336\n",
      "Epoch 355/2000 Batch    0/1 - Training Loss:  3.265  - Validation loss:  7.337\n",
      "Epoch 356/2000 Batch    0/1 - Training Loss:  3.259  - Validation loss:  7.344\n",
      "Epoch 357/2000 Batch    0/1 - Training Loss:  3.253  - Validation loss:  7.350\n",
      "Epoch 358/2000 Batch    0/1 - Training Loss:  3.247  - Validation loss:  7.352\n",
      "Epoch 359/2000 Batch    0/1 - Training Loss:  3.240  - Validation loss:  7.357\n",
      "Epoch 360/2000 Batch    0/1 - Training Loss:  3.234  - Validation loss:  7.364\n",
      "Epoch 361/2000 Batch    0/1 - Training Loss:  3.228  - Validation loss:  7.367\n",
      "Epoch 362/2000 Batch    0/1 - Training Loss:  3.222  - Validation loss:  7.371\n",
      "Epoch 363/2000 Batch    0/1 - Training Loss:  3.216  - Validation loss:  7.378\n",
      "Epoch 364/2000 Batch    0/1 - Training Loss:  3.210  - Validation loss:  7.381\n",
      "Epoch 365/2000 Batch    0/1 - Training Loss:  3.204  - Validation loss:  7.384\n",
      "Epoch 366/2000 Batch    0/1 - Training Loss:  3.198  - Validation loss:  7.391\n",
      "Epoch 367/2000 Batch    0/1 - Training Loss:  3.192  - Validation loss:  7.394\n",
      "Epoch 368/2000 Batch    0/1 - Training Loss:  3.186  - Validation loss:  7.397\n",
      "Epoch 369/2000 Batch    0/1 - Training Loss:  3.180  - Validation loss:  7.403\n",
      "Epoch 370/2000 Batch    0/1 - Training Loss:  3.174  - Validation loss:  7.406\n",
      "Epoch 371/2000 Batch    0/1 - Training Loss:  3.168  - Validation loss:  7.409\n",
      "Epoch 372/2000 Batch    0/1 - Training Loss:  3.162  - Validation loss:  7.414\n",
      "Epoch 373/2000 Batch    0/1 - Training Loss:  3.156  - Validation loss:  7.416\n",
      "Epoch 374/2000 Batch    0/1 - Training Loss:  3.151  - Validation loss:  7.419\n",
      "Epoch 375/2000 Batch    0/1 - Training Loss:  3.146  - Validation loss:  7.425\n",
      "Epoch 376/2000 Batch    0/1 - Training Loss:  3.143  - Validation loss:  7.428\n",
      "Epoch 377/2000 Batch    0/1 - Training Loss:  3.142  - Validation loss:  7.432\n",
      "Epoch 378/2000 Batch    0/1 - Training Loss:  3.139  - Validation loss:  7.436\n",
      "Epoch 379/2000 Batch    0/1 - Training Loss:  3.125  - Validation loss:  7.442\n",
      "Epoch 380/2000 Batch    0/1 - Training Loss:  3.117  - Validation loss:  7.445\n",
      "Epoch 381/2000 Batch    0/1 - Training Loss:  3.117  - Validation loss:  7.447\n",
      "Epoch 382/2000 Batch    0/1 - Training Loss:  3.107  - Validation loss:  7.455\n",
      "Epoch 383/2000 Batch    0/1 - Training Loss:  3.099  - Validation loss:  7.457\n",
      "Epoch 384/2000 Batch    0/1 - Training Loss:  3.098  - Validation loss:  7.463\n",
      "Epoch 385/2000 Batch    0/1 - Training Loss:  3.098  - Validation loss:  7.464\n",
      "Epoch 386/2000 Batch    0/1 - Training Loss:  3.088  - Validation loss:  7.469\n",
      "Epoch 387/2000 Batch    0/1 - Training Loss:  3.077  - Validation loss:  7.474\n",
      "Epoch 388/2000 Batch    0/1 - Training Loss:  3.077  - Validation loss:  7.475\n",
      "Epoch 389/2000 Batch    0/1 - Training Loss:  3.069  - Validation loss:  7.477\n",
      "Epoch 390/2000 Batch    0/1 - Training Loss:  3.060  - Validation loss:  7.482\n",
      "Epoch 391/2000 Batch    0/1 - Training Loss:  3.059  - Validation loss:  7.486\n",
      "Epoch 392/2000 Batch    0/1 - Training Loss:  3.048  - Validation loss:  7.490\n",
      "Epoch 393/2000 Batch    0/1 - Training Loss:  3.044  - Validation loss:  7.496\n",
      "Epoch 394/2000 Batch    0/1 - Training Loss:  3.041  - Validation loss:  7.499\n",
      "Epoch 395/2000 Batch    0/1 - Training Loss:  3.033  - Validation loss:  7.503\n",
      "Epoch 396/2000 Batch    0/1 - Training Loss:  3.025  - Validation loss:  7.506\n",
      "Epoch 397/2000 Batch    0/1 - Training Loss:  3.021  - Validation loss:  7.510\n",
      "Epoch 398/2000 Batch    0/1 - Training Loss:  3.015  - Validation loss:  7.516\n",
      "Epoch 399/2000 Batch    0/1 - Training Loss:  3.009  - Validation loss:  7.519\n",
      "Epoch 400/2000 Batch    0/1 - Training Loss:  3.004  - Validation loss:  7.522\n",
      "Epoch 401/2000 Batch    0/1 - Training Loss:  2.998  - Validation loss:  7.527\n",
      "Epoch 402/2000 Batch    0/1 - Training Loss:  2.992  - Validation loss:  7.530\n",
      "Epoch 403/2000 Batch    0/1 - Training Loss:  2.987  - Validation loss:  7.533\n",
      "Epoch 404/2000 Batch    0/1 - Training Loss:  2.981  - Validation loss:  7.539\n",
      "Epoch 405/2000 Batch    0/1 - Training Loss:  2.975  - Validation loss:  7.544\n",
      "Epoch 406/2000 Batch    0/1 - Training Loss:  2.970  - Validation loss:  7.548\n",
      "Epoch 407/2000 Batch    0/1 - Training Loss:  2.964  - Validation loss:  7.554\n",
      "Epoch 408/2000 Batch    0/1 - Training Loss:  2.959  - Validation loss:  7.557\n",
      "Epoch 409/2000 Batch    0/1 - Training Loss:  2.953  - Validation loss:  7.562\n",
      "Epoch 410/2000 Batch    0/1 - Training Loss:  2.947  - Validation loss:  7.567\n",
      "Epoch 411/2000 Batch    0/1 - Training Loss:  2.941  - Validation loss:  7.573\n",
      "Epoch 412/2000 Batch    0/1 - Training Loss:  2.936  - Validation loss:  7.577\n",
      "Epoch 413/2000 Batch    0/1 - Training Loss:  2.930  - Validation loss:  7.583\n",
      "Epoch 414/2000 Batch    0/1 - Training Loss:  2.924  - Validation loss:  7.589\n",
      "Epoch 415/2000 Batch    0/1 - Training Loss:  2.918  - Validation loss:  7.593\n",
      "Epoch 416/2000 Batch    0/1 - Training Loss:  2.913  - Validation loss:  7.600\n",
      "Epoch 417/2000 Batch    0/1 - Training Loss:  2.908  - Validation loss:  7.606\n",
      "Epoch 418/2000 Batch    0/1 - Training Loss:  2.902  - Validation loss:  7.611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 419/2000 Batch    0/1 - Training Loss:  2.896  - Validation loss:  7.617\n",
      "Epoch 420/2000 Batch    0/1 - Training Loss:  2.891  - Validation loss:  7.622\n",
      "Epoch 421/2000 Batch    0/1 - Training Loss:  2.885  - Validation loss:  7.628\n",
      "Epoch 422/2000 Batch    0/1 - Training Loss:  2.880  - Validation loss:  7.634\n",
      "Epoch 423/2000 Batch    0/1 - Training Loss:  2.875  - Validation loss:  7.638\n",
      "Epoch 424/2000 Batch    0/1 - Training Loss:  2.870  - Validation loss:  7.643\n",
      "Epoch 425/2000 Batch    0/1 - Training Loss:  2.865  - Validation loss:  7.648\n",
      "Epoch 426/2000 Batch    0/1 - Training Loss:  2.859  - Validation loss:  7.652\n",
      "Epoch 427/2000 Batch    0/1 - Training Loss:  2.854  - Validation loss:  7.657\n",
      "Epoch 428/2000 Batch    0/1 - Training Loss:  2.849  - Validation loss:  7.662\n",
      "Epoch 429/2000 Batch    0/1 - Training Loss:  2.844  - Validation loss:  7.665\n",
      "Epoch 430/2000 Batch    0/1 - Training Loss:  2.839  - Validation loss:  7.670\n",
      "Epoch 431/2000 Batch    0/1 - Training Loss:  2.834  - Validation loss:  7.674\n",
      "Epoch 432/2000 Batch    0/1 - Training Loss:  2.829  - Validation loss:  7.678\n",
      "Epoch 433/2000 Batch    0/1 - Training Loss:  2.824  - Validation loss:  7.682\n",
      "Epoch 434/2000 Batch    0/1 - Training Loss:  2.819  - Validation loss:  7.687\n",
      "Epoch 435/2000 Batch    0/1 - Training Loss:  2.814  - Validation loss:  7.690\n",
      "Epoch 436/2000 Batch    0/1 - Training Loss:  2.810  - Validation loss:  7.696\n",
      "Epoch 437/2000 Batch    0/1 - Training Loss:  2.806  - Validation loss:  7.698\n",
      "Epoch 438/2000 Batch    0/1 - Training Loss:  2.803  - Validation loss:  7.706\n",
      "Epoch 439/2000 Batch    0/1 - Training Loss:  2.799  - Validation loss:  7.708\n",
      "Epoch 440/2000 Batch    0/1 - Training Loss:  2.791  - Validation loss:  7.711\n",
      "Epoch 441/2000 Batch    0/1 - Training Loss:  2.783  - Validation loss:  7.718\n",
      "Epoch 442/2000 Batch    0/1 - Training Loss:  2.779  - Validation loss:  7.721\n",
      "Epoch 443/2000 Batch    0/1 - Training Loss:  2.776  - Validation loss:  7.725\n",
      "Epoch 444/2000 Batch    0/1 - Training Loss:  2.771  - Validation loss:  7.729\n",
      "Epoch 445/2000 Batch    0/1 - Training Loss:  2.764  - Validation loss:  7.735\n",
      "Epoch 446/2000 Batch    0/1 - Training Loss:  2.758  - Validation loss:  7.736\n",
      "Epoch 447/2000 Batch    0/1 - Training Loss:  2.753  - Validation loss:  7.741\n",
      "Epoch 448/2000 Batch    0/1 - Training Loss:  2.748  - Validation loss:  7.748\n",
      "Epoch 449/2000 Batch    0/1 - Training Loss:  2.744  - Validation loss:  7.749\n",
      "Epoch 450/2000 Batch    0/1 - Training Loss:  2.739  - Validation loss:  7.755\n",
      "Epoch 451/2000 Batch    0/1 - Training Loss:  2.733  - Validation loss:  7.759\n",
      "Epoch 452/2000 Batch    0/1 - Training Loss:  2.728  - Validation loss:  7.762\n",
      "Epoch 453/2000 Batch    0/1 - Training Loss:  2.724  - Validation loss:  7.766\n",
      "Epoch 454/2000 Batch    0/1 - Training Loss:  2.719  - Validation loss:  7.773\n",
      "Epoch 455/2000 Batch    0/1 - Training Loss:  2.714  - Validation loss:  7.775\n",
      "Epoch 456/2000 Batch    0/1 - Training Loss:  2.708  - Validation loss:  7.780\n",
      "Epoch 457/2000 Batch    0/1 - Training Loss:  2.704  - Validation loss:  7.786\n",
      "Epoch 458/2000 Batch    0/1 - Training Loss:  2.699  - Validation loss:  7.788\n",
      "Epoch 459/2000 Batch    0/1 - Training Loss:  2.695  - Validation loss:  7.793\n",
      "Epoch 460/2000 Batch    0/1 - Training Loss:  2.689  - Validation loss:  7.799\n",
      "Epoch 461/2000 Batch    0/1 - Training Loss:  2.684  - Validation loss:  7.801\n",
      "Epoch 462/2000 Batch    0/1 - Training Loss:  2.680  - Validation loss:  7.807\n",
      "Epoch 463/2000 Batch    0/1 - Training Loss:  2.676  - Validation loss:  7.811\n",
      "Epoch 464/2000 Batch    0/1 - Training Loss:  2.672  - Validation loss:  7.817\n",
      "Epoch 465/2000 Batch    0/1 - Training Loss:  2.669  - Validation loss:  7.819\n",
      "Epoch 466/2000 Batch    0/1 - Training Loss:  2.667  - Validation loss:  7.829\n",
      "Epoch 467/2000 Batch    0/1 - Training Loss:  2.668  - Validation loss:  7.827\n",
      "Epoch 468/2000 Batch    0/1 - Training Loss:  2.668  - Validation loss:  7.834\n",
      "Epoch 469/2000 Batch    0/1 - Training Loss:  2.654  - Validation loss:  7.843\n",
      "Epoch 470/2000 Batch    0/1 - Training Loss:  2.646  - Validation loss:  7.842\n",
      "Epoch 471/2000 Batch    0/1 - Training Loss:  2.647  - Validation loss:  7.846\n",
      "Epoch 472/2000 Batch    0/1 - Training Loss:  2.638  - Validation loss:  7.855\n",
      "Epoch 473/2000 Batch    0/1 - Training Loss:  2.632  - Validation loss:  7.859\n",
      "Epoch 474/2000 Batch    0/1 - Training Loss:  2.628  - Validation loss:  7.861\n",
      "Epoch 475/2000 Batch    0/1 - Training Loss:  2.623  - Validation loss:  7.865\n",
      "Epoch 476/2000 Batch    0/1 - Training Loss:  2.617  - Validation loss:  7.875\n",
      "Epoch 477/2000 Batch    0/1 - Training Loss:  2.612  - Validation loss:  7.876\n",
      "Epoch 478/2000 Batch    0/1 - Training Loss:  2.606  - Validation loss:  7.875\n",
      "Epoch 479/2000 Batch    0/1 - Training Loss:  2.604  - Validation loss:  7.885\n",
      "Epoch 480/2000 Batch    0/1 - Training Loss:  2.597  - Validation loss:  7.888\n",
      "Epoch 481/2000 Batch    0/1 - Training Loss:  2.592  - Validation loss:  7.885\n",
      "Epoch 482/2000 Batch    0/1 - Training Loss:  2.589  - Validation loss:  7.892\n",
      "Epoch 483/2000 Batch    0/1 - Training Loss:  2.584  - Validation loss:  7.898\n",
      "Epoch 484/2000 Batch    0/1 - Training Loss:  2.578  - Validation loss:  7.898\n",
      "Epoch 485/2000 Batch    0/1 - Training Loss:  2.573  - Validation loss:  7.900\n",
      "Epoch 486/2000 Batch    0/1 - Training Loss:  2.569  - Validation loss:  7.905\n",
      "Epoch 487/2000 Batch    0/1 - Training Loss:  2.564  - Validation loss:  7.912\n",
      "Epoch 488/2000 Batch    0/1 - Training Loss:  2.560  - Validation loss:  7.911\n",
      "Epoch 489/2000 Batch    0/1 - Training Loss:  2.554  - Validation loss:  7.916\n",
      "Epoch 490/2000 Batch    0/1 - Training Loss:  2.549  - Validation loss:  7.925\n",
      "Epoch 491/2000 Batch    0/1 - Training Loss:  2.546  - Validation loss:  7.925\n",
      "Epoch 492/2000 Batch    0/1 - Training Loss:  2.541  - Validation loss:  7.928\n",
      "Epoch 493/2000 Batch    0/1 - Training Loss:  2.537  - Validation loss:  7.931\n",
      "Epoch 494/2000 Batch    0/1 - Training Loss:  2.532  - Validation loss:  7.937\n",
      "Epoch 495/2000 Batch    0/1 - Training Loss:  2.527  - Validation loss:  7.938\n",
      "Epoch 496/2000 Batch    0/1 - Training Loss:  2.522  - Validation loss:  7.944\n",
      "Epoch 497/2000 Batch    0/1 - Training Loss:  2.518  - Validation loss:  7.952\n",
      "Epoch 498/2000 Batch    0/1 - Training Loss:  2.513  - Validation loss:  7.954\n",
      "Epoch 499/2000 Batch    0/1 - Training Loss:  2.509  - Validation loss:  7.959\n",
      "Epoch 500/2000 Batch    0/1 - Training Loss:  2.505  - Validation loss:  7.964\n",
      "Epoch 501/2000 Batch    0/1 - Training Loss:  2.501  - Validation loss:  7.972\n",
      "Epoch 502/2000 Batch    0/1 - Training Loss:  2.498  - Validation loss:  7.972\n",
      "Epoch 503/2000 Batch    0/1 - Training Loss:  2.494  - Validation loss:  7.979\n",
      "Epoch 504/2000 Batch    0/1 - Training Loss:  2.489  - Validation loss:  7.984\n",
      "Epoch 505/2000 Batch    0/1 - Training Loss:  2.483  - Validation loss:  7.988\n",
      "Epoch 506/2000 Batch    0/1 - Training Loss:  2.477  - Validation loss:  7.991\n",
      "Epoch 507/2000 Batch    0/1 - Training Loss:  2.473  - Validation loss:  7.996\n",
      "Epoch 508/2000 Batch    0/1 - Training Loss:  2.470  - Validation loss:  8.003\n",
      "Epoch 509/2000 Batch    0/1 - Training Loss:  2.466  - Validation loss:  8.003\n",
      "Epoch 510/2000 Batch    0/1 - Training Loss:  2.463  - Validation loss:  8.011\n",
      "Epoch 511/2000 Batch    0/1 - Training Loss:  2.459  - Validation loss:  8.013\n",
      "Epoch 512/2000 Batch    0/1 - Training Loss:  2.455  - Validation loss:  8.018\n",
      "Epoch 513/2000 Batch    0/1 - Training Loss:  2.449  - Validation loss:  8.022\n",
      "Epoch 514/2000 Batch    0/1 - Training Loss:  2.442  - Validation loss:  8.028\n",
      "Epoch 515/2000 Batch    0/1 - Training Loss:  2.438  - Validation loss:  8.032\n",
      "Epoch 516/2000 Batch    0/1 - Training Loss:  2.434  - Validation loss:  8.035\n",
      "Epoch 517/2000 Batch    0/1 - Training Loss:  2.430  - Validation loss:  8.041\n",
      "Epoch 518/2000 Batch    0/1 - Training Loss:  2.426  - Validation loss:  8.044\n",
      "Epoch 519/2000 Batch    0/1 - Training Loss:  2.422  - Validation loss:  8.048\n",
      "Epoch 520/2000 Batch    0/1 - Training Loss:  2.416  - Validation loss:  8.052\n",
      "Epoch 521/2000 Batch    0/1 - Training Loss:  2.411  - Validation loss:  8.058\n",
      "Epoch 522/2000 Batch    0/1 - Training Loss:  2.407  - Validation loss:  8.061\n",
      "Epoch 523/2000 Batch    0/1 - Training Loss:  2.402  - Validation loss:  8.065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 524/2000 Batch    0/1 - Training Loss:  2.397  - Validation loss:  8.071\n",
      "Epoch 525/2000 Batch    0/1 - Training Loss:  2.393  - Validation loss:  8.073\n",
      "Epoch 526/2000 Batch    0/1 - Training Loss:  2.390  - Validation loss:  8.078\n",
      "Epoch 527/2000 Batch    0/1 - Training Loss:  2.386  - Validation loss:  8.079\n",
      "Epoch 528/2000 Batch    0/1 - Training Loss:  2.385  - Validation loss:  8.087\n",
      "Epoch 529/2000 Batch    0/1 - Training Loss:  2.383  - Validation loss:  8.089\n",
      "Epoch 530/2000 Batch    0/1 - Training Loss:  2.380  - Validation loss:  8.096\n",
      "Epoch 531/2000 Batch    0/1 - Training Loss:  2.371  - Validation loss:  8.101\n",
      "Epoch 532/2000 Batch    0/1 - Training Loss:  2.364  - Validation loss:  8.106\n",
      "Epoch 533/2000 Batch    0/1 - Training Loss:  2.360  - Validation loss:  8.109\n",
      "Epoch 534/2000 Batch    0/1 - Training Loss:  2.357  - Validation loss:  8.112\n",
      "Epoch 535/2000 Batch    0/1 - Training Loss:  2.354  - Validation loss:  8.119\n",
      "Epoch 536/2000 Batch    0/1 - Training Loss:  2.349  - Validation loss:  8.118\n",
      "Epoch 537/2000 Batch    0/1 - Training Loss:  2.342  - Validation loss:  8.123\n",
      "Epoch 538/2000 Batch    0/1 - Training Loss:  2.337  - Validation loss:  8.129\n",
      "Epoch 539/2000 Batch    0/1 - Training Loss:  2.333  - Validation loss:  8.132\n",
      "Epoch 540/2000 Batch    0/1 - Training Loss:  2.329  - Validation loss:  8.137\n",
      "Epoch 541/2000 Batch    0/1 - Training Loss:  2.324  - Validation loss:  8.142\n",
      "Epoch 542/2000 Batch    0/1 - Training Loss:  2.321  - Validation loss:  8.145\n",
      "Epoch 543/2000 Batch    0/1 - Training Loss:  2.318  - Validation loss:  8.149\n",
      "Epoch 544/2000 Batch    0/1 - Training Loss:  2.317  - Validation loss:  8.152\n",
      "Epoch 545/2000 Batch    0/1 - Training Loss:  2.310  - Validation loss:  8.160\n",
      "Epoch 546/2000 Batch    0/1 - Training Loss:  2.304  - Validation loss:  8.165\n",
      "Epoch 547/2000 Batch    0/1 - Training Loss:  2.300  - Validation loss:  8.166\n",
      "Epoch 548/2000 Batch    0/1 - Training Loss:  2.294  - Validation loss:  8.173\n",
      "Epoch 549/2000 Batch    0/1 - Training Loss:  2.289  - Validation loss:  8.178\n",
      "Epoch 550/2000 Batch    0/1 - Training Loss:  2.287  - Validation loss:  8.179\n",
      "Epoch 551/2000 Batch    0/1 - Training Loss:  2.282  - Validation loss:  8.183\n",
      "Epoch 552/2000 Batch    0/1 - Training Loss:  2.277  - Validation loss:  8.189\n",
      "Epoch 553/2000 Batch    0/1 - Training Loss:  2.274  - Validation loss:  8.191\n",
      "Epoch 554/2000 Batch    0/1 - Training Loss:  2.269  - Validation loss:  8.197\n",
      "Epoch 555/2000 Batch    0/1 - Training Loss:  2.263  - Validation loss:  8.204\n",
      "Epoch 556/2000 Batch    0/1 - Training Loss:  2.259  - Validation loss:  8.209\n",
      "Epoch 557/2000 Batch    0/1 - Training Loss:  2.255  - Validation loss:  8.213\n",
      "Epoch 558/2000 Batch    0/1 - Training Loss:  2.250  - Validation loss:  8.217\n",
      "Epoch 559/2000 Batch    0/1 - Training Loss:  2.244  - Validation loss:  8.220\n",
      "Epoch 560/2000 Batch    0/1 - Training Loss:  2.241  - Validation loss:  8.223\n",
      "Epoch 561/2000 Batch    0/1 - Training Loss:  2.237  - Validation loss:  8.227\n",
      "Epoch 562/2000 Batch    0/1 - Training Loss:  2.231  - Validation loss:  8.234\n",
      "Epoch 563/2000 Batch    0/1 - Training Loss:  2.227  - Validation loss:  8.239\n",
      "Epoch 564/2000 Batch    0/1 - Training Loss:  2.223  - Validation loss:  8.244\n",
      "Epoch 565/2000 Batch    0/1 - Training Loss:  2.218  - Validation loss:  8.249\n",
      "Epoch 566/2000 Batch    0/1 - Training Loss:  2.214  - Validation loss:  8.254\n",
      "Epoch 567/2000 Batch    0/1 - Training Loss:  2.211  - Validation loss:  8.256\n",
      "Epoch 568/2000 Batch    0/1 - Training Loss:  2.207  - Validation loss:  8.263\n",
      "Epoch 569/2000 Batch    0/1 - Training Loss:  2.206  - Validation loss:  8.266\n",
      "Epoch 570/2000 Batch    0/1 - Training Loss:  2.203  - Validation loss:  8.274\n",
      "Epoch 571/2000 Batch    0/1 - Training Loss:  2.205  - Validation loss:  8.278\n",
      "Epoch 572/2000 Batch    0/1 - Training Loss:  2.202  - Validation loss:  8.284\n",
      "Epoch 573/2000 Batch    0/1 - Training Loss:  2.191  - Validation loss:  8.285\n",
      "Epoch 574/2000 Batch    0/1 - Training Loss:  2.180  - Validation loss:  8.291\n",
      "Epoch 575/2000 Batch    0/1 - Training Loss:  2.181  - Validation loss:  8.299\n",
      "Epoch 576/2000 Batch    0/1 - Training Loss:  2.177  - Validation loss:  8.298\n",
      "Epoch 577/2000 Batch    0/1 - Training Loss:  2.167  - Validation loss:  8.303\n",
      "Epoch 578/2000 Batch    0/1 - Training Loss:  2.163  - Validation loss:  8.312\n",
      "Epoch 579/2000 Batch    0/1 - Training Loss:  2.159  - Validation loss:  8.316\n",
      "Epoch 580/2000 Batch    0/1 - Training Loss:  2.158  - Validation loss:  8.318\n",
      "Epoch 581/2000 Batch    0/1 - Training Loss:  2.155  - Validation loss:  8.324\n",
      "Epoch 582/2000 Batch    0/1 - Training Loss:  2.145  - Validation loss:  8.332\n",
      "Epoch 583/2000 Batch    0/1 - Training Loss:  2.138  - Validation loss:  8.335\n",
      "Epoch 584/2000 Batch    0/1 - Training Loss:  2.135  - Validation loss:  8.337\n",
      "Epoch 585/2000 Batch    0/1 - Training Loss:  2.131  - Validation loss:  8.344\n",
      "Epoch 586/2000 Batch    0/1 - Training Loss:  2.127  - Validation loss:  8.348\n",
      "Epoch 587/2000 Batch    0/1 - Training Loss:  2.120  - Validation loss:  8.350\n",
      "Epoch 588/2000 Batch    0/1 - Training Loss:  2.116  - Validation loss:  8.355\n",
      "Epoch 589/2000 Batch    0/1 - Training Loss:  2.112  - Validation loss:  8.361\n",
      "Epoch 590/2000 Batch    0/1 - Training Loss:  2.109  - Validation loss:  8.362\n",
      "Epoch 591/2000 Batch    0/1 - Training Loss:  2.104  - Validation loss:  8.370\n",
      "Epoch 592/2000 Batch    0/1 - Training Loss:  2.098  - Validation loss:  8.375\n",
      "Epoch 593/2000 Batch    0/1 - Training Loss:  2.093  - Validation loss:  8.378\n",
      "Epoch 594/2000 Batch    0/1 - Training Loss:  2.089  - Validation loss:  8.384\n",
      "Epoch 595/2000 Batch    0/1 - Training Loss:  2.085  - Validation loss:  8.387\n",
      "Epoch 596/2000 Batch    0/1 - Training Loss:  2.082  - Validation loss:  8.391\n",
      "Epoch 597/2000 Batch    0/1 - Training Loss:  2.078  - Validation loss:  8.393\n",
      "Epoch 598/2000 Batch    0/1 - Training Loss:  2.074  - Validation loss:  8.400\n",
      "Epoch 599/2000 Batch    0/1 - Training Loss:  2.068  - Validation loss:  8.406\n",
      "Epoch 600/2000 Batch    0/1 - Training Loss:  2.063  - Validation loss:  8.409\n",
      "Epoch 601/2000 Batch    0/1 - Training Loss:  2.059  - Validation loss:  8.414\n",
      "Epoch 602/2000 Batch    0/1 - Training Loss:  2.055  - Validation loss:  8.419\n",
      "Epoch 603/2000 Batch    0/1 - Training Loss:  2.052  - Validation loss:  8.426\n",
      "Epoch 604/2000 Batch    0/1 - Training Loss:  2.048  - Validation loss:  8.427\n",
      "Epoch 605/2000 Batch    0/1 - Training Loss:  2.044  - Validation loss:  8.434\n",
      "Epoch 606/2000 Batch    0/1 - Training Loss:  2.038  - Validation loss:  8.441\n",
      "Epoch 607/2000 Batch    0/1 - Training Loss:  2.034  - Validation loss:  8.446\n",
      "Epoch 608/2000 Batch    0/1 - Training Loss:  2.029  - Validation loss:  8.450\n",
      "Epoch 609/2000 Batch    0/1 - Training Loss:  2.025  - Validation loss:  8.458\n",
      "Epoch 610/2000 Batch    0/1 - Training Loss:  2.020  - Validation loss:  8.463\n",
      "Epoch 611/2000 Batch    0/1 - Training Loss:  2.016  - Validation loss:  8.467\n",
      "Epoch 612/2000 Batch    0/1 - Training Loss:  2.012  - Validation loss:  8.472\n",
      "Epoch 613/2000 Batch    0/1 - Training Loss:  2.007  - Validation loss:  8.479\n",
      "Epoch 614/2000 Batch    0/1 - Training Loss:  2.003  - Validation loss:  8.483\n",
      "Epoch 615/2000 Batch    0/1 - Training Loss:  1.999  - Validation loss:  8.487\n",
      "Epoch 616/2000 Batch    0/1 - Training Loss:  1.995  - Validation loss:  8.494\n",
      "Epoch 617/2000 Batch    0/1 - Training Loss:  1.991  - Validation loss:  8.499\n",
      "Epoch 618/2000 Batch    0/1 - Training Loss:  1.988  - Validation loss:  8.504\n",
      "Epoch 619/2000 Batch    0/1 - Training Loss:  1.986  - Validation loss:  8.507\n",
      "Epoch 620/2000 Batch    0/1 - Training Loss:  1.989  - Validation loss:  8.515\n",
      "Epoch 621/2000 Batch    0/1 - Training Loss:  1.989  - Validation loss:  8.519\n",
      "Epoch 622/2000 Batch    0/1 - Training Loss:  1.992  - Validation loss:  8.526\n",
      "Epoch 623/2000 Batch    0/1 - Training Loss:  1.980  - Validation loss:  8.531\n",
      "Epoch 624/2000 Batch    0/1 - Training Loss:  1.967  - Validation loss:  8.538\n",
      "Epoch 625/2000 Batch    0/1 - Training Loss:  1.965  - Validation loss:  8.542\n",
      "Epoch 626/2000 Batch    0/1 - Training Loss:  1.966  - Validation loss:  8.545\n",
      "Epoch 627/2000 Batch    0/1 - Training Loss:  1.955  - Validation loss:  8.549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 628/2000 Batch    0/1 - Training Loss:  1.948  - Validation loss:  8.554\n",
      "Epoch 629/2000 Batch    0/1 - Training Loss:  1.952  - Validation loss:  8.556\n",
      "Epoch 630/2000 Batch    0/1 - Training Loss:  1.948  - Validation loss:  8.565\n",
      "Epoch 631/2000 Batch    0/1 - Training Loss:  1.938  - Validation loss:  8.570\n",
      "Epoch 632/2000 Batch    0/1 - Training Loss:  1.933  - Validation loss:  8.576\n",
      "Epoch 633/2000 Batch    0/1 - Training Loss:  1.929  - Validation loss:  8.585\n",
      "Epoch 634/2000 Batch    0/1 - Training Loss:  1.927  - Validation loss:  8.590\n",
      "Epoch 635/2000 Batch    0/1 - Training Loss:  1.927  - Validation loss:  8.595\n",
      "Epoch 636/2000 Batch    0/1 - Training Loss:  1.917  - Validation loss:  8.601\n",
      "Epoch 637/2000 Batch    0/1 - Training Loss:  1.913  - Validation loss:  8.606\n",
      "Epoch 638/2000 Batch    0/1 - Training Loss:  1.912  - Validation loss:  8.607\n",
      "Epoch 639/2000 Batch    0/1 - Training Loss:  1.904  - Validation loss:  8.611\n",
      "Epoch 640/2000 Batch    0/1 - Training Loss:  1.902  - Validation loss:  8.619\n",
      "Epoch 641/2000 Batch    0/1 - Training Loss:  1.896  - Validation loss:  8.621\n",
      "Epoch 642/2000 Batch    0/1 - Training Loss:  1.893  - Validation loss:  8.622\n",
      "Epoch 643/2000 Batch    0/1 - Training Loss:  1.890  - Validation loss:  8.631\n",
      "Epoch 644/2000 Batch    0/1 - Training Loss:  1.883  - Validation loss:  8.636\n",
      "Epoch 645/2000 Batch    0/1 - Training Loss:  1.880  - Validation loss:  8.637\n",
      "Epoch 646/2000 Batch    0/1 - Training Loss:  1.876  - Validation loss:  8.645\n",
      "Epoch 647/2000 Batch    0/1 - Training Loss:  1.871  - Validation loss:  8.652\n",
      "Epoch 648/2000 Batch    0/1 - Training Loss:  1.869  - Validation loss:  8.653\n",
      "Epoch 649/2000 Batch    0/1 - Training Loss:  1.864  - Validation loss:  8.659\n",
      "Epoch 650/2000 Batch    0/1 - Training Loss:  1.859  - Validation loss:  8.666\n",
      "Epoch 651/2000 Batch    0/1 - Training Loss:  1.856  - Validation loss:  8.670\n",
      "Epoch 652/2000 Batch    0/1 - Training Loss:  1.851  - Validation loss:  8.674\n",
      "Epoch 653/2000 Batch    0/1 - Training Loss:  1.847  - Validation loss:  8.680\n",
      "Epoch 654/2000 Batch    0/1 - Training Loss:  1.844  - Validation loss:  8.686\n",
      "Epoch 655/2000 Batch    0/1 - Training Loss:  1.839  - Validation loss:  8.690\n",
      "Epoch 656/2000 Batch    0/1 - Training Loss:  1.836  - Validation loss:  8.700\n",
      "Epoch 657/2000 Batch    0/1 - Training Loss:  1.835  - Validation loss:  8.702\n",
      "Epoch 658/2000 Batch    0/1 - Training Loss:  1.830  - Validation loss:  8.714\n",
      "Epoch 659/2000 Batch    0/1 - Training Loss:  1.828  - Validation loss:  8.716\n",
      "Epoch 660/2000 Batch    0/1 - Training Loss:  1.824  - Validation loss:  8.724\n",
      "Epoch 661/2000 Batch    0/1 - Training Loss:  1.819  - Validation loss:  8.726\n",
      "Epoch 662/2000 Batch    0/1 - Training Loss:  1.813  - Validation loss:  8.733\n",
      "Epoch 663/2000 Batch    0/1 - Training Loss:  1.809  - Validation loss:  8.738\n",
      "Epoch 664/2000 Batch    0/1 - Training Loss:  1.806  - Validation loss:  8.739\n",
      "Epoch 665/2000 Batch    0/1 - Training Loss:  1.803  - Validation loss:  8.750\n",
      "Epoch 666/2000 Batch    0/1 - Training Loss:  1.805  - Validation loss:  8.750\n",
      "Epoch 667/2000 Batch    0/1 - Training Loss:  1.803  - Validation loss:  8.761\n",
      "Epoch 668/2000 Batch    0/1 - Training Loss:  1.807  - Validation loss:  8.760\n",
      "Epoch 669/2000 Batch    0/1 - Training Loss:  1.800  - Validation loss:  8.773\n",
      "Epoch 670/2000 Batch    0/1 - Training Loss:  1.788  - Validation loss:  8.779\n",
      "Epoch 671/2000 Batch    0/1 - Training Loss:  1.781  - Validation loss:  8.779\n",
      "Epoch 672/2000 Batch    0/1 - Training Loss:  1.785  - Validation loss:  8.790\n",
      "Epoch 673/2000 Batch    0/1 - Training Loss:  1.778  - Validation loss:  8.790\n",
      "Epoch 674/2000 Batch    0/1 - Training Loss:  1.769  - Validation loss:  8.791\n",
      "Epoch 675/2000 Batch    0/1 - Training Loss:  1.768  - Validation loss:  8.799\n",
      "Epoch 676/2000 Batch    0/1 - Training Loss:  1.768  - Validation loss:  8.799\n",
      "Epoch 677/2000 Batch    0/1 - Training Loss:  1.760  - Validation loss:  8.806\n",
      "Epoch 678/2000 Batch    0/1 - Training Loss:  1.757  - Validation loss:  8.814\n",
      "Epoch 679/2000 Batch    0/1 - Training Loss:  1.750  - Validation loss:  8.820\n",
      "Epoch 680/2000 Batch    0/1 - Training Loss:  1.750  - Validation loss:  8.829\n",
      "Epoch 681/2000 Batch    0/1 - Training Loss:  1.752  - Validation loss:  8.832\n",
      "Epoch 682/2000 Batch    0/1 - Training Loss:  1.744  - Validation loss:  8.841\n",
      "Epoch 683/2000 Batch    0/1 - Training Loss:  1.739  - Validation loss:  8.845\n",
      "Epoch 684/2000 Batch    0/1 - Training Loss:  1.733  - Validation loss:  8.847\n",
      "Epoch 685/2000 Batch    0/1 - Training Loss:  1.732  - Validation loss:  8.855\n",
      "Epoch 686/2000 Batch    0/1 - Training Loss:  1.729  - Validation loss:  8.860\n",
      "Epoch 687/2000 Batch    0/1 - Training Loss:  1.721  - Validation loss:  8.865\n",
      "Epoch 688/2000 Batch    0/1 - Training Loss:  1.719  - Validation loss:  8.869\n",
      "Epoch 689/2000 Batch    0/1 - Training Loss:  1.716  - Validation loss:  8.872\n",
      "Epoch 690/2000 Batch    0/1 - Training Loss:  1.712  - Validation loss:  8.878\n",
      "Epoch 691/2000 Batch    0/1 - Training Loss:  1.715  - Validation loss:  8.883\n",
      "Epoch 692/2000 Batch    0/1 - Training Loss:  1.706  - Validation loss:  8.891\n",
      "Epoch 693/2000 Batch    0/1 - Training Loss:  1.701  - Validation loss:  8.898\n",
      "Epoch 694/2000 Batch    0/1 - Training Loss:  1.698  - Validation loss:  8.902\n",
      "Epoch 695/2000 Batch    0/1 - Training Loss:  1.692  - Validation loss:  8.906\n",
      "Epoch 696/2000 Batch    0/1 - Training Loss:  1.690  - Validation loss:  8.912\n",
      "Epoch 697/2000 Batch    0/1 - Training Loss:  1.685  - Validation loss:  8.917\n",
      "Epoch 698/2000 Batch    0/1 - Training Loss:  1.682  - Validation loss:  8.918\n",
      "Epoch 699/2000 Batch    0/1 - Training Loss:  1.679  - Validation loss:  8.923\n",
      "Epoch 700/2000 Batch    0/1 - Training Loss:  1.674  - Validation loss:  8.931\n",
      "Epoch 701/2000 Batch    0/1 - Training Loss:  1.671  - Validation loss:  8.933\n",
      "Epoch 702/2000 Batch    0/1 - Training Loss:  1.667  - Validation loss:  8.937\n",
      "Epoch 703/2000 Batch    0/1 - Training Loss:  1.663  - Validation loss:  8.945\n",
      "Epoch 704/2000 Batch    0/1 - Training Loss:  1.660  - Validation loss:  8.949\n",
      "Epoch 705/2000 Batch    0/1 - Training Loss:  1.656  - Validation loss:  8.951\n",
      "Epoch 706/2000 Batch    0/1 - Training Loss:  1.653  - Validation loss:  8.961\n",
      "Epoch 707/2000 Batch    0/1 - Training Loss:  1.650  - Validation loss:  8.965\n",
      "Epoch 708/2000 Batch    0/1 - Training Loss:  1.648  - Validation loss:  8.968\n",
      "Epoch 709/2000 Batch    0/1 - Training Loss:  1.645  - Validation loss:  8.972\n",
      "Epoch 710/2000 Batch    0/1 - Training Loss:  1.643  - Validation loss:  8.980\n",
      "Epoch 711/2000 Batch    0/1 - Training Loss:  1.638  - Validation loss:  8.983\n",
      "Epoch 712/2000 Batch    0/1 - Training Loss:  1.634  - Validation loss:  8.992\n",
      "Epoch 713/2000 Batch    0/1 - Training Loss:  1.630  - Validation loss:  8.995\n",
      "Epoch 714/2000 Batch    0/1 - Training Loss:  1.625  - Validation loss:  9.001\n",
      "Epoch 715/2000 Batch    0/1 - Training Loss:  1.621  - Validation loss:  9.007\n",
      "Epoch 716/2000 Batch    0/1 - Training Loss:  1.617  - Validation loss:  9.008\n",
      "Epoch 717/2000 Batch    0/1 - Training Loss:  1.615  - Validation loss:  9.021\n",
      "Epoch 718/2000 Batch    0/1 - Training Loss:  1.613  - Validation loss:  9.019\n",
      "Epoch 719/2000 Batch    0/1 - Training Loss:  1.616  - Validation loss:  9.035\n",
      "Epoch 720/2000 Batch    0/1 - Training Loss:  1.619  - Validation loss:  9.027\n",
      "Epoch 721/2000 Batch    0/1 - Training Loss:  1.630  - Validation loss:  9.052\n",
      "Epoch 722/2000 Batch    0/1 - Training Loss:  1.615  - Validation loss:  9.052\n",
      "Epoch 723/2000 Batch    0/1 - Training Loss:  1.600  - Validation loss:  9.064\n",
      "Epoch 724/2000 Batch    0/1 - Training Loss:  1.597  - Validation loss:  9.074\n",
      "Epoch 725/2000 Batch    0/1 - Training Loss:  1.601  - Validation loss:  9.063\n",
      "Epoch 726/2000 Batch    0/1 - Training Loss:  1.594  - Validation loss:  9.080\n",
      "Epoch 727/2000 Batch    0/1 - Training Loss:  1.582  - Validation loss:  9.090\n",
      "Epoch 728/2000 Batch    0/1 - Training Loss:  1.585  - Validation loss:  9.080\n",
      "Epoch 729/2000 Batch    0/1 - Training Loss:  1.585  - Validation loss:  9.087\n",
      "Epoch 730/2000 Batch    0/1 - Training Loss:  1.573  - Validation loss:  9.102\n",
      "Epoch 731/2000 Batch    0/1 - Training Loss:  1.574  - Validation loss:  9.099\n",
      "Epoch 732/2000 Batch    0/1 - Training Loss:  1.582  - Validation loss:  9.104\n",
      "Epoch 733/2000 Batch    0/1 - Training Loss:  1.568  - Validation loss:  9.114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 734/2000 Batch    0/1 - Training Loss:  1.558  - Validation loss:  9.126\n",
      "Epoch 735/2000 Batch    0/1 - Training Loss:  1.563  - Validation loss:  9.131\n",
      "Epoch 736/2000 Batch    0/1 - Training Loss:  1.559  - Validation loss:  9.123\n",
      "Epoch 737/2000 Batch    0/1 - Training Loss:  1.551  - Validation loss:  9.134\n",
      "Epoch 738/2000 Batch    0/1 - Training Loss:  1.545  - Validation loss:  9.149\n",
      "Epoch 739/2000 Batch    0/1 - Training Loss:  1.548  - Validation loss:  9.143\n",
      "Epoch 740/2000 Batch    0/1 - Training Loss:  1.542  - Validation loss:  9.145\n",
      "Epoch 741/2000 Batch    0/1 - Training Loss:  1.536  - Validation loss:  9.158\n",
      "Epoch 742/2000 Batch    0/1 - Training Loss:  1.534  - Validation loss:  9.163\n",
      "Epoch 743/2000 Batch    0/1 - Training Loss:  1.538  - Validation loss:  9.165\n",
      "Epoch 744/2000 Batch    0/1 - Training Loss:  1.529  - Validation loss:  9.168\n",
      "Epoch 745/2000 Batch    0/1 - Training Loss:  1.521  - Validation loss:  9.180\n",
      "Epoch 746/2000 Batch    0/1 - Training Loss:  1.520  - Validation loss:  9.187\n",
      "Epoch 747/2000 Batch    0/1 - Training Loss:  1.519  - Validation loss:  9.181\n",
      "Epoch 748/2000 Batch    0/1 - Training Loss:  1.516  - Validation loss:  9.186\n",
      "Epoch 749/2000 Batch    0/1 - Training Loss:  1.509  - Validation loss:  9.201\n",
      "Epoch 750/2000 Batch    0/1 - Training Loss:  1.506  - Validation loss:  9.205\n",
      "Epoch 751/2000 Batch    0/1 - Training Loss:  1.508  - Validation loss:  9.204\n",
      "Epoch 752/2000 Batch    0/1 - Training Loss:  1.501  - Validation loss:  9.209\n",
      "Epoch 753/2000 Batch    0/1 - Training Loss:  1.495  - Validation loss:  9.220\n",
      "Epoch 754/2000 Batch    0/1 - Training Loss:  1.495  - Validation loss:  9.224\n",
      "Epoch 755/2000 Batch    0/1 - Training Loss:  1.492  - Validation loss:  9.219\n",
      "Epoch 756/2000 Batch    0/1 - Training Loss:  1.489  - Validation loss:  9.227\n",
      "Epoch 757/2000 Batch    0/1 - Training Loss:  1.482  - Validation loss:  9.242\n",
      "Epoch 758/2000 Batch    0/1 - Training Loss:  1.481  - Validation loss:  9.243\n",
      "Epoch 759/2000 Batch    0/1 - Training Loss:  1.481  - Validation loss:  9.241\n",
      "Epoch 760/2000 Batch    0/1 - Training Loss:  1.477  - Validation loss:  9.252\n",
      "Epoch 761/2000 Batch    0/1 - Training Loss:  1.469  - Validation loss:  9.265\n",
      "Epoch 762/2000 Batch    0/1 - Training Loss:  1.469  - Validation loss:  9.263\n",
      "Epoch 763/2000 Batch    0/1 - Training Loss:  1.466  - Validation loss:  9.258\n",
      "Epoch 764/2000 Batch    0/1 - Training Loss:  1.467  - Validation loss:  9.270\n",
      "Epoch 765/2000 Batch    0/1 - Training Loss:  1.458  - Validation loss:  9.281\n",
      "Epoch 766/2000 Batch    0/1 - Training Loss:  1.455  - Validation loss:  9.278\n",
      "Epoch 767/2000 Batch    0/1 - Training Loss:  1.452  - Validation loss:  9.278\n",
      "Epoch 768/2000 Batch    0/1 - Training Loss:  1.450  - Validation loss:  9.291\n",
      "Epoch 769/2000 Batch    0/1 - Training Loss:  1.444  - Validation loss:  9.300\n",
      "Epoch 770/2000 Batch    0/1 - Training Loss:  1.442  - Validation loss:  9.295\n",
      "Epoch 771/2000 Batch    0/1 - Training Loss:  1.438  - Validation loss:  9.299\n",
      "Epoch 772/2000 Batch    0/1 - Training Loss:  1.436  - Validation loss:  9.310\n",
      "Epoch 773/2000 Batch    0/1 - Training Loss:  1.432  - Validation loss:  9.316\n",
      "Epoch 774/2000 Batch    0/1 - Training Loss:  1.429  - Validation loss:  9.314\n",
      "Epoch 775/2000 Batch    0/1 - Training Loss:  1.425  - Validation loss:  9.320\n",
      "Epoch 776/2000 Batch    0/1 - Training Loss:  1.422  - Validation loss:  9.331\n",
      "Epoch 777/2000 Batch    0/1 - Training Loss:  1.419  - Validation loss:  9.334\n",
      "Epoch 778/2000 Batch    0/1 - Training Loss:  1.415  - Validation loss:  9.334\n",
      "Epoch 779/2000 Batch    0/1 - Training Loss:  1.413  - Validation loss:  9.341\n",
      "Epoch 780/2000 Batch    0/1 - Training Loss:  1.409  - Validation loss:  9.350\n",
      "Epoch 781/2000 Batch    0/1 - Training Loss:  1.406  - Validation loss:  9.350\n",
      "Epoch 782/2000 Batch    0/1 - Training Loss:  1.403  - Validation loss:  9.352\n",
      "Epoch 783/2000 Batch    0/1 - Training Loss:  1.400  - Validation loss:  9.361\n",
      "Epoch 784/2000 Batch    0/1 - Training Loss:  1.398  - Validation loss:  9.364\n",
      "Epoch 785/2000 Batch    0/1 - Training Loss:  1.396  - Validation loss:  9.366\n",
      "Epoch 786/2000 Batch    0/1 - Training Loss:  1.396  - Validation loss:  9.367\n",
      "Epoch 787/2000 Batch    0/1 - Training Loss:  1.393  - Validation loss:  9.380\n",
      "Epoch 788/2000 Batch    0/1 - Training Loss:  1.389  - Validation loss:  9.380\n",
      "Epoch 789/2000 Batch    0/1 - Training Loss:  1.382  - Validation loss:  9.381\n",
      "Epoch 790/2000 Batch    0/1 - Training Loss:  1.379  - Validation loss:  9.389\n",
      "Epoch 791/2000 Batch    0/1 - Training Loss:  1.377  - Validation loss:  9.391\n",
      "Epoch 792/2000 Batch    0/1 - Training Loss:  1.377  - Validation loss:  9.394\n",
      "Epoch 793/2000 Batch    0/1 - Training Loss:  1.378  - Validation loss:  9.391\n",
      "Epoch 794/2000 Batch    0/1 - Training Loss:  1.374  - Validation loss:  9.405\n",
      "Epoch 795/2000 Batch    0/1 - Training Loss:  1.367  - Validation loss:  9.404\n",
      "Epoch 796/2000 Batch    0/1 - Training Loss:  1.361  - Validation loss:  9.404\n",
      "Epoch 797/2000 Batch    0/1 - Training Loss:  1.360  - Validation loss:  9.417\n",
      "Epoch 798/2000 Batch    0/1 - Training Loss:  1.360  - Validation loss:  9.417\n",
      "Epoch 799/2000 Batch    0/1 - Training Loss:  1.357  - Validation loss:  9.421\n",
      "Epoch 800/2000 Batch    0/1 - Training Loss:  1.354  - Validation loss:  9.421\n",
      "Epoch 801/2000 Batch    0/1 - Training Loss:  1.348  - Validation loss:  9.430\n",
      "Epoch 802/2000 Batch    0/1 - Training Loss:  1.343  - Validation loss:  9.432\n",
      "Epoch 803/2000 Batch    0/1 - Training Loss:  1.340  - Validation loss:  9.432\n",
      "Epoch 804/2000 Batch    0/1 - Training Loss:  1.339  - Validation loss:  9.442\n",
      "Epoch 805/2000 Batch    0/1 - Training Loss:  1.337  - Validation loss:  9.445\n",
      "Epoch 806/2000 Batch    0/1 - Training Loss:  1.334  - Validation loss:  9.450\n",
      "Epoch 807/2000 Batch    0/1 - Training Loss:  1.331  - Validation loss:  9.449\n",
      "Epoch 808/2000 Batch    0/1 - Training Loss:  1.325  - Validation loss:  9.461\n",
      "Epoch 809/2000 Batch    0/1 - Training Loss:  1.321  - Validation loss:  9.466\n",
      "Epoch 810/2000 Batch    0/1 - Training Loss:  1.319  - Validation loss:  9.461\n",
      "Epoch 811/2000 Batch    0/1 - Training Loss:  1.318  - Validation loss:  9.472\n",
      "Epoch 812/2000 Batch    0/1 - Training Loss:  1.315  - Validation loss:  9.474\n",
      "Epoch 813/2000 Batch    0/1 - Training Loss:  1.313  - Validation loss:  9.478\n",
      "Epoch 814/2000 Batch    0/1 - Training Loss:  1.312  - Validation loss:  9.474\n",
      "Epoch 815/2000 Batch    0/1 - Training Loss:  1.307  - Validation loss:  9.489\n",
      "Epoch 816/2000 Batch    0/1 - Training Loss:  1.301  - Validation loss:  9.491\n",
      "Epoch 817/2000 Batch    0/1 - Training Loss:  1.297  - Validation loss:  9.490\n",
      "Epoch 818/2000 Batch    0/1 - Training Loss:  1.296  - Validation loss:  9.502\n",
      "Epoch 819/2000 Batch    0/1 - Training Loss:  1.295  - Validation loss:  9.504\n",
      "Epoch 820/2000 Batch    0/1 - Training Loss:  1.292  - Validation loss:  9.508\n",
      "Epoch 821/2000 Batch    0/1 - Training Loss:  1.290  - Validation loss:  9.503\n",
      "Epoch 822/2000 Batch    0/1 - Training Loss:  1.285  - Validation loss:  9.516\n",
      "Epoch 823/2000 Batch    0/1 - Training Loss:  1.280  - Validation loss:  9.520\n",
      "Epoch 824/2000 Batch    0/1 - Training Loss:  1.277  - Validation loss:  9.515\n",
      "Epoch 825/2000 Batch    0/1 - Training Loss:  1.277  - Validation loss:  9.530\n",
      "Epoch 826/2000 Batch    0/1 - Training Loss:  1.275  - Validation loss:  9.531\n",
      "Epoch 827/2000 Batch    0/1 - Training Loss:  1.274  - Validation loss:  9.536\n",
      "Epoch 828/2000 Batch    0/1 - Training Loss:  1.271  - Validation loss:  9.531\n",
      "Epoch 829/2000 Batch    0/1 - Training Loss:  1.266  - Validation loss:  9.546\n",
      "Epoch 830/2000 Batch    0/1 - Training Loss:  1.260  - Validation loss:  9.552\n",
      "Epoch 831/2000 Batch    0/1 - Training Loss:  1.258  - Validation loss:  9.546\n",
      "Epoch 832/2000 Batch    0/1 - Training Loss:  1.258  - Validation loss:  9.561\n",
      "Epoch 833/2000 Batch    0/1 - Training Loss:  1.255  - Validation loss:  9.564\n",
      "Epoch 834/2000 Batch    0/1 - Training Loss:  1.251  - Validation loss:  9.566\n",
      "Epoch 835/2000 Batch    0/1 - Training Loss:  1.246  - Validation loss:  9.564\n",
      "Epoch 836/2000 Batch    0/1 - Training Loss:  1.243  - Validation loss:  9.576\n",
      "Epoch 837/2000 Batch    0/1 - Training Loss:  1.239  - Validation loss:  9.584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 838/2000 Batch    0/1 - Training Loss:  1.237  - Validation loss:  9.579\n",
      "Epoch 839/2000 Batch    0/1 - Training Loss:  1.235  - Validation loss:  9.591\n",
      "Epoch 840/2000 Batch    0/1 - Training Loss:  1.232  - Validation loss:  9.596\n",
      "Epoch 841/2000 Batch    0/1 - Training Loss:  1.230  - Validation loss:  9.601\n",
      "Epoch 842/2000 Batch    0/1 - Training Loss:  1.228  - Validation loss:  9.595\n",
      "Epoch 843/2000 Batch    0/1 - Training Loss:  1.226  - Validation loss:  9.613\n",
      "Epoch 844/2000 Batch    0/1 - Training Loss:  1.221  - Validation loss:  9.614\n",
      "Epoch 845/2000 Batch    0/1 - Training Loss:  1.216  - Validation loss:  9.614\n",
      "Epoch 846/2000 Batch    0/1 - Training Loss:  1.214  - Validation loss:  9.627\n",
      "Epoch 847/2000 Batch    0/1 - Training Loss:  1.212  - Validation loss:  9.629\n",
      "Epoch 848/2000 Batch    0/1 - Training Loss:  1.212  - Validation loss:  9.636\n",
      "Epoch 849/2000 Batch    0/1 - Training Loss:  1.212  - Validation loss:  9.629\n",
      "Epoch 850/2000 Batch    0/1 - Training Loss:  1.208  - Validation loss:  9.647\n",
      "Epoch 851/2000 Batch    0/1 - Training Loss:  1.201  - Validation loss:  9.649\n",
      "Epoch 852/2000 Batch    0/1 - Training Loss:  1.196  - Validation loss:  9.646\n",
      "Epoch 853/2000 Batch    0/1 - Training Loss:  1.197  - Validation loss:  9.663\n",
      "Epoch 854/2000 Batch    0/1 - Training Loss:  1.195  - Validation loss:  9.662\n",
      "Epoch 855/2000 Batch    0/1 - Training Loss:  1.191  - Validation loss:  9.666\n",
      "Epoch 856/2000 Batch    0/1 - Training Loss:  1.188  - Validation loss:  9.666\n",
      "Epoch 857/2000 Batch    0/1 - Training Loss:  1.184  - Validation loss:  9.678\n",
      "Epoch 858/2000 Batch    0/1 - Training Loss:  1.181  - Validation loss:  9.681\n",
      "Epoch 859/2000 Batch    0/1 - Training Loss:  1.177  - Validation loss:  9.682\n",
      "Epoch 860/2000 Batch    0/1 - Training Loss:  1.176  - Validation loss:  9.695\n",
      "Epoch 861/2000 Batch    0/1 - Training Loss:  1.173  - Validation loss:  9.697\n",
      "Epoch 862/2000 Batch    0/1 - Training Loss:  1.171  - Validation loss:  9.701\n",
      "Epoch 863/2000 Batch    0/1 - Training Loss:  1.170  - Validation loss:  9.699\n",
      "Epoch 864/2000 Batch    0/1 - Training Loss:  1.166  - Validation loss:  9.716\n",
      "Epoch 865/2000 Batch    0/1 - Training Loss:  1.162  - Validation loss:  9.716\n",
      "Epoch 866/2000 Batch    0/1 - Training Loss:  1.158  - Validation loss:  9.716\n",
      "Epoch 867/2000 Batch    0/1 - Training Loss:  1.156  - Validation loss:  9.730\n",
      "Epoch 868/2000 Batch    0/1 - Training Loss:  1.154  - Validation loss:  9.730\n",
      "Epoch 869/2000 Batch    0/1 - Training Loss:  1.153  - Validation loss:  9.736\n",
      "Epoch 870/2000 Batch    0/1 - Training Loss:  1.155  - Validation loss:  9.731\n",
      "Epoch 871/2000 Batch    0/1 - Training Loss:  1.152  - Validation loss:  9.750\n",
      "Epoch 872/2000 Batch    0/1 - Training Loss:  1.148  - Validation loss:  9.747\n",
      "Epoch 873/2000 Batch    0/1 - Training Loss:  1.141  - Validation loss:  9.749\n",
      "Epoch 874/2000 Batch    0/1 - Training Loss:  1.138  - Validation loss:  9.765\n",
      "Epoch 875/2000 Batch    0/1 - Training Loss:  1.138  - Validation loss:  9.763\n",
      "Epoch 876/2000 Batch    0/1 - Training Loss:  1.136  - Validation loss:  9.768\n",
      "Epoch 877/2000 Batch    0/1 - Training Loss:  1.136  - Validation loss:  9.766\n",
      "Epoch 878/2000 Batch    0/1 - Training Loss:  1.128  - Validation loss:  9.781\n",
      "Epoch 879/2000 Batch    0/1 - Training Loss:  1.125  - Validation loss:  9.782\n",
      "Epoch 880/2000 Batch    0/1 - Training Loss:  1.123  - Validation loss:  9.779\n",
      "Epoch 881/2000 Batch    0/1 - Training Loss:  1.123  - Validation loss:  9.797\n",
      "Epoch 882/2000 Batch    0/1 - Training Loss:  1.119  - Validation loss:  9.797\n",
      "Epoch 883/2000 Batch    0/1 - Training Loss:  1.116  - Validation loss:  9.798\n",
      "Epoch 884/2000 Batch    0/1 - Training Loss:  1.113  - Validation loss:  9.800\n",
      "Epoch 885/2000 Batch    0/1 - Training Loss:  1.108  - Validation loss:  9.812\n",
      "Epoch 886/2000 Batch    0/1 - Training Loss:  1.107  - Validation loss:  9.816\n",
      "Epoch 887/2000 Batch    0/1 - Training Loss:  1.103  - Validation loss:  9.813\n",
      "Epoch 888/2000 Batch    0/1 - Training Loss:  1.102  - Validation loss:  9.827\n",
      "Epoch 889/2000 Batch    0/1 - Training Loss:  1.098  - Validation loss:  9.830\n",
      "Epoch 890/2000 Batch    0/1 - Training Loss:  1.096  - Validation loss:  9.829\n",
      "Epoch 891/2000 Batch    0/1 - Training Loss:  1.093  - Validation loss:  9.833\n",
      "Epoch 892/2000 Batch    0/1 - Training Loss:  1.090  - Validation loss:  9.845\n",
      "Epoch 893/2000 Batch    0/1 - Training Loss:  1.088  - Validation loss:  9.845\n",
      "Epoch 894/2000 Batch    0/1 - Training Loss:  1.084  - Validation loss:  9.849\n",
      "Epoch 895/2000 Batch    0/1 - Training Loss:  1.082  - Validation loss:  9.859\n",
      "Epoch 896/2000 Batch    0/1 - Training Loss:  1.079  - Validation loss:  9.863\n",
      "Epoch 897/2000 Batch    0/1 - Training Loss:  1.077  - Validation loss:  9.863\n",
      "Epoch 898/2000 Batch    0/1 - Training Loss:  1.074  - Validation loss:  9.869\n",
      "Epoch 899/2000 Batch    0/1 - Training Loss:  1.071  - Validation loss:  9.877\n",
      "Epoch 900/2000 Batch    0/1 - Training Loss:  1.070  - Validation loss:  9.877\n",
      "Epoch 901/2000 Batch    0/1 - Training Loss:  1.067  - Validation loss:  9.884\n",
      "Epoch 902/2000 Batch    0/1 - Training Loss:  1.065  - Validation loss:  9.891\n",
      "Epoch 903/2000 Batch    0/1 - Training Loss:  1.063  - Validation loss:  9.896\n",
      "Epoch 904/2000 Batch    0/1 - Training Loss:  1.062  - Validation loss:  9.894\n",
      "Epoch 905/2000 Batch    0/1 - Training Loss:  1.061  - Validation loss:  9.908\n",
      "Epoch 906/2000 Batch    0/1 - Training Loss:  1.060  - Validation loss:  9.906\n",
      "Epoch 907/2000 Batch    0/1 - Training Loss:  1.056  - Validation loss:  9.914\n",
      "Epoch 908/2000 Batch    0/1 - Training Loss:  1.052  - Validation loss:  9.917\n",
      "Epoch 909/2000 Batch    0/1 - Training Loss:  1.047  - Validation loss:  9.923\n",
      "Epoch 910/2000 Batch    0/1 - Training Loss:  1.044  - Validation loss:  9.929\n",
      "Epoch 911/2000 Batch    0/1 - Training Loss:  1.044  - Validation loss:  9.926\n",
      "Epoch 912/2000 Batch    0/1 - Training Loss:  1.044  - Validation loss:  9.938\n",
      "Epoch 913/2000 Batch    0/1 - Training Loss:  1.046  - Validation loss:  9.937\n",
      "Epoch 914/2000 Batch    0/1 - Training Loss:  1.045  - Validation loss:  9.948\n",
      "Epoch 915/2000 Batch    0/1 - Training Loss:  1.042  - Validation loss:  9.945\n",
      "Epoch 916/2000 Batch    0/1 - Training Loss:  1.033  - Validation loss:  9.956\n",
      "Epoch 917/2000 Batch    0/1 - Training Loss:  1.030  - Validation loss:  9.964\n",
      "Epoch 918/2000 Batch    0/1 - Training Loss:  1.030  - Validation loss:  9.954\n",
      "Epoch 919/2000 Batch    0/1 - Training Loss:  1.028  - Validation loss:  9.966\n",
      "Epoch 920/2000 Batch    0/1 - Training Loss:  1.022  - Validation loss:  9.973\n",
      "Epoch 921/2000 Batch    0/1 - Training Loss:  1.020  - Validation loss:  9.970\n",
      "Epoch 922/2000 Batch    0/1 - Training Loss:  1.018  - Validation loss:  9.979\n",
      "Epoch 923/2000 Batch    0/1 - Training Loss:  1.017  - Validation loss:  9.984\n",
      "Epoch 924/2000 Batch    0/1 - Training Loss:  1.017  - Validation loss:  9.996\n",
      "Epoch 925/2000 Batch    0/1 - Training Loss:  1.015  - Validation loss:  9.988\n",
      "Epoch 926/2000 Batch    0/1 - Training Loss:  1.010  - Validation loss: 10.001\n",
      "Epoch 927/2000 Batch    0/1 - Training Loss:  1.005  - Validation loss: 10.009\n",
      "Epoch 928/2000 Batch    0/1 - Training Loss:  1.003  - Validation loss: 10.003\n",
      "Epoch 929/2000 Batch    0/1 - Training Loss:  1.003  - Validation loss: 10.014\n",
      "Epoch 930/2000 Batch    0/1 - Training Loss:  1.000  - Validation loss: 10.017\n",
      "Epoch 931/2000 Batch    0/1 - Training Loss:  0.994  - Validation loss: 10.021\n",
      "Epoch 932/2000 Batch    0/1 - Training Loss:  0.991  - Validation loss: 10.025\n",
      "Epoch 933/2000 Batch    0/1 - Training Loss:  0.990  - Validation loss: 10.027\n",
      "Epoch 934/2000 Batch    0/1 - Training Loss:  0.988  - Validation loss: 10.039\n",
      "Epoch 935/2000 Batch    0/1 - Training Loss:  0.987  - Validation loss: 10.035\n",
      "Epoch 936/2000 Batch    0/1 - Training Loss:  0.984  - Validation loss: 10.043\n",
      "Epoch 937/2000 Batch    0/1 - Training Loss:  0.980  - Validation loss: 10.048\n",
      "Epoch 938/2000 Batch    0/1 - Training Loss:  0.977  - Validation loss: 10.052\n",
      "Epoch 939/2000 Batch    0/1 - Training Loss:  0.975  - Validation loss: 10.058\n",
      "Epoch 940/2000 Batch    0/1 - Training Loss:  0.974  - Validation loss: 10.056\n",
      "Epoch 941/2000 Batch    0/1 - Training Loss:  0.971  - Validation loss: 10.067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 942/2000 Batch    0/1 - Training Loss:  0.968  - Validation loss: 10.066\n",
      "Epoch 943/2000 Batch    0/1 - Training Loss:  0.965  - Validation loss: 10.070\n",
      "Epoch 944/2000 Batch    0/1 - Training Loss:  0.962  - Validation loss: 10.078\n",
      "Epoch 945/2000 Batch    0/1 - Training Loss:  0.960  - Validation loss: 10.080\n",
      "Epoch 946/2000 Batch    0/1 - Training Loss:  0.958  - Validation loss: 10.085\n",
      "Epoch 947/2000 Batch    0/1 - Training Loss:  0.956  - Validation loss: 10.087\n",
      "Epoch 948/2000 Batch    0/1 - Training Loss:  0.954  - Validation loss: 10.099\n",
      "Epoch 949/2000 Batch    0/1 - Training Loss:  0.953  - Validation loss: 10.094\n",
      "Epoch 950/2000 Batch    0/1 - Training Loss:  0.950  - Validation loss: 10.103\n",
      "Epoch 951/2000 Batch    0/1 - Training Loss:  0.946  - Validation loss: 10.107\n",
      "Epoch 952/2000 Batch    0/1 - Training Loss:  0.943  - Validation loss: 10.107\n",
      "Epoch 953/2000 Batch    0/1 - Training Loss:  0.941  - Validation loss: 10.114\n",
      "Epoch 954/2000 Batch    0/1 - Training Loss:  0.940  - Validation loss: 10.114\n",
      "Epoch 955/2000 Batch    0/1 - Training Loss:  0.941  - Validation loss: 10.126\n",
      "Epoch 956/2000 Batch    0/1 - Training Loss:  0.944  - Validation loss: 10.117\n",
      "Epoch 957/2000 Batch    0/1 - Training Loss:  0.945  - Validation loss: 10.137\n",
      "Epoch 958/2000 Batch    0/1 - Training Loss:  0.938  - Validation loss: 10.130\n",
      "Epoch 959/2000 Batch    0/1 - Training Loss:  0.930  - Validation loss: 10.132\n",
      "Epoch 960/2000 Batch    0/1 - Training Loss:  0.929  - Validation loss: 10.148\n",
      "Epoch 961/2000 Batch    0/1 - Training Loss:  0.930  - Validation loss: 10.138\n",
      "Epoch 962/2000 Batch    0/1 - Training Loss:  0.926  - Validation loss: 10.147\n",
      "Epoch 963/2000 Batch    0/1 - Training Loss:  0.920  - Validation loss: 10.153\n",
      "Epoch 964/2000 Batch    0/1 - Training Loss:  0.917  - Validation loss: 10.154\n",
      "Epoch 965/2000 Batch    0/1 - Training Loss:  0.916  - Validation loss: 10.163\n",
      "Epoch 966/2000 Batch    0/1 - Training Loss:  0.918  - Validation loss: 10.161\n",
      "Epoch 967/2000 Batch    0/1 - Training Loss:  0.918  - Validation loss: 10.178\n",
      "Epoch 968/2000 Batch    0/1 - Training Loss:  0.914  - Validation loss: 10.170\n",
      "Epoch 969/2000 Batch    0/1 - Training Loss:  0.909  - Validation loss: 10.176\n",
      "Epoch 970/2000 Batch    0/1 - Training Loss:  0.905  - Validation loss: 10.190\n",
      "Epoch 971/2000 Batch    0/1 - Training Loss:  0.906  - Validation loss: 10.181\n",
      "Epoch 972/2000 Batch    0/1 - Training Loss:  0.903  - Validation loss: 10.189\n",
      "Epoch 973/2000 Batch    0/1 - Training Loss:  0.897  - Validation loss: 10.196\n",
      "Epoch 974/2000 Batch    0/1 - Training Loss:  0.896  - Validation loss: 10.194\n",
      "Epoch 975/2000 Batch    0/1 - Training Loss:  0.893  - Validation loss: 10.201\n",
      "Epoch 976/2000 Batch    0/1 - Training Loss:  0.895  - Validation loss: 10.203\n",
      "Epoch 977/2000 Batch    0/1 - Training Loss:  0.903  - Validation loss: 10.217\n",
      "Epoch 978/2000 Batch    0/1 - Training Loss:  0.900  - Validation loss: 10.206\n",
      "Epoch 979/2000 Batch    0/1 - Training Loss:  0.893  - Validation loss: 10.221\n",
      "Epoch 980/2000 Batch    0/1 - Training Loss:  0.887  - Validation loss: 10.232\n",
      "Epoch 981/2000 Batch    0/1 - Training Loss:  0.889  - Validation loss: 10.219\n",
      "Epoch 982/2000 Batch    0/1 - Training Loss:  0.886  - Validation loss: 10.228\n",
      "Epoch 983/2000 Batch    0/1 - Training Loss:  0.879  - Validation loss: 10.238\n",
      "Epoch 984/2000 Batch    0/1 - Training Loss:  0.882  - Validation loss: 10.233\n",
      "Epoch 985/2000 Batch    0/1 - Training Loss:  0.873  - Validation loss: 10.239\n",
      "Epoch 986/2000 Batch    0/1 - Training Loss:  0.874  - Validation loss: 10.242\n",
      "Epoch 987/2000 Batch    0/1 - Training Loss:  0.869  - Validation loss: 10.251\n",
      "Epoch 988/2000 Batch    0/1 - Training Loss:  0.869  - Validation loss: 10.256\n",
      "Epoch 989/2000 Batch    0/1 - Training Loss:  0.863  - Validation loss: 10.252\n",
      "Epoch 990/2000 Batch    0/1 - Training Loss:  0.862  - Validation loss: 10.265\n",
      "Epoch 991/2000 Batch    0/1 - Training Loss:  0.860  - Validation loss: 10.271\n",
      "Epoch 992/2000 Batch    0/1 - Training Loss:  0.854  - Validation loss: 10.267\n",
      "Epoch 993/2000 Batch    0/1 - Training Loss:  0.855  - Validation loss: 10.277\n",
      "Epoch 994/2000 Batch    0/1 - Training Loss:  0.852  - Validation loss: 10.280\n",
      "Epoch 995/2000 Batch    0/1 - Training Loss:  0.849  - Validation loss: 10.281\n",
      "Epoch 996/2000 Batch    0/1 - Training Loss:  0.845  - Validation loss: 10.285\n",
      "Epoch 997/2000 Batch    0/1 - Training Loss:  0.845  - Validation loss: 10.284\n",
      "Epoch 998/2000 Batch    0/1 - Training Loss:  0.841  - Validation loss: 10.291\n",
      "Epoch 999/2000 Batch    0/1 - Training Loss:  0.838  - Validation loss: 10.291\n",
      "Epoch 1000/2000 Batch    0/1 - Training Loss:  0.836  - Validation loss: 10.293\n",
      "Epoch 1001/2000 Batch    0/1 - Training Loss:  0.834  - Validation loss: 10.306\n",
      "Epoch 1002/2000 Batch    0/1 - Training Loss:  0.832  - Validation loss: 10.307\n",
      "Epoch 1003/2000 Batch    0/1 - Training Loss:  0.829  - Validation loss: 10.309\n",
      "Epoch 1004/2000 Batch    0/1 - Training Loss:  0.828  - Validation loss: 10.316\n",
      "Epoch 1005/2000 Batch    0/1 - Training Loss:  0.824  - Validation loss: 10.317\n",
      "Epoch 1006/2000 Batch    0/1 - Training Loss:  0.822  - Validation loss: 10.318\n",
      "Epoch 1007/2000 Batch    0/1 - Training Loss:  0.820  - Validation loss: 10.322\n",
      "Epoch 1008/2000 Batch    0/1 - Training Loss:  0.818  - Validation loss: 10.328\n",
      "Epoch 1009/2000 Batch    0/1 - Training Loss:  0.815  - Validation loss: 10.330\n",
      "Epoch 1010/2000 Batch    0/1 - Training Loss:  0.813  - Validation loss: 10.334\n",
      "Epoch 1011/2000 Batch    0/1 - Training Loss:  0.811  - Validation loss: 10.340\n",
      "Epoch 1012/2000 Batch    0/1 - Training Loss:  0.809  - Validation loss: 10.343\n",
      "Epoch 1013/2000 Batch    0/1 - Training Loss:  0.806  - Validation loss: 10.346\n",
      "Epoch 1014/2000 Batch    0/1 - Training Loss:  0.804  - Validation loss: 10.349\n",
      "Epoch 1015/2000 Batch    0/1 - Training Loss:  0.802  - Validation loss: 10.357\n",
      "Epoch 1016/2000 Batch    0/1 - Training Loss:  0.800  - Validation loss: 10.356\n",
      "Epoch 1017/2000 Batch    0/1 - Training Loss:  0.797  - Validation loss: 10.361\n",
      "Epoch 1018/2000 Batch    0/1 - Training Loss:  0.795  - Validation loss: 10.366\n",
      "Epoch 1019/2000 Batch    0/1 - Training Loss:  0.793  - Validation loss: 10.368\n",
      "Epoch 1020/2000 Batch    0/1 - Training Loss:  0.790  - Validation loss: 10.370\n",
      "Epoch 1021/2000 Batch    0/1 - Training Loss:  0.788  - Validation loss: 10.372\n",
      "Epoch 1022/2000 Batch    0/1 - Training Loss:  0.786  - Validation loss: 10.375\n",
      "Epoch 1023/2000 Batch    0/1 - Training Loss:  0.784  - Validation loss: 10.375\n",
      "Epoch 1024/2000 Batch    0/1 - Training Loss:  0.782  - Validation loss: 10.380\n",
      "Epoch 1025/2000 Batch    0/1 - Training Loss:  0.779  - Validation loss: 10.386\n",
      "Epoch 1026/2000 Batch    0/1 - Training Loss:  0.777  - Validation loss: 10.386\n",
      "Epoch 1027/2000 Batch    0/1 - Training Loss:  0.775  - Validation loss: 10.393\n",
      "Epoch 1028/2000 Batch    0/1 - Training Loss:  0.773  - Validation loss: 10.398\n",
      "Epoch 1029/2000 Batch    0/1 - Training Loss:  0.771  - Validation loss: 10.400\n",
      "Epoch 1030/2000 Batch    0/1 - Training Loss:  0.769  - Validation loss: 10.408\n",
      "Epoch 1031/2000 Batch    0/1 - Training Loss:  0.767  - Validation loss: 10.409\n",
      "Epoch 1032/2000 Batch    0/1 - Training Loss:  0.765  - Validation loss: 10.419\n",
      "Epoch 1033/2000 Batch    0/1 - Training Loss:  0.763  - Validation loss: 10.418\n",
      "Epoch 1034/2000 Batch    0/1 - Training Loss:  0.760  - Validation loss: 10.425\n",
      "Epoch 1035/2000 Batch    0/1 - Training Loss:  0.758  - Validation loss: 10.428\n",
      "Epoch 1036/2000 Batch    0/1 - Training Loss:  0.757  - Validation loss: 10.432\n",
      "Epoch 1037/2000 Batch    0/1 - Training Loss:  0.756  - Validation loss: 10.431\n",
      "Epoch 1038/2000 Batch    0/1 - Training Loss:  0.756  - Validation loss: 10.446\n",
      "Epoch 1039/2000 Batch    0/1 - Training Loss:  0.755  - Validation loss: 10.432\n",
      "Epoch 1040/2000 Batch    0/1 - Training Loss:  0.754  - Validation loss: 10.456\n",
      "Epoch 1041/2000 Batch    0/1 - Training Loss:  0.749  - Validation loss: 10.446\n",
      "Epoch 1042/2000 Batch    0/1 - Training Loss:  0.744  - Validation loss: 10.451\n",
      "Epoch 1043/2000 Batch    0/1 - Training Loss:  0.741  - Validation loss: 10.465\n",
      "Epoch 1044/2000 Batch    0/1 - Training Loss:  0.742  - Validation loss: 10.453\n",
      "Epoch 1045/2000 Batch    0/1 - Training Loss:  0.743  - Validation loss: 10.473\n",
      "Epoch 1046/2000 Batch    0/1 - Training Loss:  0.745  - Validation loss: 10.461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1047/2000 Batch    0/1 - Training Loss:  0.749  - Validation loss: 10.478\n",
      "Epoch 1048/2000 Batch    0/1 - Training Loss:  0.746  - Validation loss: 10.475\n",
      "Epoch 1049/2000 Batch    0/1 - Training Loss:  0.739  - Validation loss: 10.480\n",
      "Epoch 1050/2000 Batch    0/1 - Training Loss:  0.731  - Validation loss: 10.495\n",
      "Epoch 1051/2000 Batch    0/1 - Training Loss:  0.737  - Validation loss: 10.483\n",
      "Epoch 1052/2000 Batch    0/1 - Training Loss:  0.733  - Validation loss: 10.497\n",
      "Epoch 1053/2000 Batch    0/1 - Training Loss:  0.722  - Validation loss: 10.502\n",
      "Epoch 1054/2000 Batch    0/1 - Training Loss:  0.728  - Validation loss: 10.499\n",
      "Epoch 1055/2000 Batch    0/1 - Training Loss:  0.724  - Validation loss: 10.507\n",
      "Epoch 1056/2000 Batch    0/1 - Training Loss:  0.723  - Validation loss: 10.507\n",
      "Epoch 1057/2000 Batch    0/1 - Training Loss:  0.730  - Validation loss: 10.523\n",
      "Epoch 1058/2000 Batch    0/1 - Training Loss:  0.719  - Validation loss: 10.509\n",
      "Epoch 1059/2000 Batch    0/1 - Training Loss:  0.721  - Validation loss: 10.519\n",
      "Epoch 1060/2000 Batch    0/1 - Training Loss:  0.711  - Validation loss: 10.539\n",
      "Epoch 1061/2000 Batch    0/1 - Training Loss:  0.717  - Validation loss: 10.519\n",
      "Epoch 1062/2000 Batch    0/1 - Training Loss:  0.709  - Validation loss: 10.524\n",
      "Epoch 1063/2000 Batch    0/1 - Training Loss:  0.708  - Validation loss: 10.537\n",
      "Epoch 1064/2000 Batch    0/1 - Training Loss:  0.703  - Validation loss: 10.538\n",
      "Epoch 1065/2000 Batch    0/1 - Training Loss:  0.703  - Validation loss: 10.543\n",
      "Epoch 1066/2000 Batch    0/1 - Training Loss:  0.703  - Validation loss: 10.545\n",
      "Epoch 1067/2000 Batch    0/1 - Training Loss:  0.697  - Validation loss: 10.553\n",
      "Epoch 1068/2000 Batch    0/1 - Training Loss:  0.696  - Validation loss: 10.558\n",
      "Epoch 1069/2000 Batch    0/1 - Training Loss:  0.692  - Validation loss: 10.555\n",
      "Epoch 1070/2000 Batch    0/1 - Training Loss:  0.691  - Validation loss: 10.564\n",
      "Epoch 1071/2000 Batch    0/1 - Training Loss:  0.687  - Validation loss: 10.572\n",
      "Epoch 1072/2000 Batch    0/1 - Training Loss:  0.685  - Validation loss: 10.569\n",
      "Epoch 1073/2000 Batch    0/1 - Training Loss:  0.683  - Validation loss: 10.575\n",
      "Epoch 1074/2000 Batch    0/1 - Training Loss:  0.679  - Validation loss: 10.583\n",
      "Epoch 1075/2000 Batch    0/1 - Training Loss:  0.678  - Validation loss: 10.588\n",
      "Epoch 1076/2000 Batch    0/1 - Training Loss:  0.675  - Validation loss: 10.589\n",
      "Epoch 1077/2000 Batch    0/1 - Training Loss:  0.675  - Validation loss: 10.594\n",
      "Epoch 1078/2000 Batch    0/1 - Training Loss:  0.670  - Validation loss: 10.600\n",
      "Epoch 1079/2000 Batch    0/1 - Training Loss:  0.669  - Validation loss: 10.597\n",
      "Epoch 1080/2000 Batch    0/1 - Training Loss:  0.666  - Validation loss: 10.602\n",
      "Epoch 1081/2000 Batch    0/1 - Training Loss:  0.664  - Validation loss: 10.611\n",
      "Epoch 1082/2000 Batch    0/1 - Training Loss:  0.662  - Validation loss: 10.610\n",
      "Epoch 1083/2000 Batch    0/1 - Training Loss:  0.660  - Validation loss: 10.617\n",
      "Epoch 1084/2000 Batch    0/1 - Training Loss:  0.658  - Validation loss: 10.621\n",
      "Epoch 1085/2000 Batch    0/1 - Training Loss:  0.656  - Validation loss: 10.624\n",
      "Epoch 1086/2000 Batch    0/1 - Training Loss:  0.654  - Validation loss: 10.625\n",
      "Epoch 1087/2000 Batch    0/1 - Training Loss:  0.652  - Validation loss: 10.628\n",
      "Epoch 1088/2000 Batch    0/1 - Training Loss:  0.650  - Validation loss: 10.635\n",
      "Epoch 1089/2000 Batch    0/1 - Training Loss:  0.648  - Validation loss: 10.635\n",
      "Epoch 1090/2000 Batch    0/1 - Training Loss:  0.645  - Validation loss: 10.638\n",
      "Epoch 1091/2000 Batch    0/1 - Training Loss:  0.643  - Validation loss: 10.647\n",
      "Epoch 1092/2000 Batch    0/1 - Training Loss:  0.641  - Validation loss: 10.650\n",
      "Epoch 1093/2000 Batch    0/1 - Training Loss:  0.639  - Validation loss: 10.652\n",
      "Epoch 1094/2000 Batch    0/1 - Training Loss:  0.637  - Validation loss: 10.657\n",
      "Epoch 1095/2000 Batch    0/1 - Training Loss:  0.635  - Validation loss: 10.663\n",
      "Epoch 1096/2000 Batch    0/1 - Training Loss:  0.633  - Validation loss: 10.665\n",
      "Epoch 1097/2000 Batch    0/1 - Training Loss:  0.631  - Validation loss: 10.669\n",
      "Epoch 1098/2000 Batch    0/1 - Training Loss:  0.629  - Validation loss: 10.673\n",
      "Epoch 1099/2000 Batch    0/1 - Training Loss:  0.627  - Validation loss: 10.678\n",
      "Epoch 1100/2000 Batch    0/1 - Training Loss:  0.625  - Validation loss: 10.680\n",
      "Epoch 1101/2000 Batch    0/1 - Training Loss:  0.623  - Validation loss: 10.687\n",
      "Epoch 1102/2000 Batch    0/1 - Training Loss:  0.621  - Validation loss: 10.689\n",
      "Epoch 1103/2000 Batch    0/1 - Training Loss:  0.619  - Validation loss: 10.695\n",
      "Epoch 1104/2000 Batch    0/1 - Training Loss:  0.618  - Validation loss: 10.699\n",
      "Epoch 1105/2000 Batch    0/1 - Training Loss:  0.618  - Validation loss: 10.706\n",
      "Epoch 1106/2000 Batch    0/1 - Training Loss:  0.617  - Validation loss: 10.707\n",
      "Epoch 1107/2000 Batch    0/1 - Training Loss:  0.615  - Validation loss: 10.711\n",
      "Epoch 1108/2000 Batch    0/1 - Training Loss:  0.614  - Validation loss: 10.704\n",
      "Epoch 1109/2000 Batch    0/1 - Training Loss:  0.610  - Validation loss: 10.718\n",
      "Epoch 1110/2000 Batch    0/1 - Training Loss:  0.607  - Validation loss: 10.724\n",
      "Epoch 1111/2000 Batch    0/1 - Training Loss:  0.605  - Validation loss: 10.725\n",
      "Epoch 1112/2000 Batch    0/1 - Training Loss:  0.602  - Validation loss: 10.732\n",
      "Epoch 1113/2000 Batch    0/1 - Training Loss:  0.601  - Validation loss: 10.737\n",
      "Epoch 1114/2000 Batch    0/1 - Training Loss:  0.601  - Validation loss: 10.748\n",
      "Epoch 1115/2000 Batch    0/1 - Training Loss:  0.600  - Validation loss: 10.747\n",
      "Epoch 1116/2000 Batch    0/1 - Training Loss:  0.603  - Validation loss: 10.762\n",
      "Epoch 1117/2000 Batch    0/1 - Training Loss:  0.602  - Validation loss: 10.752\n",
      "Epoch 1118/2000 Batch    0/1 - Training Loss:  0.598  - Validation loss: 10.767\n",
      "Epoch 1119/2000 Batch    0/1 - Training Loss:  0.590  - Validation loss: 10.771\n",
      "Epoch 1120/2000 Batch    0/1 - Training Loss:  0.589  - Validation loss: 10.768\n",
      "Epoch 1121/2000 Batch    0/1 - Training Loss:  0.591  - Validation loss: 10.780\n",
      "Epoch 1122/2000 Batch    0/1 - Training Loss:  0.588  - Validation loss: 10.772\n",
      "Epoch 1123/2000 Batch    0/1 - Training Loss:  0.586  - Validation loss: 10.787\n",
      "Epoch 1124/2000 Batch    0/1 - Training Loss:  0.581  - Validation loss: 10.789\n",
      "Epoch 1125/2000 Batch    0/1 - Training Loss:  0.578  - Validation loss: 10.788\n",
      "Epoch 1126/2000 Batch    0/1 - Training Loss:  0.579  - Validation loss: 10.804\n",
      "Epoch 1127/2000 Batch    0/1 - Training Loss:  0.579  - Validation loss: 10.791\n",
      "Epoch 1128/2000 Batch    0/1 - Training Loss:  0.577  - Validation loss: 10.808\n",
      "Epoch 1129/2000 Batch    0/1 - Training Loss:  0.573  - Validation loss: 10.810\n",
      "Epoch 1130/2000 Batch    0/1 - Training Loss:  0.569  - Validation loss: 10.811\n",
      "Epoch 1131/2000 Batch    0/1 - Training Loss:  0.568  - Validation loss: 10.821\n",
      "Epoch 1132/2000 Batch    0/1 - Training Loss:  0.568  - Validation loss: 10.815\n",
      "Epoch 1133/2000 Batch    0/1 - Training Loss:  0.567  - Validation loss: 10.831\n",
      "Epoch 1134/2000 Batch    0/1 - Training Loss:  0.565  - Validation loss: 10.826\n",
      "Epoch 1135/2000 Batch    0/1 - Training Loss:  0.562  - Validation loss: 10.834\n",
      "Epoch 1136/2000 Batch    0/1 - Training Loss:  0.558  - Validation loss: 10.840\n",
      "Epoch 1137/2000 Batch    0/1 - Training Loss:  0.557  - Validation loss: 10.837\n",
      "Epoch 1138/2000 Batch    0/1 - Training Loss:  0.557  - Validation loss: 10.851\n",
      "Epoch 1139/2000 Batch    0/1 - Training Loss:  0.556  - Validation loss: 10.843\n",
      "Epoch 1140/2000 Batch    0/1 - Training Loss:  0.555  - Validation loss: 10.861\n",
      "Epoch 1141/2000 Batch    0/1 - Training Loss:  0.553  - Validation loss: 10.858\n",
      "Epoch 1142/2000 Batch    0/1 - Training Loss:  0.550  - Validation loss: 10.864\n",
      "Epoch 1143/2000 Batch    0/1 - Training Loss:  0.548  - Validation loss: 10.867\n",
      "Epoch 1144/2000 Batch    0/1 - Training Loss:  0.546  - Validation loss: 10.867\n",
      "Epoch 1145/2000 Batch    0/1 - Training Loss:  0.545  - Validation loss: 10.874\n",
      "Epoch 1146/2000 Batch    0/1 - Training Loss:  0.544  - Validation loss: 10.871\n",
      "Epoch 1147/2000 Batch    0/1 - Training Loss:  0.543  - Validation loss: 10.885\n",
      "Epoch 1148/2000 Batch    0/1 - Training Loss:  0.540  - Validation loss: 10.881\n",
      "Epoch 1149/2000 Batch    0/1 - Training Loss:  0.539  - Validation loss: 10.888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1150/2000 Batch    0/1 - Training Loss:  0.537  - Validation loss: 10.895\n",
      "Epoch 1151/2000 Batch    0/1 - Training Loss:  0.534  - Validation loss: 10.900\n",
      "Epoch 1152/2000 Batch    0/1 - Training Loss:  0.533  - Validation loss: 10.900\n",
      "Epoch 1153/2000 Batch    0/1 - Training Loss:  0.531  - Validation loss: 10.900\n",
      "Epoch 1154/2000 Batch    0/1 - Training Loss:  0.530  - Validation loss: 10.911\n",
      "Epoch 1155/2000 Batch    0/1 - Training Loss:  0.528  - Validation loss: 10.912\n",
      "Epoch 1156/2000 Batch    0/1 - Training Loss:  0.527  - Validation loss: 10.917\n",
      "Epoch 1157/2000 Batch    0/1 - Training Loss:  0.525  - Validation loss: 10.925\n",
      "Epoch 1158/2000 Batch    0/1 - Training Loss:  0.524  - Validation loss: 10.922\n",
      "Epoch 1159/2000 Batch    0/1 - Training Loss:  0.522  - Validation loss: 10.927\n",
      "Epoch 1160/2000 Batch    0/1 - Training Loss:  0.520  - Validation loss: 10.937\n",
      "Epoch 1161/2000 Batch    0/1 - Training Loss:  0.519  - Validation loss: 10.935\n",
      "Epoch 1162/2000 Batch    0/1 - Training Loss:  0.518  - Validation loss: 10.941\n",
      "Epoch 1163/2000 Batch    0/1 - Training Loss:  0.517  - Validation loss: 10.943\n",
      "Epoch 1164/2000 Batch    0/1 - Training Loss:  0.516  - Validation loss: 10.949\n",
      "Epoch 1165/2000 Batch    0/1 - Training Loss:  0.515  - Validation loss: 10.947\n",
      "Epoch 1166/2000 Batch    0/1 - Training Loss:  0.516  - Validation loss: 10.958\n",
      "Epoch 1167/2000 Batch    0/1 - Training Loss:  0.514  - Validation loss: 10.958\n",
      "Epoch 1168/2000 Batch    0/1 - Training Loss:  0.511  - Validation loss: 10.960\n",
      "Epoch 1169/2000 Batch    0/1 - Training Loss:  0.507  - Validation loss: 10.969\n",
      "Epoch 1170/2000 Batch    0/1 - Training Loss:  0.505  - Validation loss: 10.973\n",
      "Epoch 1171/2000 Batch    0/1 - Training Loss:  0.505  - Validation loss: 10.977\n",
      "Epoch 1172/2000 Batch    0/1 - Training Loss:  0.504  - Validation loss: 10.980\n",
      "Epoch 1173/2000 Batch    0/1 - Training Loss:  0.505  - Validation loss: 10.983\n",
      "Epoch 1174/2000 Batch    0/1 - Training Loss:  0.505  - Validation loss: 10.985\n",
      "Epoch 1175/2000 Batch    0/1 - Training Loss:  0.506  - Validation loss: 11.000\n",
      "Epoch 1176/2000 Batch    0/1 - Training Loss:  0.504  - Validation loss: 10.990\n",
      "Epoch 1177/2000 Batch    0/1 - Training Loss:  0.499  - Validation loss: 11.002\n",
      "Epoch 1178/2000 Batch    0/1 - Training Loss:  0.494  - Validation loss: 11.006\n",
      "Epoch 1179/2000 Batch    0/1 - Training Loss:  0.495  - Validation loss: 10.999\n",
      "Epoch 1180/2000 Batch    0/1 - Training Loss:  0.496  - Validation loss: 11.010\n",
      "Epoch 1181/2000 Batch    0/1 - Training Loss:  0.495  - Validation loss: 11.011\n",
      "Epoch 1182/2000 Batch    0/1 - Training Loss:  0.496  - Validation loss: 11.018\n",
      "Epoch 1183/2000 Batch    0/1 - Training Loss:  0.489  - Validation loss: 11.016\n",
      "Epoch 1184/2000 Batch    0/1 - Training Loss:  0.487  - Validation loss: 11.021\n",
      "Epoch 1185/2000 Batch    0/1 - Training Loss:  0.486  - Validation loss: 11.038\n",
      "Epoch 1186/2000 Batch    0/1 - Training Loss:  0.485  - Validation loss: 11.024\n",
      "Epoch 1187/2000 Batch    0/1 - Training Loss:  0.485  - Validation loss: 11.034\n",
      "Epoch 1188/2000 Batch    0/1 - Training Loss:  0.480  - Validation loss: 11.039\n",
      "Epoch 1189/2000 Batch    0/1 - Training Loss:  0.480  - Validation loss: 11.033\n",
      "Epoch 1190/2000 Batch    0/1 - Training Loss:  0.479  - Validation loss: 11.047\n",
      "Epoch 1191/2000 Batch    0/1 - Training Loss:  0.478  - Validation loss: 11.049\n",
      "Epoch 1192/2000 Batch    0/1 - Training Loss:  0.481  - Validation loss: 11.055\n",
      "Epoch 1193/2000 Batch    0/1 - Training Loss:  0.479  - Validation loss: 11.055\n",
      "Epoch 1194/2000 Batch    0/1 - Training Loss:  0.478  - Validation loss: 11.065\n",
      "Epoch 1195/2000 Batch    0/1 - Training Loss:  0.474  - Validation loss: 11.065\n",
      "Epoch 1196/2000 Batch    0/1 - Training Loss:  0.470  - Validation loss: 11.061\n",
      "Epoch 1197/2000 Batch    0/1 - Training Loss:  0.471  - Validation loss: 11.071\n",
      "Epoch 1198/2000 Batch    0/1 - Training Loss:  0.469  - Validation loss: 11.077\n",
      "Epoch 1199/2000 Batch    0/1 - Training Loss:  0.466  - Validation loss: 11.077\n",
      "Epoch 1200/2000 Batch    0/1 - Training Loss:  0.464  - Validation loss: 11.081\n",
      "Epoch 1201/2000 Batch    0/1 - Training Loss:  0.463  - Validation loss: 11.087\n",
      "Epoch 1202/2000 Batch    0/1 - Training Loss:  0.463  - Validation loss: 11.091\n",
      "Epoch 1203/2000 Batch    0/1 - Training Loss:  0.462  - Validation loss: 11.096\n",
      "Epoch 1204/2000 Batch    0/1 - Training Loss:  0.460  - Validation loss: 11.105\n",
      "Epoch 1205/2000 Batch    0/1 - Training Loss:  0.459  - Validation loss: 11.098\n",
      "Epoch 1206/2000 Batch    0/1 - Training Loss:  0.457  - Validation loss: 11.107\n",
      "Epoch 1207/2000 Batch    0/1 - Training Loss:  0.455  - Validation loss: 11.112\n",
      "Epoch 1208/2000 Batch    0/1 - Training Loss:  0.454  - Validation loss: 11.108\n",
      "Epoch 1209/2000 Batch    0/1 - Training Loss:  0.453  - Validation loss: 11.119\n",
      "Epoch 1210/2000 Batch    0/1 - Training Loss:  0.452  - Validation loss: 11.120\n",
      "Epoch 1211/2000 Batch    0/1 - Training Loss:  0.451  - Validation loss: 11.124\n",
      "Epoch 1212/2000 Batch    0/1 - Training Loss:  0.449  - Validation loss: 11.121\n",
      "Epoch 1213/2000 Batch    0/1 - Training Loss:  0.447  - Validation loss: 11.130\n",
      "Epoch 1214/2000 Batch    0/1 - Training Loss:  0.446  - Validation loss: 11.135\n",
      "Epoch 1215/2000 Batch    0/1 - Training Loss:  0.444  - Validation loss: 11.133\n",
      "Epoch 1216/2000 Batch    0/1 - Training Loss:  0.443  - Validation loss: 11.140\n",
      "Epoch 1217/2000 Batch    0/1 - Training Loss:  0.442  - Validation loss: 11.140\n",
      "Epoch 1218/2000 Batch    0/1 - Training Loss:  0.441  - Validation loss: 11.146\n",
      "Epoch 1219/2000 Batch    0/1 - Training Loss:  0.440  - Validation loss: 11.151\n",
      "Epoch 1220/2000 Batch    0/1 - Training Loss:  0.439  - Validation loss: 11.155\n",
      "Epoch 1221/2000 Batch    0/1 - Training Loss:  0.438  - Validation loss: 11.153\n",
      "Epoch 1222/2000 Batch    0/1 - Training Loss:  0.437  - Validation loss: 11.165\n",
      "Epoch 1223/2000 Batch    0/1 - Training Loss:  0.436  - Validation loss: 11.157\n",
      "Epoch 1224/2000 Batch    0/1 - Training Loss:  0.434  - Validation loss: 11.167\n",
      "Epoch 1225/2000 Batch    0/1 - Training Loss:  0.432  - Validation loss: 11.167\n",
      "Epoch 1226/2000 Batch    0/1 - Training Loss:  0.431  - Validation loss: 11.172\n",
      "Epoch 1227/2000 Batch    0/1 - Training Loss:  0.430  - Validation loss: 11.179\n",
      "Epoch 1228/2000 Batch    0/1 - Training Loss:  0.428  - Validation loss: 11.178\n",
      "Epoch 1229/2000 Batch    0/1 - Training Loss:  0.428  - Validation loss: 11.185\n",
      "Epoch 1230/2000 Batch    0/1 - Training Loss:  0.427  - Validation loss: 11.183\n",
      "Epoch 1231/2000 Batch    0/1 - Training Loss:  0.427  - Validation loss: 11.195\n",
      "Epoch 1232/2000 Batch    0/1 - Training Loss:  0.427  - Validation loss: 11.183\n",
      "Epoch 1233/2000 Batch    0/1 - Training Loss:  0.426  - Validation loss: 11.198\n",
      "Epoch 1234/2000 Batch    0/1 - Training Loss:  0.424  - Validation loss: 11.196\n",
      "Epoch 1235/2000 Batch    0/1 - Training Loss:  0.423  - Validation loss: 11.201\n",
      "Epoch 1236/2000 Batch    0/1 - Training Loss:  0.420  - Validation loss: 11.205\n",
      "Epoch 1237/2000 Batch    0/1 - Training Loss:  0.418  - Validation loss: 11.204\n",
      "Epoch 1238/2000 Batch    0/1 - Training Loss:  0.417  - Validation loss: 11.212\n",
      "Epoch 1239/2000 Batch    0/1 - Training Loss:  0.417  - Validation loss: 11.209\n",
      "Epoch 1240/2000 Batch    0/1 - Training Loss:  0.418  - Validation loss: 11.218\n",
      "Epoch 1241/2000 Batch    0/1 - Training Loss:  0.416  - Validation loss: 11.213\n",
      "Epoch 1242/2000 Batch    0/1 - Training Loss:  0.414  - Validation loss: 11.226\n",
      "Epoch 1243/2000 Batch    0/1 - Training Loss:  0.411  - Validation loss: 11.224\n",
      "Epoch 1244/2000 Batch    0/1 - Training Loss:  0.409  - Validation loss: 11.225\n",
      "Epoch 1245/2000 Batch    0/1 - Training Loss:  0.409  - Validation loss: 11.232\n",
      "Epoch 1246/2000 Batch    0/1 - Training Loss:  0.408  - Validation loss: 11.232\n",
      "Epoch 1247/2000 Batch    0/1 - Training Loss:  0.408  - Validation loss: 11.241\n",
      "Epoch 1248/2000 Batch    0/1 - Training Loss:  0.408  - Validation loss: 11.234\n",
      "Epoch 1249/2000 Batch    0/1 - Training Loss:  0.407  - Validation loss: 11.248\n",
      "Epoch 1250/2000 Batch    0/1 - Training Loss:  0.405  - Validation loss: 11.241\n",
      "Epoch 1251/2000 Batch    0/1 - Training Loss:  0.402  - Validation loss: 11.250\n",
      "Epoch 1252/2000 Batch    0/1 - Training Loss:  0.400  - Validation loss: 11.253\n",
      "Epoch 1253/2000 Batch    0/1 - Training Loss:  0.400  - Validation loss: 11.246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1254/2000 Batch    0/1 - Training Loss:  0.400  - Validation loss: 11.263\n",
      "Epoch 1255/2000 Batch    0/1 - Training Loss:  0.399  - Validation loss: 11.259\n",
      "Epoch 1256/2000 Batch    0/1 - Training Loss:  0.399  - Validation loss: 11.264\n",
      "Epoch 1257/2000 Batch    0/1 - Training Loss:  0.397  - Validation loss: 11.264\n",
      "Epoch 1258/2000 Batch    0/1 - Training Loss:  0.394  - Validation loss: 11.270\n",
      "Epoch 1259/2000 Batch    0/1 - Training Loss:  0.394  - Validation loss: 11.272\n",
      "Epoch 1260/2000 Batch    0/1 - Training Loss:  0.394  - Validation loss: 11.279\n",
      "Epoch 1261/2000 Batch    0/1 - Training Loss:  0.391  - Validation loss: 11.286\n",
      "Epoch 1262/2000 Batch    0/1 - Training Loss:  0.391  - Validation loss: 11.277\n",
      "Epoch 1263/2000 Batch    0/1 - Training Loss:  0.390  - Validation loss: 11.287\n",
      "Epoch 1264/2000 Batch    0/1 - Training Loss:  0.388  - Validation loss: 11.297\n",
      "Epoch 1265/2000 Batch    0/1 - Training Loss:  0.388  - Validation loss: 11.292\n",
      "Epoch 1266/2000 Batch    0/1 - Training Loss:  0.386  - Validation loss: 11.298\n",
      "Epoch 1267/2000 Batch    0/1 - Training Loss:  0.384  - Validation loss: 11.304\n",
      "Epoch 1268/2000 Batch    0/1 - Training Loss:  0.383  - Validation loss: 11.302\n",
      "Epoch 1269/2000 Batch    0/1 - Training Loss:  0.382  - Validation loss: 11.309\n",
      "Epoch 1270/2000 Batch    0/1 - Training Loss:  0.381  - Validation loss: 11.306\n",
      "Epoch 1271/2000 Batch    0/1 - Training Loss:  0.380  - Validation loss: 11.312\n",
      "Epoch 1272/2000 Batch    0/1 - Training Loss:  0.379  - Validation loss: 11.313\n",
      "Epoch 1273/2000 Batch    0/1 - Training Loss:  0.378  - Validation loss: 11.320\n",
      "Epoch 1274/2000 Batch    0/1 - Training Loss:  0.377  - Validation loss: 11.319\n",
      "Epoch 1275/2000 Batch    0/1 - Training Loss:  0.377  - Validation loss: 11.324\n",
      "Epoch 1276/2000 Batch    0/1 - Training Loss:  0.376  - Validation loss: 11.327\n",
      "Epoch 1277/2000 Batch    0/1 - Training Loss:  0.375  - Validation loss: 11.332\n",
      "Epoch 1278/2000 Batch    0/1 - Training Loss:  0.374  - Validation loss: 11.324\n",
      "Epoch 1279/2000 Batch    0/1 - Training Loss:  0.373  - Validation loss: 11.339\n",
      "Epoch 1280/2000 Batch    0/1 - Training Loss:  0.371  - Validation loss: 11.336\n",
      "Epoch 1281/2000 Batch    0/1 - Training Loss:  0.369  - Validation loss: 11.339\n",
      "Epoch 1282/2000 Batch    0/1 - Training Loss:  0.368  - Validation loss: 11.344\n",
      "Epoch 1283/2000 Batch    0/1 - Training Loss:  0.368  - Validation loss: 11.339\n",
      "Epoch 1284/2000 Batch    0/1 - Training Loss:  0.368  - Validation loss: 11.356\n",
      "Epoch 1285/2000 Batch    0/1 - Training Loss:  0.369  - Validation loss: 11.348\n",
      "Epoch 1286/2000 Batch    0/1 - Training Loss:  0.369  - Validation loss: 11.358\n",
      "Epoch 1287/2000 Batch    0/1 - Training Loss:  0.367  - Validation loss: 11.357\n",
      "Epoch 1288/2000 Batch    0/1 - Training Loss:  0.363  - Validation loss: 11.357\n",
      "Epoch 1289/2000 Batch    0/1 - Training Loss:  0.361  - Validation loss: 11.368\n",
      "Epoch 1290/2000 Batch    0/1 - Training Loss:  0.363  - Validation loss: 11.360\n",
      "Epoch 1291/2000 Batch    0/1 - Training Loss:  0.364  - Validation loss: 11.372\n",
      "Epoch 1292/2000 Batch    0/1 - Training Loss:  0.364  - Validation loss: 11.369\n",
      "Epoch 1293/2000 Batch    0/1 - Training Loss:  0.361  - Validation loss: 11.374\n",
      "Epoch 1294/2000 Batch    0/1 - Training Loss:  0.356  - Validation loss: 11.380\n",
      "Epoch 1295/2000 Batch    0/1 - Training Loss:  0.357  - Validation loss: 11.378\n",
      "Epoch 1296/2000 Batch    0/1 - Training Loss:  0.361  - Validation loss: 11.389\n",
      "Epoch 1297/2000 Batch    0/1 - Training Loss:  0.360  - Validation loss: 11.378\n",
      "Epoch 1298/2000 Batch    0/1 - Training Loss:  0.356  - Validation loss: 11.392\n",
      "Epoch 1299/2000 Batch    0/1 - Training Loss:  0.353  - Validation loss: 11.397\n",
      "Epoch 1300/2000 Batch    0/1 - Training Loss:  0.353  - Validation loss: 11.393\n",
      "Epoch 1301/2000 Batch    0/1 - Training Loss:  0.354  - Validation loss: 11.409\n",
      "Epoch 1302/2000 Batch    0/1 - Training Loss:  0.352  - Validation loss: 11.399\n",
      "Epoch 1303/2000 Batch    0/1 - Training Loss:  0.349  - Validation loss: 11.403\n",
      "Epoch 1304/2000 Batch    0/1 - Training Loss:  0.348  - Validation loss: 11.415\n",
      "Epoch 1305/2000 Batch    0/1 - Training Loss:  0.348  - Validation loss: 11.414\n",
      "Epoch 1306/2000 Batch    0/1 - Training Loss:  0.347  - Validation loss: 11.418\n",
      "Epoch 1307/2000 Batch    0/1 - Training Loss:  0.345  - Validation loss: 11.421\n",
      "Epoch 1308/2000 Batch    0/1 - Training Loss:  0.343  - Validation loss: 11.427\n",
      "Epoch 1309/2000 Batch    0/1 - Training Loss:  0.343  - Validation loss: 11.429\n",
      "Epoch 1310/2000 Batch    0/1 - Training Loss:  0.341  - Validation loss: 11.422\n",
      "Epoch 1311/2000 Batch    0/1 - Training Loss:  0.340  - Validation loss: 11.431\n",
      "Epoch 1312/2000 Batch    0/1 - Training Loss:  0.339  - Validation loss: 11.436\n",
      "Epoch 1313/2000 Batch    0/1 - Training Loss:  0.338  - Validation loss: 11.433\n",
      "Epoch 1314/2000 Batch    0/1 - Training Loss:  0.337  - Validation loss: 11.440\n",
      "Epoch 1315/2000 Batch    0/1 - Training Loss:  0.336  - Validation loss: 11.443\n",
      "Epoch 1316/2000 Batch    0/1 - Training Loss:  0.335  - Validation loss: 11.443\n",
      "Epoch 1317/2000 Batch    0/1 - Training Loss:  0.334  - Validation loss: 11.448\n",
      "Epoch 1318/2000 Batch    0/1 - Training Loss:  0.333  - Validation loss: 11.446\n",
      "Epoch 1319/2000 Batch    0/1 - Training Loss:  0.333  - Validation loss: 11.455\n",
      "Epoch 1320/2000 Batch    0/1 - Training Loss:  0.332  - Validation loss: 11.454\n",
      "Epoch 1321/2000 Batch    0/1 - Training Loss:  0.330  - Validation loss: 11.459\n",
      "Epoch 1322/2000 Batch    0/1 - Training Loss:  0.329  - Validation loss: 11.463\n",
      "Epoch 1323/2000 Batch    0/1 - Training Loss:  0.328  - Validation loss: 11.464\n",
      "Epoch 1324/2000 Batch    0/1 - Training Loss:  0.327  - Validation loss: 11.472\n",
      "Epoch 1325/2000 Batch    0/1 - Training Loss:  0.327  - Validation loss: 11.470\n",
      "Epoch 1326/2000 Batch    0/1 - Training Loss:  0.326  - Validation loss: 11.474\n",
      "Epoch 1327/2000 Batch    0/1 - Training Loss:  0.325  - Validation loss: 11.478\n",
      "Epoch 1328/2000 Batch    0/1 - Training Loss:  0.324  - Validation loss: 11.481\n",
      "Epoch 1329/2000 Batch    0/1 - Training Loss:  0.323  - Validation loss: 11.484\n",
      "Epoch 1330/2000 Batch    0/1 - Training Loss:  0.322  - Validation loss: 11.487\n",
      "Epoch 1331/2000 Batch    0/1 - Training Loss:  0.321  - Validation loss: 11.488\n",
      "Epoch 1332/2000 Batch    0/1 - Training Loss:  0.320  - Validation loss: 11.491\n",
      "Epoch 1333/2000 Batch    0/1 - Training Loss:  0.319  - Validation loss: 11.496\n",
      "Epoch 1334/2000 Batch    0/1 - Training Loss:  0.318  - Validation loss: 11.497\n",
      "Epoch 1335/2000 Batch    0/1 - Training Loss:  0.317  - Validation loss: 11.500\n",
      "Epoch 1336/2000 Batch    0/1 - Training Loss:  0.317  - Validation loss: 11.502\n",
      "Epoch 1337/2000 Batch    0/1 - Training Loss:  0.316  - Validation loss: 11.506\n",
      "Epoch 1338/2000 Batch    0/1 - Training Loss:  0.315  - Validation loss: 11.509\n",
      "Epoch 1339/2000 Batch    0/1 - Training Loss:  0.314  - Validation loss: 11.511\n",
      "Epoch 1340/2000 Batch    0/1 - Training Loss:  0.313  - Validation loss: 11.514\n",
      "Epoch 1341/2000 Batch    0/1 - Training Loss:  0.312  - Validation loss: 11.517\n",
      "Epoch 1342/2000 Batch    0/1 - Training Loss:  0.311  - Validation loss: 11.521\n",
      "Epoch 1343/2000 Batch    0/1 - Training Loss:  0.310  - Validation loss: 11.522\n",
      "Epoch 1344/2000 Batch    0/1 - Training Loss:  0.310  - Validation loss: 11.527\n",
      "Epoch 1345/2000 Batch    0/1 - Training Loss:  0.309  - Validation loss: 11.529\n",
      "Epoch 1346/2000 Batch    0/1 - Training Loss:  0.308  - Validation loss: 11.533\n",
      "Epoch 1347/2000 Batch    0/1 - Training Loss:  0.307  - Validation loss: 11.534\n",
      "Epoch 1348/2000 Batch    0/1 - Training Loss:  0.307  - Validation loss: 11.538\n",
      "Epoch 1349/2000 Batch    0/1 - Training Loss:  0.306  - Validation loss: 11.542\n",
      "Epoch 1350/2000 Batch    0/1 - Training Loss:  0.305  - Validation loss: 11.545\n",
      "Epoch 1351/2000 Batch    0/1 - Training Loss:  0.304  - Validation loss: 11.544\n",
      "Epoch 1352/2000 Batch    0/1 - Training Loss:  0.303  - Validation loss: 11.549\n",
      "Epoch 1353/2000 Batch    0/1 - Training Loss:  0.302  - Validation loss: 11.553\n",
      "Epoch 1354/2000 Batch    0/1 - Training Loss:  0.301  - Validation loss: 11.556\n",
      "Epoch 1355/2000 Batch    0/1 - Training Loss:  0.300  - Validation loss: 11.558\n",
      "Epoch 1356/2000 Batch    0/1 - Training Loss:  0.299  - Validation loss: 11.560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1357/2000 Batch    0/1 - Training Loss:  0.299  - Validation loss: 11.565\n",
      "Epoch 1358/2000 Batch    0/1 - Training Loss:  0.298  - Validation loss: 11.567\n",
      "Epoch 1359/2000 Batch    0/1 - Training Loss:  0.297  - Validation loss: 11.571\n",
      "Epoch 1360/2000 Batch    0/1 - Training Loss:  0.296  - Validation loss: 11.571\n",
      "Epoch 1361/2000 Batch    0/1 - Training Loss:  0.296  - Validation loss: 11.578\n",
      "Epoch 1362/2000 Batch    0/1 - Training Loss:  0.295  - Validation loss: 11.580\n",
      "Epoch 1363/2000 Batch    0/1 - Training Loss:  0.294  - Validation loss: 11.583\n",
      "Epoch 1364/2000 Batch    0/1 - Training Loss:  0.293  - Validation loss: 11.585\n",
      "Epoch 1365/2000 Batch    0/1 - Training Loss:  0.292  - Validation loss: 11.588\n",
      "Epoch 1366/2000 Batch    0/1 - Training Loss:  0.291  - Validation loss: 11.591\n",
      "Epoch 1367/2000 Batch    0/1 - Training Loss:  0.290  - Validation loss: 11.594\n",
      "Epoch 1368/2000 Batch    0/1 - Training Loss:  0.290  - Validation loss: 11.600\n",
      "Epoch 1369/2000 Batch    0/1 - Training Loss:  0.289  - Validation loss: 11.604\n",
      "Epoch 1370/2000 Batch    0/1 - Training Loss:  0.288  - Validation loss: 11.607\n",
      "Epoch 1371/2000 Batch    0/1 - Training Loss:  0.287  - Validation loss: 11.608\n",
      "Epoch 1372/2000 Batch    0/1 - Training Loss:  0.286  - Validation loss: 11.610\n",
      "Epoch 1373/2000 Batch    0/1 - Training Loss:  0.286  - Validation loss: 11.612\n",
      "Epoch 1374/2000 Batch    0/1 - Training Loss:  0.285  - Validation loss: 11.615\n",
      "Epoch 1375/2000 Batch    0/1 - Training Loss:  0.285  - Validation loss: 11.619\n",
      "Epoch 1376/2000 Batch    0/1 - Training Loss:  0.285  - Validation loss: 11.624\n",
      "Epoch 1377/2000 Batch    0/1 - Training Loss:  0.284  - Validation loss: 11.626\n",
      "Epoch 1378/2000 Batch    0/1 - Training Loss:  0.283  - Validation loss: 11.635\n",
      "Epoch 1379/2000 Batch    0/1 - Training Loss:  0.283  - Validation loss: 11.626\n",
      "Epoch 1380/2000 Batch    0/1 - Training Loss:  0.281  - Validation loss: 11.635\n",
      "Epoch 1381/2000 Batch    0/1 - Training Loss:  0.281  - Validation loss: 11.635\n",
      "Epoch 1382/2000 Batch    0/1 - Training Loss:  0.281  - Validation loss: 11.647\n",
      "Epoch 1383/2000 Batch    0/1 - Training Loss:  0.278  - Validation loss: 11.647\n",
      "Epoch 1384/2000 Batch    0/1 - Training Loss:  0.278  - Validation loss: 11.658\n",
      "Epoch 1385/2000 Batch    0/1 - Training Loss:  0.279  - Validation loss: 11.654\n",
      "Epoch 1386/2000 Batch    0/1 - Training Loss:  0.276  - Validation loss: 11.651\n",
      "Epoch 1387/2000 Batch    0/1 - Training Loss:  0.277  - Validation loss: 11.664\n",
      "Epoch 1388/2000 Batch    0/1 - Training Loss:  0.277  - Validation loss: 11.664\n",
      "Epoch 1389/2000 Batch    0/1 - Training Loss:  0.276  - Validation loss: 11.665\n",
      "Epoch 1390/2000 Batch    0/1 - Training Loss:  0.276  - Validation loss: 11.673\n",
      "Epoch 1391/2000 Batch    0/1 - Training Loss:  0.273  - Validation loss: 11.675\n",
      "Epoch 1392/2000 Batch    0/1 - Training Loss:  0.272  - Validation loss: 11.671\n",
      "Epoch 1393/2000 Batch    0/1 - Training Loss:  0.272  - Validation loss: 11.670\n",
      "Epoch 1394/2000 Batch    0/1 - Training Loss:  0.270  - Validation loss: 11.679\n",
      "Epoch 1395/2000 Batch    0/1 - Training Loss:  0.269  - Validation loss: 11.677\n",
      "Epoch 1396/2000 Batch    0/1 - Training Loss:  0.267  - Validation loss: 11.672\n",
      "Epoch 1397/2000 Batch    0/1 - Training Loss:  0.268  - Validation loss: 11.685\n",
      "Epoch 1398/2000 Batch    0/1 - Training Loss:  0.267  - Validation loss: 11.677\n",
      "Epoch 1399/2000 Batch    0/1 - Training Loss:  0.266  - Validation loss: 11.678\n",
      "Epoch 1400/2000 Batch    0/1 - Training Loss:  0.265  - Validation loss: 11.687\n",
      "Epoch 1401/2000 Batch    0/1 - Training Loss:  0.264  - Validation loss: 11.682\n",
      "Epoch 1402/2000 Batch    0/1 - Training Loss:  0.264  - Validation loss: 11.695\n",
      "Epoch 1403/2000 Batch    0/1 - Training Loss:  0.263  - Validation loss: 11.695\n",
      "Epoch 1404/2000 Batch    0/1 - Training Loss:  0.261  - Validation loss: 11.700\n",
      "Epoch 1405/2000 Batch    0/1 - Training Loss:  0.261  - Validation loss: 11.704\n",
      "Epoch 1406/2000 Batch    0/1 - Training Loss:  0.259  - Validation loss: 11.698\n",
      "Epoch 1407/2000 Batch    0/1 - Training Loss:  0.259  - Validation loss: 11.709\n",
      "Epoch 1408/2000 Batch    0/1 - Training Loss:  0.258  - Validation loss: 11.705\n",
      "Epoch 1409/2000 Batch    0/1 - Training Loss:  0.257  - Validation loss: 11.709\n",
      "Epoch 1410/2000 Batch    0/1 - Training Loss:  0.256  - Validation loss: 11.716\n",
      "Epoch 1411/2000 Batch    0/1 - Training Loss:  0.255  - Validation loss: 11.714\n",
      "Epoch 1412/2000 Batch    0/1 - Training Loss:  0.255  - Validation loss: 11.720\n",
      "Epoch 1413/2000 Batch    0/1 - Training Loss:  0.254  - Validation loss: 11.723\n",
      "Epoch 1414/2000 Batch    0/1 - Training Loss:  0.253  - Validation loss: 11.725\n",
      "Epoch 1415/2000 Batch    0/1 - Training Loss:  0.253  - Validation loss: 11.731\n",
      "Epoch 1416/2000 Batch    0/1 - Training Loss:  0.252  - Validation loss: 11.727\n",
      "Epoch 1417/2000 Batch    0/1 - Training Loss:  0.251  - Validation loss: 11.734\n",
      "Epoch 1418/2000 Batch    0/1 - Training Loss:  0.250  - Validation loss: 11.736\n",
      "Epoch 1419/2000 Batch    0/1 - Training Loss:  0.249  - Validation loss: 11.736\n",
      "Epoch 1420/2000 Batch    0/1 - Training Loss:  0.249  - Validation loss: 11.746\n",
      "Epoch 1421/2000 Batch    0/1 - Training Loss:  0.248  - Validation loss: 11.744\n",
      "Epoch 1422/2000 Batch    0/1 - Training Loss:  0.247  - Validation loss: 11.748\n",
      "Epoch 1423/2000 Batch    0/1 - Training Loss:  0.246  - Validation loss: 11.754\n",
      "Epoch 1424/2000 Batch    0/1 - Training Loss:  0.246  - Validation loss: 11.751\n",
      "Epoch 1425/2000 Batch    0/1 - Training Loss:  0.245  - Validation loss: 11.761\n",
      "Epoch 1426/2000 Batch    0/1 - Training Loss:  0.245  - Validation loss: 11.757\n",
      "Epoch 1427/2000 Batch    0/1 - Training Loss:  0.244  - Validation loss: 11.763\n",
      "Epoch 1428/2000 Batch    0/1 - Training Loss:  0.243  - Validation loss: 11.767\n",
      "Epoch 1429/2000 Batch    0/1 - Training Loss:  0.242  - Validation loss: 11.765\n",
      "Epoch 1430/2000 Batch    0/1 - Training Loss:  0.242  - Validation loss: 11.772\n",
      "Epoch 1431/2000 Batch    0/1 - Training Loss:  0.241  - Validation loss: 11.776\n",
      "Epoch 1432/2000 Batch    0/1 - Training Loss:  0.240  - Validation loss: 11.774\n",
      "Epoch 1433/2000 Batch    0/1 - Training Loss:  0.240  - Validation loss: 11.783\n",
      "Epoch 1434/2000 Batch    0/1 - Training Loss:  0.239  - Validation loss: 11.779\n",
      "Epoch 1435/2000 Batch    0/1 - Training Loss:  0.238  - Validation loss: 11.786\n",
      "Epoch 1436/2000 Batch    0/1 - Training Loss:  0.237  - Validation loss: 11.791\n",
      "Epoch 1437/2000 Batch    0/1 - Training Loss:  0.237  - Validation loss: 11.790\n",
      "Epoch 1438/2000 Batch    0/1 - Training Loss:  0.236  - Validation loss: 11.796\n",
      "Epoch 1439/2000 Batch    0/1 - Training Loss:  0.236  - Validation loss: 11.797\n",
      "Epoch 1440/2000 Batch    0/1 - Training Loss:  0.235  - Validation loss: 11.799\n",
      "Epoch 1441/2000 Batch    0/1 - Training Loss:  0.234  - Validation loss: 11.805\n",
      "Epoch 1442/2000 Batch    0/1 - Training Loss:  0.234  - Validation loss: 11.802\n",
      "Epoch 1443/2000 Batch    0/1 - Training Loss:  0.233  - Validation loss: 11.810\n",
      "Epoch 1444/2000 Batch    0/1 - Training Loss:  0.232  - Validation loss: 11.811\n",
      "Epoch 1445/2000 Batch    0/1 - Training Loss:  0.232  - Validation loss: 11.814\n",
      "Epoch 1446/2000 Batch    0/1 - Training Loss:  0.231  - Validation loss: 11.819\n",
      "Epoch 1447/2000 Batch    0/1 - Training Loss:  0.231  - Validation loss: 11.818\n",
      "Epoch 1448/2000 Batch    0/1 - Training Loss:  0.230  - Validation loss: 11.823\n",
      "Epoch 1449/2000 Batch    0/1 - Training Loss:  0.229  - Validation loss: 11.828\n",
      "Epoch 1450/2000 Batch    0/1 - Training Loss:  0.229  - Validation loss: 11.827\n",
      "Epoch 1451/2000 Batch    0/1 - Training Loss:  0.228  - Validation loss: 11.835\n",
      "Epoch 1452/2000 Batch    0/1 - Training Loss:  0.228  - Validation loss: 11.833\n",
      "Epoch 1453/2000 Batch    0/1 - Training Loss:  0.227  - Validation loss: 11.838\n",
      "Epoch 1454/2000 Batch    0/1 - Training Loss:  0.226  - Validation loss: 11.842\n",
      "Epoch 1455/2000 Batch    0/1 - Training Loss:  0.226  - Validation loss: 11.841\n",
      "Epoch 1456/2000 Batch    0/1 - Training Loss:  0.225  - Validation loss: 11.848\n",
      "Epoch 1457/2000 Batch    0/1 - Training Loss:  0.224  - Validation loss: 11.849\n",
      "Epoch 1458/2000 Batch    0/1 - Training Loss:  0.224  - Validation loss: 11.852\n",
      "Epoch 1459/2000 Batch    0/1 - Training Loss:  0.223  - Validation loss: 11.855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1460/2000 Batch    0/1 - Training Loss:  0.223  - Validation loss: 11.857\n",
      "Epoch 1461/2000 Batch    0/1 - Training Loss:  0.222  - Validation loss: 11.861\n",
      "Epoch 1462/2000 Batch    0/1 - Training Loss:  0.222  - Validation loss: 11.865\n",
      "Epoch 1463/2000 Batch    0/1 - Training Loss:  0.221  - Validation loss: 11.865\n",
      "Epoch 1464/2000 Batch    0/1 - Training Loss:  0.220  - Validation loss: 11.872\n",
      "Epoch 1465/2000 Batch    0/1 - Training Loss:  0.220  - Validation loss: 11.870\n",
      "Epoch 1466/2000 Batch    0/1 - Training Loss:  0.219  - Validation loss: 11.875\n",
      "Epoch 1467/2000 Batch    0/1 - Training Loss:  0.218  - Validation loss: 11.878\n",
      "Epoch 1468/2000 Batch    0/1 - Training Loss:  0.218  - Validation loss: 11.879\n",
      "Epoch 1469/2000 Batch    0/1 - Training Loss:  0.217  - Validation loss: 11.886\n",
      "Epoch 1470/2000 Batch    0/1 - Training Loss:  0.217  - Validation loss: 11.884\n",
      "Epoch 1471/2000 Batch    0/1 - Training Loss:  0.217  - Validation loss: 11.891\n",
      "Epoch 1472/2000 Batch    0/1 - Training Loss:  0.216  - Validation loss: 11.892\n",
      "Epoch 1473/2000 Batch    0/1 - Training Loss:  0.215  - Validation loss: 11.892\n",
      "Epoch 1474/2000 Batch    0/1 - Training Loss:  0.215  - Validation loss: 11.899\n",
      "Epoch 1475/2000 Batch    0/1 - Training Loss:  0.214  - Validation loss: 11.899\n",
      "Epoch 1476/2000 Batch    0/1 - Training Loss:  0.213  - Validation loss: 11.905\n",
      "Epoch 1477/2000 Batch    0/1 - Training Loss:  0.213  - Validation loss: 11.905\n",
      "Epoch 1478/2000 Batch    0/1 - Training Loss:  0.213  - Validation loss: 11.909\n",
      "Epoch 1479/2000 Batch    0/1 - Training Loss:  0.212  - Validation loss: 11.911\n",
      "Epoch 1480/2000 Batch    0/1 - Training Loss:  0.212  - Validation loss: 11.913\n",
      "Epoch 1481/2000 Batch    0/1 - Training Loss:  0.211  - Validation loss: 11.918\n",
      "Epoch 1482/2000 Batch    0/1 - Training Loss:  0.210  - Validation loss: 11.918\n",
      "Epoch 1483/2000 Batch    0/1 - Training Loss:  0.209  - Validation loss: 11.922\n",
      "Epoch 1484/2000 Batch    0/1 - Training Loss:  0.209  - Validation loss: 11.926\n",
      "Epoch 1485/2000 Batch    0/1 - Training Loss:  0.209  - Validation loss: 11.929\n",
      "Epoch 1486/2000 Batch    0/1 - Training Loss:  0.208  - Validation loss: 11.934\n",
      "Epoch 1487/2000 Batch    0/1 - Training Loss:  0.208  - Validation loss: 11.933\n",
      "Epoch 1488/2000 Batch    0/1 - Training Loss:  0.207  - Validation loss: 11.936\n",
      "Epoch 1489/2000 Batch    0/1 - Training Loss:  0.207  - Validation loss: 11.939\n",
      "Epoch 1490/2000 Batch    0/1 - Training Loss:  0.206  - Validation loss: 11.939\n",
      "Epoch 1491/2000 Batch    0/1 - Training Loss:  0.205  - Validation loss: 11.944\n",
      "Epoch 1492/2000 Batch    0/1 - Training Loss:  0.205  - Validation loss: 11.947\n",
      "Epoch 1493/2000 Batch    0/1 - Training Loss:  0.204  - Validation loss: 11.953\n",
      "Epoch 1494/2000 Batch    0/1 - Training Loss:  0.205  - Validation loss: 11.963\n",
      "Epoch 1495/2000 Batch    0/1 - Training Loss:  0.205  - Validation loss: 11.956\n",
      "Epoch 1496/2000 Batch    0/1 - Training Loss:  0.204  - Validation loss: 11.964\n",
      "Epoch 1497/2000 Batch    0/1 - Training Loss:  0.204  - Validation loss: 11.971\n",
      "Epoch 1498/2000 Batch    0/1 - Training Loss:  0.203  - Validation loss: 11.969\n",
      "Epoch 1499/2000 Batch    0/1 - Training Loss:  0.202  - Validation loss: 11.976\n",
      "Epoch 1500/2000 Batch    0/1 - Training Loss:  0.202  - Validation loss: 11.974\n",
      "Epoch 1501/2000 Batch    0/1 - Training Loss:  0.201  - Validation loss: 11.976\n",
      "Epoch 1502/2000 Batch    0/1 - Training Loss:  0.201  - Validation loss: 11.979\n",
      "Epoch 1503/2000 Batch    0/1 - Training Loss:  0.200  - Validation loss: 11.978\n",
      "Epoch 1504/2000 Batch    0/1 - Training Loss:  0.199  - Validation loss: 11.978\n",
      "Epoch 1505/2000 Batch    0/1 - Training Loss:  0.199  - Validation loss: 11.984\n",
      "Epoch 1506/2000 Batch    0/1 - Training Loss:  0.198  - Validation loss: 11.983\n",
      "Epoch 1507/2000 Batch    0/1 - Training Loss:  0.197  - Validation loss: 11.985\n",
      "Epoch 1508/2000 Batch    0/1 - Training Loss:  0.197  - Validation loss: 11.988\n",
      "Epoch 1509/2000 Batch    0/1 - Training Loss:  0.196  - Validation loss: 11.987\n",
      "Epoch 1510/2000 Batch    0/1 - Training Loss:  0.196  - Validation loss: 11.994\n",
      "Epoch 1511/2000 Batch    0/1 - Training Loss:  0.196  - Validation loss: 11.995\n",
      "Epoch 1512/2000 Batch    0/1 - Training Loss:  0.195  - Validation loss: 12.001\n",
      "Epoch 1513/2000 Batch    0/1 - Training Loss:  0.195  - Validation loss: 12.003\n",
      "Epoch 1514/2000 Batch    0/1 - Training Loss:  0.194  - Validation loss: 12.006\n",
      "Epoch 1515/2000 Batch    0/1 - Training Loss:  0.194  - Validation loss: 12.009\n",
      "Epoch 1516/2000 Batch    0/1 - Training Loss:  0.193  - Validation loss: 12.010\n",
      "Epoch 1517/2000 Batch    0/1 - Training Loss:  0.192  - Validation loss: 12.013\n",
      "Epoch 1518/2000 Batch    0/1 - Training Loss:  0.192  - Validation loss: 12.015\n",
      "Epoch 1519/2000 Batch    0/1 - Training Loss:  0.192  - Validation loss: 12.018\n",
      "Epoch 1520/2000 Batch    0/1 - Training Loss:  0.191  - Validation loss: 12.019\n",
      "Epoch 1521/2000 Batch    0/1 - Training Loss:  0.191  - Validation loss: 12.024\n",
      "Epoch 1522/2000 Batch    0/1 - Training Loss:  0.190  - Validation loss: 12.027\n",
      "Epoch 1523/2000 Batch    0/1 - Training Loss:  0.190  - Validation loss: 12.028\n",
      "Epoch 1524/2000 Batch    0/1 - Training Loss:  0.189  - Validation loss: 12.030\n",
      "Epoch 1525/2000 Batch    0/1 - Training Loss:  0.189  - Validation loss: 12.032\n",
      "Epoch 1526/2000 Batch    0/1 - Training Loss:  0.188  - Validation loss: 12.037\n",
      "Epoch 1527/2000 Batch    0/1 - Training Loss:  0.188  - Validation loss: 12.039\n",
      "Epoch 1528/2000 Batch    0/1 - Training Loss:  0.187  - Validation loss: 12.042\n",
      "Epoch 1529/2000 Batch    0/1 - Training Loss:  0.187  - Validation loss: 12.042\n",
      "Epoch 1530/2000 Batch    0/1 - Training Loss:  0.186  - Validation loss: 12.046\n",
      "Epoch 1531/2000 Batch    0/1 - Training Loss:  0.186  - Validation loss: 12.049\n",
      "Epoch 1532/2000 Batch    0/1 - Training Loss:  0.185  - Validation loss: 12.051\n",
      "Epoch 1533/2000 Batch    0/1 - Training Loss:  0.185  - Validation loss: 12.055\n",
      "Epoch 1534/2000 Batch    0/1 - Training Loss:  0.184  - Validation loss: 12.056\n",
      "Epoch 1535/2000 Batch    0/1 - Training Loss:  0.184  - Validation loss: 12.061\n",
      "Epoch 1536/2000 Batch    0/1 - Training Loss:  0.184  - Validation loss: 12.062\n",
      "Epoch 1537/2000 Batch    0/1 - Training Loss:  0.183  - Validation loss: 12.063\n",
      "Epoch 1538/2000 Batch    0/1 - Training Loss:  0.183  - Validation loss: 12.067\n",
      "Epoch 1539/2000 Batch    0/1 - Training Loss:  0.182  - Validation loss: 12.068\n",
      "Epoch 1540/2000 Batch    0/1 - Training Loss:  0.182  - Validation loss: 12.072\n",
      "Epoch 1541/2000 Batch    0/1 - Training Loss:  0.181  - Validation loss: 12.076\n",
      "Epoch 1542/2000 Batch    0/1 - Training Loss:  0.181  - Validation loss: 12.077\n",
      "Epoch 1543/2000 Batch    0/1 - Training Loss:  0.180  - Validation loss: 12.081\n",
      "Epoch 1544/2000 Batch    0/1 - Training Loss:  0.180  - Validation loss: 12.083\n",
      "Epoch 1545/2000 Batch    0/1 - Training Loss:  0.179  - Validation loss: 12.086\n",
      "Epoch 1546/2000 Batch    0/1 - Training Loss:  0.179  - Validation loss: 12.090\n",
      "Epoch 1547/2000 Batch    0/1 - Training Loss:  0.179  - Validation loss: 12.090\n",
      "Epoch 1548/2000 Batch    0/1 - Training Loss:  0.178  - Validation loss: 12.093\n",
      "Epoch 1549/2000 Batch    0/1 - Training Loss:  0.178  - Validation loss: 12.095\n",
      "Epoch 1550/2000 Batch    0/1 - Training Loss:  0.177  - Validation loss: 12.098\n",
      "Epoch 1551/2000 Batch    0/1 - Training Loss:  0.177  - Validation loss: 12.101\n",
      "Epoch 1552/2000 Batch    0/1 - Training Loss:  0.177  - Validation loss: 12.103\n",
      "Epoch 1553/2000 Batch    0/1 - Training Loss:  0.176  - Validation loss: 12.106\n",
      "Epoch 1554/2000 Batch    0/1 - Training Loss:  0.176  - Validation loss: 12.109\n",
      "Epoch 1555/2000 Batch    0/1 - Training Loss:  0.175  - Validation loss: 12.111\n",
      "Epoch 1556/2000 Batch    0/1 - Training Loss:  0.175  - Validation loss: 12.114\n",
      "Epoch 1557/2000 Batch    0/1 - Training Loss:  0.174  - Validation loss: 12.115\n",
      "Epoch 1558/2000 Batch    0/1 - Training Loss:  0.174  - Validation loss: 12.118\n",
      "Epoch 1559/2000 Batch    0/1 - Training Loss:  0.173  - Validation loss: 12.121\n",
      "Epoch 1560/2000 Batch    0/1 - Training Loss:  0.173  - Validation loss: 12.124\n",
      "Epoch 1561/2000 Batch    0/1 - Training Loss:  0.173  - Validation loss: 12.127\n",
      "Epoch 1562/2000 Batch    0/1 - Training Loss:  0.172  - Validation loss: 12.130\n",
      "Epoch 1563/2000 Batch    0/1 - Training Loss:  0.172  - Validation loss: 12.129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1564/2000 Batch    0/1 - Training Loss:  0.172  - Validation loss: 12.134\n",
      "Epoch 1565/2000 Batch    0/1 - Training Loss:  0.171  - Validation loss: 12.136\n",
      "Epoch 1566/2000 Batch    0/1 - Training Loss:  0.171  - Validation loss: 12.137\n",
      "Epoch 1567/2000 Batch    0/1 - Training Loss:  0.170  - Validation loss: 12.143\n",
      "Epoch 1568/2000 Batch    0/1 - Training Loss:  0.170  - Validation loss: 12.143\n",
      "Epoch 1569/2000 Batch    0/1 - Training Loss:  0.170  - Validation loss: 12.148\n",
      "Epoch 1570/2000 Batch    0/1 - Training Loss:  0.170  - Validation loss: 12.148\n",
      "Epoch 1571/2000 Batch    0/1 - Training Loss:  0.169  - Validation loss: 12.148\n",
      "Epoch 1572/2000 Batch    0/1 - Training Loss:  0.169  - Validation loss: 12.155\n",
      "Epoch 1573/2000 Batch    0/1 - Training Loss:  0.168  - Validation loss: 12.155\n",
      "Epoch 1574/2000 Batch    0/1 - Training Loss:  0.168  - Validation loss: 12.158\n",
      "Epoch 1575/2000 Batch    0/1 - Training Loss:  0.167  - Validation loss: 12.161\n",
      "Epoch 1576/2000 Batch    0/1 - Training Loss:  0.167  - Validation loss: 12.163\n",
      "Epoch 1577/2000 Batch    0/1 - Training Loss:  0.167  - Validation loss: 12.166\n",
      "Epoch 1578/2000 Batch    0/1 - Training Loss:  0.166  - Validation loss: 12.169\n",
      "Epoch 1579/2000 Batch    0/1 - Training Loss:  0.166  - Validation loss: 12.169\n",
      "Epoch 1580/2000 Batch    0/1 - Training Loss:  0.165  - Validation loss: 12.174\n",
      "Epoch 1581/2000 Batch    0/1 - Training Loss:  0.165  - Validation loss: 12.176\n",
      "Epoch 1582/2000 Batch    0/1 - Training Loss:  0.164  - Validation loss: 12.177\n",
      "Epoch 1583/2000 Batch    0/1 - Training Loss:  0.164  - Validation loss: 12.182\n",
      "Epoch 1584/2000 Batch    0/1 - Training Loss:  0.164  - Validation loss: 12.181\n",
      "Epoch 1585/2000 Batch    0/1 - Training Loss:  0.164  - Validation loss: 12.186\n",
      "Epoch 1586/2000 Batch    0/1 - Training Loss:  0.163  - Validation loss: 12.191\n",
      "Epoch 1587/2000 Batch    0/1 - Training Loss:  0.163  - Validation loss: 12.189\n",
      "Epoch 1588/2000 Batch    0/1 - Training Loss:  0.162  - Validation loss: 12.193\n",
      "Epoch 1589/2000 Batch    0/1 - Training Loss:  0.162  - Validation loss: 12.197\n",
      "Epoch 1590/2000 Batch    0/1 - Training Loss:  0.161  - Validation loss: 12.198\n",
      "Epoch 1591/2000 Batch    0/1 - Training Loss:  0.161  - Validation loss: 12.203\n",
      "Epoch 1592/2000 Batch    0/1 - Training Loss:  0.161  - Validation loss: 12.202\n",
      "Epoch 1593/2000 Batch    0/1 - Training Loss:  0.160  - Validation loss: 12.204\n",
      "Epoch 1594/2000 Batch    0/1 - Training Loss:  0.160  - Validation loss: 12.211\n",
      "Epoch 1595/2000 Batch    0/1 - Training Loss:  0.160  - Validation loss: 12.208\n",
      "Epoch 1596/2000 Batch    0/1 - Training Loss:  0.160  - Validation loss: 12.216\n",
      "Epoch 1597/2000 Batch    0/1 - Training Loss:  0.159  - Validation loss: 12.215\n",
      "Epoch 1598/2000 Batch    0/1 - Training Loss:  0.159  - Validation loss: 12.214\n",
      "Epoch 1599/2000 Batch    0/1 - Training Loss:  0.159  - Validation loss: 12.226\n",
      "Epoch 1600/2000 Batch    0/1 - Training Loss:  0.158  - Validation loss: 12.221\n",
      "Epoch 1601/2000 Batch    0/1 - Training Loss:  0.158  - Validation loss: 12.225\n",
      "Epoch 1602/2000 Batch    0/1 - Training Loss:  0.157  - Validation loss: 12.233\n",
      "Epoch 1603/2000 Batch    0/1 - Training Loss:  0.158  - Validation loss: 12.227\n",
      "Epoch 1604/2000 Batch    0/1 - Training Loss:  0.157  - Validation loss: 12.235\n",
      "Epoch 1605/2000 Batch    0/1 - Training Loss:  0.156  - Validation loss: 12.239\n",
      "Epoch 1606/2000 Batch    0/1 - Training Loss:  0.156  - Validation loss: 12.234\n",
      "Epoch 1607/2000 Batch    0/1 - Training Loss:  0.156  - Validation loss: 12.243\n",
      "Epoch 1608/2000 Batch    0/1 - Training Loss:  0.155  - Validation loss: 12.243\n",
      "Epoch 1609/2000 Batch    0/1 - Training Loss:  0.156  - Validation loss: 12.244\n",
      "Epoch 1610/2000 Batch    0/1 - Training Loss:  0.155  - Validation loss: 12.253\n",
      "Epoch 1611/2000 Batch    0/1 - Training Loss:  0.154  - Validation loss: 12.251\n",
      "Epoch 1612/2000 Batch    0/1 - Training Loss:  0.154  - Validation loss: 12.251\n",
      "Epoch 1613/2000 Batch    0/1 - Training Loss:  0.154  - Validation loss: 12.258\n",
      "Epoch 1614/2000 Batch    0/1 - Training Loss:  0.153  - Validation loss: 12.259\n",
      "Epoch 1615/2000 Batch    0/1 - Training Loss:  0.153  - Validation loss: 12.259\n",
      "Epoch 1616/2000 Batch    0/1 - Training Loss:  0.153  - Validation loss: 12.264\n",
      "Epoch 1617/2000 Batch    0/1 - Training Loss:  0.153  - Validation loss: 12.263\n",
      "Epoch 1618/2000 Batch    0/1 - Training Loss:  0.152  - Validation loss: 12.265\n",
      "Epoch 1619/2000 Batch    0/1 - Training Loss:  0.151  - Validation loss: 12.273\n",
      "Epoch 1620/2000 Batch    0/1 - Training Loss:  0.151  - Validation loss: 12.269\n",
      "Epoch 1621/2000 Batch    0/1 - Training Loss:  0.151  - Validation loss: 12.271\n",
      "Epoch 1622/2000 Batch    0/1 - Training Loss:  0.150  - Validation loss: 12.277\n",
      "Epoch 1623/2000 Batch    0/1 - Training Loss:  0.150  - Validation loss: 12.275\n",
      "Epoch 1624/2000 Batch    0/1 - Training Loss:  0.149  - Validation loss: 12.279\n",
      "Epoch 1625/2000 Batch    0/1 - Training Loss:  0.149  - Validation loss: 12.280\n",
      "Epoch 1626/2000 Batch    0/1 - Training Loss:  0.149  - Validation loss: 12.281\n",
      "Epoch 1627/2000 Batch    0/1 - Training Loss:  0.149  - Validation loss: 12.288\n",
      "Epoch 1628/2000 Batch    0/1 - Training Loss:  0.149  - Validation loss: 12.289\n",
      "Epoch 1629/2000 Batch    0/1 - Training Loss:  0.148  - Validation loss: 12.294\n",
      "Epoch 1630/2000 Batch    0/1 - Training Loss:  0.148  - Validation loss: 12.297\n",
      "Epoch 1631/2000 Batch    0/1 - Training Loss:  0.148  - Validation loss: 12.297\n",
      "Epoch 1632/2000 Batch    0/1 - Training Loss:  0.148  - Validation loss: 12.294\n",
      "Epoch 1633/2000 Batch    0/1 - Training Loss:  0.147  - Validation loss: 12.299\n",
      "Epoch 1634/2000 Batch    0/1 - Training Loss:  0.146  - Validation loss: 12.303\n",
      "Epoch 1635/2000 Batch    0/1 - Training Loss:  0.146  - Validation loss: 12.304\n",
      "Epoch 1636/2000 Batch    0/1 - Training Loss:  0.146  - Validation loss: 12.308\n",
      "Epoch 1637/2000 Batch    0/1 - Training Loss:  0.146  - Validation loss: 12.306\n",
      "Epoch 1638/2000 Batch    0/1 - Training Loss:  0.145  - Validation loss: 12.310\n",
      "Epoch 1639/2000 Batch    0/1 - Training Loss:  0.144  - Validation loss: 12.316\n",
      "Epoch 1640/2000 Batch    0/1 - Training Loss:  0.145  - Validation loss: 12.314\n",
      "Epoch 1641/2000 Batch    0/1 - Training Loss:  0.144  - Validation loss: 12.317\n",
      "Epoch 1642/2000 Batch    0/1 - Training Loss:  0.144  - Validation loss: 12.319\n",
      "Epoch 1643/2000 Batch    0/1 - Training Loss:  0.143  - Validation loss: 12.320\n",
      "Epoch 1644/2000 Batch    0/1 - Training Loss:  0.143  - Validation loss: 12.326\n",
      "Epoch 1645/2000 Batch    0/1 - Training Loss:  0.143  - Validation loss: 12.326\n",
      "Epoch 1646/2000 Batch    0/1 - Training Loss:  0.142  - Validation loss: 12.325\n",
      "Epoch 1647/2000 Batch    0/1 - Training Loss:  0.142  - Validation loss: 12.332\n",
      "Epoch 1648/2000 Batch    0/1 - Training Loss:  0.142  - Validation loss: 12.334\n",
      "Epoch 1649/2000 Batch    0/1 - Training Loss:  0.142  - Validation loss: 12.340\n",
      "Epoch 1650/2000 Batch    0/1 - Training Loss:  0.142  - Validation loss: 12.341\n",
      "Epoch 1651/2000 Batch    0/1 - Training Loss:  0.141  - Validation loss: 12.340\n",
      "Epoch 1652/2000 Batch    0/1 - Training Loss:  0.141  - Validation loss: 12.343\n",
      "Epoch 1653/2000 Batch    0/1 - Training Loss:  0.140  - Validation loss: 12.347\n",
      "Epoch 1654/2000 Batch    0/1 - Training Loss:  0.140  - Validation loss: 12.349\n",
      "Epoch 1655/2000 Batch    0/1 - Training Loss:  0.140  - Validation loss: 12.351\n",
      "Epoch 1656/2000 Batch    0/1 - Training Loss:  0.139  - Validation loss: 12.350\n",
      "Epoch 1657/2000 Batch    0/1 - Training Loss:  0.139  - Validation loss: 12.355\n",
      "Epoch 1658/2000 Batch    0/1 - Training Loss:  0.139  - Validation loss: 12.355\n",
      "Epoch 1659/2000 Batch    0/1 - Training Loss:  0.138  - Validation loss: 12.359\n",
      "Epoch 1660/2000 Batch    0/1 - Training Loss:  0.138  - Validation loss: 12.365\n",
      "Epoch 1661/2000 Batch    0/1 - Training Loss:  0.138  - Validation loss: 12.365\n",
      "Epoch 1662/2000 Batch    0/1 - Training Loss:  0.138  - Validation loss: 12.365\n",
      "Epoch 1663/2000 Batch    0/1 - Training Loss:  0.137  - Validation loss: 12.368\n",
      "Epoch 1664/2000 Batch    0/1 - Training Loss:  0.137  - Validation loss: 12.369\n",
      "Epoch 1665/2000 Batch    0/1 - Training Loss:  0.137  - Validation loss: 12.373\n",
      "Epoch 1666/2000 Batch    0/1 - Training Loss:  0.137  - Validation loss: 12.375\n",
      "Epoch 1667/2000 Batch    0/1 - Training Loss:  0.137  - Validation loss: 12.381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1668/2000 Batch    0/1 - Training Loss:  0.136  - Validation loss: 12.381\n",
      "Epoch 1669/2000 Batch    0/1 - Training Loss:  0.136  - Validation loss: 12.383\n",
      "Epoch 1670/2000 Batch    0/1 - Training Loss:  0.135  - Validation loss: 12.388\n",
      "Epoch 1671/2000 Batch    0/1 - Training Loss:  0.135  - Validation loss: 12.391\n",
      "Epoch 1672/2000 Batch    0/1 - Training Loss:  0.135  - Validation loss: 12.391\n",
      "Epoch 1673/2000 Batch    0/1 - Training Loss:  0.134  - Validation loss: 12.395\n",
      "Epoch 1674/2000 Batch    0/1 - Training Loss:  0.134  - Validation loss: 12.394\n",
      "Epoch 1675/2000 Batch    0/1 - Training Loss:  0.134  - Validation loss: 12.397\n",
      "Epoch 1676/2000 Batch    0/1 - Training Loss:  0.133  - Validation loss: 12.404\n",
      "Epoch 1677/2000 Batch    0/1 - Training Loss:  0.133  - Validation loss: 12.402\n",
      "Epoch 1678/2000 Batch    0/1 - Training Loss:  0.133  - Validation loss: 12.403\n",
      "Epoch 1679/2000 Batch    0/1 - Training Loss:  0.132  - Validation loss: 12.405\n",
      "Epoch 1680/2000 Batch    0/1 - Training Loss:  0.132  - Validation loss: 12.406\n",
      "Epoch 1681/2000 Batch    0/1 - Training Loss:  0.132  - Validation loss: 12.411\n",
      "Epoch 1682/2000 Batch    0/1 - Training Loss:  0.132  - Validation loss: 12.413\n",
      "Epoch 1683/2000 Batch    0/1 - Training Loss:  0.131  - Validation loss: 12.416\n",
      "Epoch 1684/2000 Batch    0/1 - Training Loss:  0.131  - Validation loss: 12.417\n",
      "Epoch 1685/2000 Batch    0/1 - Training Loss:  0.131  - Validation loss: 12.421\n",
      "Epoch 1686/2000 Batch    0/1 - Training Loss:  0.130  - Validation loss: 12.425\n",
      "Epoch 1687/2000 Batch    0/1 - Training Loss:  0.130  - Validation loss: 12.425\n",
      "Epoch 1688/2000 Batch    0/1 - Training Loss:  0.130  - Validation loss: 12.428\n",
      "Epoch 1689/2000 Batch    0/1 - Training Loss:  0.129  - Validation loss: 12.428\n",
      "Epoch 1690/2000 Batch    0/1 - Training Loss:  0.129  - Validation loss: 12.431\n",
      "Epoch 1691/2000 Batch    0/1 - Training Loss:  0.129  - Validation loss: 12.437\n",
      "Epoch 1692/2000 Batch    0/1 - Training Loss:  0.129  - Validation loss: 12.436\n",
      "Epoch 1693/2000 Batch    0/1 - Training Loss:  0.128  - Validation loss: 12.440\n",
      "Epoch 1694/2000 Batch    0/1 - Training Loss:  0.128  - Validation loss: 12.441\n",
      "Epoch 1695/2000 Batch    0/1 - Training Loss:  0.128  - Validation loss: 12.443\n",
      "Epoch 1696/2000 Batch    0/1 - Training Loss:  0.128  - Validation loss: 12.450\n",
      "Epoch 1697/2000 Batch    0/1 - Training Loss:  0.128  - Validation loss: 12.447\n",
      "Epoch 1698/2000 Batch    0/1 - Training Loss:  0.127  - Validation loss: 12.448\n",
      "Epoch 1699/2000 Batch    0/1 - Training Loss:  0.127  - Validation loss: 12.454\n",
      "Epoch 1700/2000 Batch    0/1 - Training Loss:  0.126  - Validation loss: 12.455\n",
      "Epoch 1701/2000 Batch    0/1 - Training Loss:  0.126  - Validation loss: 12.457\n",
      "Epoch 1702/2000 Batch    0/1 - Training Loss:  0.126  - Validation loss: 12.459\n",
      "Epoch 1703/2000 Batch    0/1 - Training Loss:  0.125  - Validation loss: 12.461\n",
      "Epoch 1704/2000 Batch    0/1 - Training Loss:  0.125  - Validation loss: 12.462\n",
      "Epoch 1705/2000 Batch    0/1 - Training Loss:  0.125  - Validation loss: 12.466\n",
      "Epoch 1706/2000 Batch    0/1 - Training Loss:  0.125  - Validation loss: 12.468\n",
      "Epoch 1707/2000 Batch    0/1 - Training Loss:  0.124  - Validation loss: 12.470\n",
      "Epoch 1708/2000 Batch    0/1 - Training Loss:  0.124  - Validation loss: 12.471\n",
      "Epoch 1709/2000 Batch    0/1 - Training Loss:  0.124  - Validation loss: 12.475\n",
      "Epoch 1710/2000 Batch    0/1 - Training Loss:  0.124  - Validation loss: 12.474\n",
      "Epoch 1711/2000 Batch    0/1 - Training Loss:  0.123  - Validation loss: 12.477\n",
      "Epoch 1712/2000 Batch    0/1 - Training Loss:  0.123  - Validation loss: 12.481\n",
      "Epoch 1713/2000 Batch    0/1 - Training Loss:  0.123  - Validation loss: 12.481\n",
      "Epoch 1714/2000 Batch    0/1 - Training Loss:  0.123  - Validation loss: 12.484\n",
      "Epoch 1715/2000 Batch    0/1 - Training Loss:  0.123  - Validation loss: 12.486\n",
      "Epoch 1716/2000 Batch    0/1 - Training Loss:  0.122  - Validation loss: 12.490\n",
      "Epoch 1717/2000 Batch    0/1 - Training Loss:  0.122  - Validation loss: 12.493\n",
      "Epoch 1718/2000 Batch    0/1 - Training Loss:  0.122  - Validation loss: 12.493\n",
      "Epoch 1719/2000 Batch    0/1 - Training Loss:  0.122  - Validation loss: 12.495\n",
      "Epoch 1720/2000 Batch    0/1 - Training Loss:  0.121  - Validation loss: 12.493\n",
      "Epoch 1721/2000 Batch    0/1 - Training Loss:  0.121  - Validation loss: 12.494\n",
      "Epoch 1722/2000 Batch    0/1 - Training Loss:  0.121  - Validation loss: 12.499\n",
      "Epoch 1723/2000 Batch    0/1 - Training Loss:  0.121  - Validation loss: 12.496\n",
      "Epoch 1724/2000 Batch    0/1 - Training Loss:  0.120  - Validation loss: 12.498\n",
      "Epoch 1725/2000 Batch    0/1 - Training Loss:  0.120  - Validation loss: 12.501\n",
      "Epoch 1726/2000 Batch    0/1 - Training Loss:  0.120  - Validation loss: 12.504\n",
      "Epoch 1727/2000 Batch    0/1 - Training Loss:  0.120  - Validation loss: 12.510\n",
      "Epoch 1728/2000 Batch    0/1 - Training Loss:  0.120  - Validation loss: 12.506\n",
      "Epoch 1729/2000 Batch    0/1 - Training Loss:  0.119  - Validation loss: 12.504\n",
      "Epoch 1730/2000 Batch    0/1 - Training Loss:  0.119  - Validation loss: 12.512\n",
      "Epoch 1731/2000 Batch    0/1 - Training Loss:  0.120  - Validation loss: 12.512\n",
      "Epoch 1732/2000 Batch    0/1 - Training Loss:  0.119  - Validation loss: 12.512\n",
      "Epoch 1733/2000 Batch    0/1 - Training Loss:  0.118  - Validation loss: 12.521\n",
      "Epoch 1734/2000 Batch    0/1 - Training Loss:  0.119  - Validation loss: 12.517\n",
      "Epoch 1735/2000 Batch    0/1 - Training Loss:  0.118  - Validation loss: 12.517\n",
      "Epoch 1736/2000 Batch    0/1 - Training Loss:  0.118  - Validation loss: 12.528\n",
      "Epoch 1737/2000 Batch    0/1 - Training Loss:  0.119  - Validation loss: 12.525\n",
      "Epoch 1738/2000 Batch    0/1 - Training Loss:  0.118  - Validation loss: 12.522\n",
      "Epoch 1739/2000 Batch    0/1 - Training Loss:  0.117  - Validation loss: 12.531\n",
      "Epoch 1740/2000 Batch    0/1 - Training Loss:  0.118  - Validation loss: 12.529\n",
      "Epoch 1741/2000 Batch    0/1 - Training Loss:  0.117  - Validation loss: 12.528\n",
      "Epoch 1742/2000 Batch    0/1 - Training Loss:  0.116  - Validation loss: 12.536\n",
      "Epoch 1743/2000 Batch    0/1 - Training Loss:  0.117  - Validation loss: 12.536\n",
      "Epoch 1744/2000 Batch    0/1 - Training Loss:  0.116  - Validation loss: 12.535\n",
      "Epoch 1745/2000 Batch    0/1 - Training Loss:  0.116  - Validation loss: 12.543\n",
      "Epoch 1746/2000 Batch    0/1 - Training Loss:  0.116  - Validation loss: 12.543\n",
      "Epoch 1747/2000 Batch    0/1 - Training Loss:  0.115  - Validation loss: 12.541\n",
      "Epoch 1748/2000 Batch    0/1 - Training Loss:  0.115  - Validation loss: 12.547\n",
      "Epoch 1749/2000 Batch    0/1 - Training Loss:  0.116  - Validation loss: 12.544\n",
      "Epoch 1750/2000 Batch    0/1 - Training Loss:  0.115  - Validation loss: 12.546\n",
      "Epoch 1751/2000 Batch    0/1 - Training Loss:  0.114  - Validation loss: 12.554\n",
      "Epoch 1752/2000 Batch    0/1 - Training Loss:  0.115  - Validation loss: 12.553\n",
      "Epoch 1753/2000 Batch    0/1 - Training Loss:  0.115  - Validation loss: 12.553\n",
      "Epoch 1754/2000 Batch    0/1 - Training Loss:  0.113  - Validation loss: 12.553\n",
      "Epoch 1755/2000 Batch    0/1 - Training Loss:  0.114  - Validation loss: 12.555\n",
      "Epoch 1756/2000 Batch    0/1 - Training Loss:  0.115  - Validation loss: 12.562\n",
      "Epoch 1757/2000 Batch    0/1 - Training Loss:  0.113  - Validation loss: 12.562\n",
      "Epoch 1758/2000 Batch    0/1 - Training Loss:  0.114  - Validation loss: 12.566\n",
      "Epoch 1759/2000 Batch    0/1 - Training Loss:  0.113  - Validation loss: 12.567\n",
      "Epoch 1760/2000 Batch    0/1 - Training Loss:  0.113  - Validation loss: 12.568\n",
      "Epoch 1761/2000 Batch    0/1 - Training Loss:  0.113  - Validation loss: 12.571\n",
      "Epoch 1762/2000 Batch    0/1 - Training Loss:  0.112  - Validation loss: 12.575\n",
      "Epoch 1763/2000 Batch    0/1 - Training Loss:  0.112  - Validation loss: 12.570\n",
      "Epoch 1764/2000 Batch    0/1 - Training Loss:  0.111  - Validation loss: 12.569\n",
      "Epoch 1765/2000 Batch    0/1 - Training Loss:  0.111  - Validation loss: 12.574\n",
      "Epoch 1766/2000 Batch    0/1 - Training Loss:  0.111  - Validation loss: 12.577\n",
      "Epoch 1767/2000 Batch    0/1 - Training Loss:  0.110  - Validation loss: 12.581\n",
      "Epoch 1768/2000 Batch    0/1 - Training Loss:  0.111  - Validation loss: 12.580\n",
      "Epoch 1769/2000 Batch    0/1 - Training Loss:  0.110  - Validation loss: 12.582\n",
      "Epoch 1770/2000 Batch    0/1 - Training Loss:  0.110  - Validation loss: 12.581\n",
      "Epoch 1771/2000 Batch    0/1 - Training Loss:  0.109  - Validation loss: 12.583\n",
      "Epoch 1772/2000 Batch    0/1 - Training Loss:  0.109  - Validation loss: 12.586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1773/2000 Batch    0/1 - Training Loss:  0.109  - Validation loss: 12.582\n",
      "Epoch 1774/2000 Batch    0/1 - Training Loss:  0.109  - Validation loss: 12.585\n",
      "Epoch 1775/2000 Batch    0/1 - Training Loss:  0.109  - Validation loss: 12.587\n",
      "Epoch 1776/2000 Batch    0/1 - Training Loss:  0.108  - Validation loss: 12.592\n",
      "Epoch 1777/2000 Batch    0/1 - Training Loss:  0.108  - Validation loss: 12.596\n",
      "Epoch 1778/2000 Batch    0/1 - Training Loss:  0.108  - Validation loss: 12.596\n",
      "Epoch 1779/2000 Batch    0/1 - Training Loss:  0.108  - Validation loss: 12.599\n",
      "Epoch 1780/2000 Batch    0/1 - Training Loss:  0.108  - Validation loss: 12.599\n",
      "Epoch 1781/2000 Batch    0/1 - Training Loss:  0.107  - Validation loss: 12.601\n",
      "Epoch 1782/2000 Batch    0/1 - Training Loss:  0.107  - Validation loss: 12.604\n",
      "Epoch 1783/2000 Batch    0/1 - Training Loss:  0.107  - Validation loss: 12.604\n",
      "Epoch 1784/2000 Batch    0/1 - Training Loss:  0.107  - Validation loss: 12.609\n",
      "Epoch 1785/2000 Batch    0/1 - Training Loss:  0.107  - Validation loss: 12.608\n",
      "Epoch 1786/2000 Batch    0/1 - Training Loss:  0.106  - Validation loss: 12.608\n",
      "Epoch 1787/2000 Batch    0/1 - Training Loss:  0.106  - Validation loss: 12.616\n",
      "Epoch 1788/2000 Batch    0/1 - Training Loss:  0.106  - Validation loss: 12.617\n",
      "Epoch 1789/2000 Batch    0/1 - Training Loss:  0.105  - Validation loss: 12.615\n",
      "Epoch 1790/2000 Batch    0/1 - Training Loss:  0.105  - Validation loss: 12.620\n",
      "Epoch 1791/2000 Batch    0/1 - Training Loss:  0.106  - Validation loss: 12.617\n",
      "Epoch 1792/2000 Batch    0/1 - Training Loss:  0.105  - Validation loss: 12.619\n",
      "Epoch 1793/2000 Batch    0/1 - Training Loss:  0.105  - Validation loss: 12.627\n",
      "Epoch 1794/2000 Batch    0/1 - Training Loss:  0.106  - Validation loss: 12.624\n",
      "Epoch 1795/2000 Batch    0/1 - Training Loss:  0.104  - Validation loss: 12.621\n",
      "Epoch 1796/2000 Batch    0/1 - Training Loss:  0.105  - Validation loss: 12.633\n",
      "Epoch 1797/2000 Batch    0/1 - Training Loss:  0.106  - Validation loss: 12.631\n",
      "Epoch 1798/2000 Batch    0/1 - Training Loss:  0.104  - Validation loss: 12.626\n",
      "Epoch 1799/2000 Batch    0/1 - Training Loss:  0.105  - Validation loss: 12.641\n",
      "Epoch 1800/2000 Batch    0/1 - Training Loss:  0.106  - Validation loss: 12.639\n",
      "Epoch 1801/2000 Batch    0/1 - Training Loss:  0.104  - Validation loss: 12.631\n",
      "Epoch 1802/2000 Batch    0/1 - Training Loss:  0.105  - Validation loss: 12.644\n",
      "Epoch 1803/2000 Batch    0/1 - Training Loss:  0.107  - Validation loss: 12.648\n",
      "Epoch 1804/2000 Batch    0/1 - Training Loss:  0.104  - Validation loss: 12.638\n",
      "Epoch 1805/2000 Batch    0/1 - Training Loss:  0.104  - Validation loss: 12.646\n",
      "Epoch 1806/2000 Batch    0/1 - Training Loss:  0.105  - Validation loss: 12.653\n",
      "Epoch 1807/2000 Batch    0/1 - Training Loss:  0.103  - Validation loss: 12.648\n",
      "Epoch 1808/2000 Batch    0/1 - Training Loss:  0.104  - Validation loss: 12.652\n",
      "Epoch 1809/2000 Batch    0/1 - Training Loss:  0.104  - Validation loss: 12.653\n",
      "Epoch 1810/2000 Batch    0/1 - Training Loss:  0.102  - Validation loss: 12.653\n",
      "Epoch 1811/2000 Batch    0/1 - Training Loss:  0.104  - Validation loss: 12.658\n",
      "Epoch 1812/2000 Batch    0/1 - Training Loss:  0.102  - Validation loss: 12.657\n",
      "Epoch 1813/2000 Batch    0/1 - Training Loss:  0.102  - Validation loss: 12.658\n",
      "Epoch 1814/2000 Batch    0/1 - Training Loss:  0.103  - Validation loss: 12.663\n",
      "Epoch 1815/2000 Batch    0/1 - Training Loss:  0.101  - Validation loss: 12.665\n",
      "Epoch 1816/2000 Batch    0/1 - Training Loss:  0.102  - Validation loss: 12.663\n",
      "Epoch 1817/2000 Batch    0/1 - Training Loss:  0.101  - Validation loss: 12.668\n",
      "Epoch 1818/2000 Batch    0/1 - Training Loss:  0.100  - Validation loss: 12.672\n",
      "Epoch 1819/2000 Batch    0/1 - Training Loss:  0.102  - Validation loss: 12.668\n",
      "Epoch 1820/2000 Batch    0/1 - Training Loss:  0.100  - Validation loss: 12.671\n",
      "Epoch 1821/2000 Batch    0/1 - Training Loss:  0.100  - Validation loss: 12.675\n",
      "Epoch 1822/2000 Batch    0/1 - Training Loss:  0.100  - Validation loss: 12.674\n",
      "Epoch 1823/2000 Batch    0/1 - Training Loss:  0.099  - Validation loss: 12.677\n",
      "Epoch 1824/2000 Batch    0/1 - Training Loss:  0.100  - Validation loss: 12.685\n",
      "Epoch 1825/2000 Batch    0/1 - Training Loss:  0.100  - Validation loss: 12.679\n",
      "Epoch 1826/2000 Batch    0/1 - Training Loss:  0.099  - Validation loss: 12.674\n",
      "Epoch 1827/2000 Batch    0/1 - Training Loss:  0.100  - Validation loss: 12.687\n",
      "Epoch 1828/2000 Batch    0/1 - Training Loss:  0.099  - Validation loss: 12.689\n",
      "Epoch 1829/2000 Batch    0/1 - Training Loss:  0.098  - Validation loss: 12.683\n",
      "Epoch 1830/2000 Batch    0/1 - Training Loss:  0.099  - Validation loss: 12.689\n",
      "Epoch 1831/2000 Batch    0/1 - Training Loss:  0.098  - Validation loss: 12.693\n",
      "Epoch 1832/2000 Batch    0/1 - Training Loss:  0.098  - Validation loss: 12.688\n",
      "Epoch 1833/2000 Batch    0/1 - Training Loss:  0.097  - Validation loss: 12.691\n",
      "Epoch 1834/2000 Batch    0/1 - Training Loss:  0.097  - Validation loss: 12.699\n",
      "Epoch 1835/2000 Batch    0/1 - Training Loss:  0.097  - Validation loss: 12.699\n",
      "Epoch 1836/2000 Batch    0/1 - Training Loss:  0.097  - Validation loss: 12.698\n",
      "Epoch 1837/2000 Batch    0/1 - Training Loss:  0.096  - Validation loss: 12.701\n",
      "Epoch 1838/2000 Batch    0/1 - Training Loss:  0.096  - Validation loss: 12.704\n",
      "Epoch 1839/2000 Batch    0/1 - Training Loss:  0.096  - Validation loss: 12.707\n",
      "Epoch 1840/2000 Batch    0/1 - Training Loss:  0.096  - Validation loss: 12.710\n",
      "Epoch 1841/2000 Batch    0/1 - Training Loss:  0.096  - Validation loss: 12.709\n",
      "Epoch 1842/2000 Batch    0/1 - Training Loss:  0.095  - Validation loss: 12.711\n",
      "Epoch 1843/2000 Batch    0/1 - Training Loss:  0.095  - Validation loss: 12.713\n",
      "Epoch 1844/2000 Batch    0/1 - Training Loss:  0.095  - Validation loss: 12.716\n",
      "Epoch 1845/2000 Batch    0/1 - Training Loss:  0.095  - Validation loss: 12.722\n",
      "Epoch 1846/2000 Batch    0/1 - Training Loss:  0.095  - Validation loss: 12.720\n",
      "Epoch 1847/2000 Batch    0/1 - Training Loss:  0.094  - Validation loss: 12.721\n",
      "Epoch 1848/2000 Batch    0/1 - Training Loss:  0.094  - Validation loss: 12.724\n",
      "Epoch 1849/2000 Batch    0/1 - Training Loss:  0.094  - Validation loss: 12.725\n",
      "Epoch 1850/2000 Batch    0/1 - Training Loss:  0.094  - Validation loss: 12.729\n",
      "Epoch 1851/2000 Batch    0/1 - Training Loss:  0.094  - Validation loss: 12.730\n",
      "Epoch 1852/2000 Batch    0/1 - Training Loss:  0.093  - Validation loss: 12.733\n",
      "Epoch 1853/2000 Batch    0/1 - Training Loss:  0.093  - Validation loss: 12.732\n",
      "Epoch 1854/2000 Batch    0/1 - Training Loss:  0.093  - Validation loss: 12.735\n",
      "Epoch 1855/2000 Batch    0/1 - Training Loss:  0.093  - Validation loss: 12.739\n",
      "Epoch 1856/2000 Batch    0/1 - Training Loss:  0.093  - Validation loss: 12.738\n",
      "Epoch 1857/2000 Batch    0/1 - Training Loss:  0.093  - Validation loss: 12.741\n",
      "Epoch 1858/2000 Batch    0/1 - Training Loss:  0.092  - Validation loss: 12.741\n",
      "Epoch 1859/2000 Batch    0/1 - Training Loss:  0.092  - Validation loss: 12.743\n",
      "Epoch 1860/2000 Batch    0/1 - Training Loss:  0.092  - Validation loss: 12.747\n",
      "Epoch 1861/2000 Batch    0/1 - Training Loss:  0.092  - Validation loss: 12.748\n",
      "Epoch 1862/2000 Batch    0/1 - Training Loss:  0.092  - Validation loss: 12.750\n",
      "Epoch 1863/2000 Batch    0/1 - Training Loss:  0.092  - Validation loss: 12.750\n",
      "Epoch 1864/2000 Batch    0/1 - Training Loss:  0.091  - Validation loss: 12.752\n",
      "Epoch 1865/2000 Batch    0/1 - Training Loss:  0.091  - Validation loss: 12.757\n",
      "Epoch 1866/2000 Batch    0/1 - Training Loss:  0.091  - Validation loss: 12.756\n",
      "Epoch 1867/2000 Batch    0/1 - Training Loss:  0.091  - Validation loss: 12.759\n",
      "Epoch 1868/2000 Batch    0/1 - Training Loss:  0.091  - Validation loss: 12.760\n",
      "Epoch 1869/2000 Batch    0/1 - Training Loss:  0.090  - Validation loss: 12.761\n",
      "Epoch 1870/2000 Batch    0/1 - Training Loss:  0.090  - Validation loss: 12.766\n",
      "Epoch 1871/2000 Batch    0/1 - Training Loss:  0.090  - Validation loss: 12.765\n",
      "Epoch 1872/2000 Batch    0/1 - Training Loss:  0.090  - Validation loss: 12.768\n",
      "Epoch 1873/2000 Batch    0/1 - Training Loss:  0.090  - Validation loss: 12.769\n",
      "Epoch 1874/2000 Batch    0/1 - Training Loss:  0.090  - Validation loss: 12.770\n",
      "Epoch 1875/2000 Batch    0/1 - Training Loss:  0.090  - Validation loss: 12.775\n",
      "Epoch 1876/2000 Batch    0/1 - Training Loss:  0.090  - Validation loss: 12.773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1877/2000 Batch    0/1 - Training Loss:  0.089  - Validation loss: 12.776\n",
      "Epoch 1878/2000 Batch    0/1 - Training Loss:  0.089  - Validation loss: 12.778\n",
      "Epoch 1879/2000 Batch    0/1 - Training Loss:  0.089  - Validation loss: 12.779\n",
      "Epoch 1880/2000 Batch    0/1 - Training Loss:  0.089  - Validation loss: 12.784\n",
      "Epoch 1881/2000 Batch    0/1 - Training Loss:  0.089  - Validation loss: 12.782\n",
      "Epoch 1882/2000 Batch    0/1 - Training Loss:  0.088  - Validation loss: 12.785\n",
      "Epoch 1883/2000 Batch    0/1 - Training Loss:  0.088  - Validation loss: 12.787\n",
      "Epoch 1884/2000 Batch    0/1 - Training Loss:  0.088  - Validation loss: 12.789\n",
      "Epoch 1885/2000 Batch    0/1 - Training Loss:  0.088  - Validation loss: 12.793\n",
      "Epoch 1886/2000 Batch    0/1 - Training Loss:  0.088  - Validation loss: 12.791\n",
      "Epoch 1887/2000 Batch    0/1 - Training Loss:  0.088  - Validation loss: 12.793\n",
      "Epoch 1888/2000 Batch    0/1 - Training Loss:  0.087  - Validation loss: 12.796\n",
      "Epoch 1889/2000 Batch    0/1 - Training Loss:  0.087  - Validation loss: 12.797\n",
      "Epoch 1890/2000 Batch    0/1 - Training Loss:  0.087  - Validation loss: 12.801\n",
      "Epoch 1891/2000 Batch    0/1 - Training Loss:  0.087  - Validation loss: 12.799\n",
      "Epoch 1892/2000 Batch    0/1 - Training Loss:  0.087  - Validation loss: 12.802\n",
      "Epoch 1893/2000 Batch    0/1 - Training Loss:  0.087  - Validation loss: 12.805\n",
      "Epoch 1894/2000 Batch    0/1 - Training Loss:  0.086  - Validation loss: 12.806\n",
      "Epoch 1895/2000 Batch    0/1 - Training Loss:  0.086  - Validation loss: 12.810\n",
      "Epoch 1896/2000 Batch    0/1 - Training Loss:  0.086  - Validation loss: 12.808\n",
      "Epoch 1897/2000 Batch    0/1 - Training Loss:  0.086  - Validation loss: 12.810\n",
      "Epoch 1898/2000 Batch    0/1 - Training Loss:  0.086  - Validation loss: 12.813\n",
      "Epoch 1899/2000 Batch    0/1 - Training Loss:  0.086  - Validation loss: 12.814\n",
      "Epoch 1900/2000 Batch    0/1 - Training Loss:  0.085  - Validation loss: 12.819\n",
      "Epoch 1901/2000 Batch    0/1 - Training Loss:  0.085  - Validation loss: 12.816\n",
      "Epoch 1902/2000 Batch    0/1 - Training Loss:  0.085  - Validation loss: 12.818\n",
      "Epoch 1903/2000 Batch    0/1 - Training Loss:  0.085  - Validation loss: 12.822\n",
      "Epoch 1904/2000 Batch    0/1 - Training Loss:  0.085  - Validation loss: 12.823\n",
      "Epoch 1905/2000 Batch    0/1 - Training Loss:  0.085  - Validation loss: 12.826\n",
      "Epoch 1906/2000 Batch    0/1 - Training Loss:  0.085  - Validation loss: 12.824\n",
      "Epoch 1907/2000 Batch    0/1 - Training Loss:  0.084  - Validation loss: 12.827\n",
      "Epoch 1908/2000 Batch    0/1 - Training Loss:  0.084  - Validation loss: 12.829\n",
      "Epoch 1909/2000 Batch    0/1 - Training Loss:  0.084  - Validation loss: 12.831\n",
      "Epoch 1910/2000 Batch    0/1 - Training Loss:  0.084  - Validation loss: 12.835\n",
      "Epoch 1911/2000 Batch    0/1 - Training Loss:  0.084  - Validation loss: 12.832\n",
      "Epoch 1912/2000 Batch    0/1 - Training Loss:  0.084  - Validation loss: 12.835\n",
      "Epoch 1913/2000 Batch    0/1 - Training Loss:  0.083  - Validation loss: 12.838\n",
      "Epoch 1914/2000 Batch    0/1 - Training Loss:  0.083  - Validation loss: 12.839\n",
      "Epoch 1915/2000 Batch    0/1 - Training Loss:  0.083  - Validation loss: 12.843\n",
      "Epoch 1916/2000 Batch    0/1 - Training Loss:  0.083  - Validation loss: 12.841\n",
      "Epoch 1917/2000 Batch    0/1 - Training Loss:  0.083  - Validation loss: 12.842\n",
      "Epoch 1918/2000 Batch    0/1 - Training Loss:  0.083  - Validation loss: 12.847\n",
      "Epoch 1919/2000 Batch    0/1 - Training Loss:  0.083  - Validation loss: 12.847\n",
      "Epoch 1920/2000 Batch    0/1 - Training Loss:  0.082  - Validation loss: 12.848\n",
      "Epoch 1921/2000 Batch    0/1 - Training Loss:  0.082  - Validation loss: 12.851\n",
      "Epoch 1922/2000 Batch    0/1 - Training Loss:  0.082  - Validation loss: 12.851\n",
      "Epoch 1923/2000 Batch    0/1 - Training Loss:  0.082  - Validation loss: 12.854\n",
      "Epoch 1924/2000 Batch    0/1 - Training Loss:  0.082  - Validation loss: 12.854\n",
      "Epoch 1925/2000 Batch    0/1 - Training Loss:  0.082  - Validation loss: 12.857\n",
      "Epoch 1926/2000 Batch    0/1 - Training Loss:  0.081  - Validation loss: 12.862\n",
      "Epoch 1927/2000 Batch    0/1 - Training Loss:  0.082  - Validation loss: 12.858\n",
      "Epoch 1928/2000 Batch    0/1 - Training Loss:  0.081  - Validation loss: 12.858\n",
      "Epoch 1929/2000 Batch    0/1 - Training Loss:  0.081  - Validation loss: 12.868\n",
      "Epoch 1930/2000 Batch    0/1 - Training Loss:  0.082  - Validation loss: 12.864\n",
      "Epoch 1931/2000 Batch    0/1 - Training Loss:  0.081  - Validation loss: 12.862\n",
      "Epoch 1932/2000 Batch    0/1 - Training Loss:  0.081  - Validation loss: 12.875\n",
      "Epoch 1933/2000 Batch    0/1 - Training Loss:  0.082  - Validation loss: 12.869\n",
      "Epoch 1934/2000 Batch    0/1 - Training Loss:  0.081  - Validation loss: 12.860\n",
      "Epoch 1935/2000 Batch    0/1 - Training Loss:  0.081  - Validation loss: 12.877\n",
      "Epoch 1936/2000 Batch    0/1 - Training Loss:  0.082  - Validation loss: 12.882\n",
      "Epoch 1937/2000 Batch    0/1 - Training Loss:  0.081  - Validation loss: 12.871\n",
      "Epoch 1938/2000 Batch    0/1 - Training Loss:  0.081  - Validation loss: 12.879\n",
      "Epoch 1939/2000 Batch    0/1 - Training Loss:  0.081  - Validation loss: 12.885\n",
      "Epoch 1940/2000 Batch    0/1 - Training Loss:  0.080  - Validation loss: 12.877\n",
      "Epoch 1941/2000 Batch    0/1 - Training Loss:  0.081  - Validation loss: 12.874\n",
      "Epoch 1942/2000 Batch    0/1 - Training Loss:  0.080  - Validation loss: 12.885\n",
      "Epoch 1943/2000 Batch    0/1 - Training Loss:  0.080  - Validation loss: 12.888\n",
      "Epoch 1944/2000 Batch    0/1 - Training Loss:  0.080  - Validation loss: 12.884\n",
      "Epoch 1945/2000 Batch    0/1 - Training Loss:  0.079  - Validation loss: 12.880\n",
      "Epoch 1946/2000 Batch    0/1 - Training Loss:  0.079  - Validation loss: 12.890\n",
      "Epoch 1947/2000 Batch    0/1 - Training Loss:  0.079  - Validation loss: 12.897\n",
      "Epoch 1948/2000 Batch    0/1 - Training Loss:  0.079  - Validation loss: 12.892\n",
      "Epoch 1949/2000 Batch    0/1 - Training Loss:  0.079  - Validation loss: 12.892\n",
      "Epoch 1950/2000 Batch    0/1 - Training Loss:  0.078  - Validation loss: 12.903\n",
      "Epoch 1951/2000 Batch    0/1 - Training Loss:  0.078  - Validation loss: 12.899\n",
      "Epoch 1952/2000 Batch    0/1 - Training Loss:  0.078  - Validation loss: 12.892\n",
      "Epoch 1953/2000 Batch    0/1 - Training Loss:  0.078  - Validation loss: 12.905\n",
      "Epoch 1954/2000 Batch    0/1 - Training Loss:  0.078  - Validation loss: 12.908\n",
      "Epoch 1955/2000 Batch    0/1 - Training Loss:  0.078  - Validation loss: 12.898\n",
      "Epoch 1956/2000 Batch    0/1 - Training Loss:  0.077  - Validation loss: 12.900\n",
      "Epoch 1957/2000 Batch    0/1 - Training Loss:  0.077  - Validation loss: 12.911\n",
      "Epoch 1958/2000 Batch    0/1 - Training Loss:  0.077  - Validation loss: 12.910\n",
      "Epoch 1959/2000 Batch    0/1 - Training Loss:  0.077  - Validation loss: 12.905\n",
      "Epoch 1960/2000 Batch    0/1 - Training Loss:  0.077  - Validation loss: 12.916\n",
      "Epoch 1961/2000 Batch    0/1 - Training Loss:  0.077  - Validation loss: 12.917\n",
      "Epoch 1962/2000 Batch    0/1 - Training Loss:  0.076  - Validation loss: 12.911\n",
      "Epoch 1963/2000 Batch    0/1 - Training Loss:  0.076  - Validation loss: 12.916\n",
      "Epoch 1964/2000 Batch    0/1 - Training Loss:  0.077  - Validation loss: 12.918\n",
      "Epoch 1965/2000 Batch    0/1 - Training Loss:  0.076  - Validation loss: 12.916\n",
      "Epoch 1966/2000 Batch    0/1 - Training Loss:  0.076  - Validation loss: 12.918\n",
      "Epoch 1967/2000 Batch    0/1 - Training Loss:  0.076  - Validation loss: 12.926\n",
      "Epoch 1968/2000 Batch    0/1 - Training Loss:  0.076  - Validation loss: 12.925\n",
      "Epoch 1969/2000 Batch    0/1 - Training Loss:  0.076  - Validation loss: 12.927\n",
      "Epoch 1970/2000 Batch    0/1 - Training Loss:  0.076  - Validation loss: 12.928\n",
      "Epoch 1971/2000 Batch    0/1 - Training Loss:  0.075  - Validation loss: 12.928\n",
      "Epoch 1972/2000 Batch    0/1 - Training Loss:  0.075  - Validation loss: 12.930\n",
      "Epoch 1973/2000 Batch    0/1 - Training Loss:  0.075  - Validation loss: 12.932\n",
      "Epoch 1974/2000 Batch    0/1 - Training Loss:  0.075  - Validation loss: 12.933\n",
      "Epoch 1975/2000 Batch    0/1 - Training Loss:  0.075  - Validation loss: 12.936\n",
      "Epoch 1976/2000 Batch    0/1 - Training Loss:  0.075  - Validation loss: 12.933\n",
      "Epoch 1977/2000 Batch    0/1 - Training Loss:  0.074  - Validation loss: 12.935\n",
      "Epoch 1978/2000 Batch    0/1 - Training Loss:  0.074  - Validation loss: 12.944\n",
      "Epoch 1979/2000 Batch    0/1 - Training Loss:  0.074  - Validation loss: 12.943\n",
      "Epoch 1980/2000 Batch    0/1 - Training Loss:  0.074  - Validation loss: 12.940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1981/2000 Batch    0/1 - Training Loss:  0.074  - Validation loss: 12.946\n",
      "Epoch 1982/2000 Batch    0/1 - Training Loss:  0.074  - Validation loss: 12.948\n",
      "Epoch 1983/2000 Batch    0/1 - Training Loss:  0.074  - Validation loss: 12.945\n",
      "Epoch 1984/2000 Batch    0/1 - Training Loss:  0.074  - Validation loss: 12.949\n",
      "Epoch 1985/2000 Batch    0/1 - Training Loss:  0.073  - Validation loss: 12.956\n",
      "Epoch 1986/2000 Batch    0/1 - Training Loss:  0.073  - Validation loss: 12.954\n",
      "Epoch 1987/2000 Batch    0/1 - Training Loss:  0.073  - Validation loss: 12.951\n",
      "Epoch 1988/2000 Batch    0/1 - Training Loss:  0.073  - Validation loss: 12.954\n",
      "Epoch 1989/2000 Batch    0/1 - Training Loss:  0.073  - Validation loss: 12.959\n",
      "Epoch 1990/2000 Batch    0/1 - Training Loss:  0.073  - Validation loss: 12.961\n",
      "Epoch 1991/2000 Batch    0/1 - Training Loss:  0.072  - Validation loss: 12.961\n",
      "Epoch 1992/2000 Batch    0/1 - Training Loss:  0.072  - Validation loss: 12.964\n",
      "Epoch 1993/2000 Batch    0/1 - Training Loss:  0.072  - Validation loss: 12.965\n",
      "Epoch 1994/2000 Batch    0/1 - Training Loss:  0.072  - Validation loss: 12.965\n",
      "Epoch 1995/2000 Batch    0/1 - Training Loss:  0.072  - Validation loss: 12.967\n",
      "Epoch 1996/2000 Batch    0/1 - Training Loss:  0.072  - Validation loss: 12.969\n",
      "Epoch 1997/2000 Batch    0/1 - Training Loss:  0.072  - Validation loss: 12.969\n",
      "Epoch 1998/2000 Batch    0/1 - Training Loss:  0.071  - Validation loss: 12.970\n",
      "Epoch 1999/2000 Batch    0/1 - Training Loss:  0.071  - Validation loss: 12.973\n",
      "Epoch 2000/2000 Batch    0/1 - Training Loss:  0.071  - Validation loss: 12.975\n",
      "Model Trained and Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from data/trained_model.ckpt\n",
      "原始输入: 戌岁祝福万事顺\n",
      "\n",
      "Source\n",
      "  Word 编号:    [517, 43, 112, 111, 525, 699, 622]\n",
      "  Input Words: 戌 岁 祝 福 万 事 顺\n",
      "\n",
      "Target\n",
      "  Word 编号:       [522, 43, 630, 111, 666, 441, 62]\n",
      "  Response Words: 狗 岁 年 福 足 食 人\n"
     ]
    }
   ],
   "source": [
    "#鸡鸣万户晓 鹤舞一年春\n",
    "#戌岁祝福万事顺 狗年兆丰五谷香\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# 超参数\n",
    "# Number of Epochs\n",
    "epochs = 2000\n",
    "# Batch Size\n",
    "batch_size = 100\n",
    "# RNN Size\n",
    "rnn_size = 50\n",
    "# Number of Layers\n",
    "num_layers = 3\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 15\n",
    "decoding_embedding_size = 15\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "source = open(\"./source.txt\",'w')\n",
    "target = open(\"./target.txt\",'w')\n",
    "\n",
    "with open(\"./对联.txt\",'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip().split(\" \")\n",
    "        source.write(line[0]+'\\n')\n",
    "        target.write(line[1]+'\\n')\n",
    "source.close()\n",
    "target.close()\n",
    "\n",
    "with open('./source.txt','r',encoding='utf-8') as f:\n",
    "    source_data = f.read()\n",
    "\n",
    "with open('./target.txt','r',encoding='utf-8') as f:\n",
    "    target_data = f.read()\n",
    "\n",
    "print(source_data.split('\\n')[:10])\n",
    "print(target_data.split('\\n')[:10])\n",
    "\n",
    "\n",
    "def extract_character_vocab(data):\n",
    "    \"\"\"\n",
    "    :param data:\n",
    "    :return: 字符映射表\n",
    "    \"\"\"\n",
    "    special_words = ['<PAD>','<UNK>','<GO>','<EOS>']\n",
    "    set_words = list(set([character for line in data.split('\\n') for character in line]))\n",
    "    int_to_vocab = {idx:word for idx,word in enumerate(special_words + set_words)}\n",
    "    vocab_to_int = {word:idx for idx,word in int_to_vocab.items()}\n",
    "\n",
    "    return int_to_vocab,vocab_to_int\n",
    "\n",
    "# 得到输入和输出的字符映射表\n",
    "source_int_to_letter,source_letter_to_int = extract_character_vocab(source_data+target_data)\n",
    "target_int_to_letter,target_letter_to_int = extract_character_vocab(source_data+target_data)\n",
    "\n",
    "# 将每一行转换成字符id的list\n",
    "source_int = [[source_letter_to_int.get(letter,source_letter_to_int['<UNK>'])\n",
    "               for letter in line] for line in source_data.split('\\n')]\n",
    "\n",
    "target_int = [[target_letter_to_int.get(letter, target_letter_to_int['<UNK>'])\n",
    "               for letter in line] + [target_letter_to_int['<EOS>']] for line in target_data.split('\\n')]\n",
    "\n",
    "print(source_int)\n",
    "print(target_int)\n",
    "\n",
    "# 输入层\n",
    "def get_inputs():\n",
    "\n",
    "    inputs = tf.placeholder(tf.int32,[None,None],name='inputs')\n",
    "    targets = tf.placeholder(tf.int32,[None,None],name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32,name='learning_rate')\n",
    "\n",
    "    # 定义target序列最大长度（之后target_sequence_length和source_sequence_length会作为feed_dict的参数）\n",
    "    target_sequence_length = tf.placeholder(tf.int32,(None,),name='target_sequence_length')\n",
    "    max_target_sequence_length = tf.reduce_max(target_sequence_length,name='max_target_len')\n",
    "    source_sequence_length = tf.placeholder(tf.int32,(None,),name='source_sequence_length')\n",
    "\n",
    "    return inputs,targets,learning_rate,target_sequence_length,max_target_sequence_length,source_sequence_length\n",
    "\n",
    "\n",
    "# Encoder\n",
    "\"\"\"\n",
    "在Encoder端，我们需要进行两步，第一步要对我们的输入进行Embedding，再把Embedding以后的向量传给RNN进行处理。\n",
    "\n",
    "在Embedding中，我们使用tf.contrib.layers.embed_sequence，它会对每个batch执行embedding操作。\n",
    "\"\"\"\n",
    "\n",
    "def get_encoder_layer(input_data,rnn_size,num_layers,source_sequence_length,source_vocab_size,encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    构造Encoder层\n",
    "\n",
    "    参数说明：\n",
    "    - input_data: 输入tensor\n",
    "    - rnn_size: rnn隐层结点数量\n",
    "    - num_layers: 堆叠的rnn cell数量\n",
    "    - source_sequence_length: 源数据的序列长度\n",
    "    - source_vocab_size: 源数据的词典大小\n",
    "    - encoding_embedding_size: embedding的大小\n",
    "    \"\"\"\n",
    "    # https://www.tensorflow.org/versions/r1.4/api_docs/python/tf/contrib/layers/embed_sequence\n",
    "    \"\"\"\n",
    "    embed_sequence(\n",
    "    ids,\n",
    "    vocab_size=None,\n",
    "    embed_dim=None,\n",
    "    unique=False,\n",
    "    initializer=None,\n",
    "    regularizer=None,\n",
    "    trainable=True,\n",
    "    scope=None,\n",
    "    reuse=None\n",
    "    )\n",
    "    ids: [batch_size, doc_length] Tensor of type int32 or int64 with symbol ids.\n",
    "    \n",
    "    return : Tensor of [batch_size, doc_length, embed_dim] with embedded sequences.\n",
    "    \"\"\"\n",
    "    encoder_embed_input = tf.contrib.layers.embed_sequence(input_data,source_vocab_size,encoding_embedding_size)\n",
    "\n",
    "    def get_lstm_cell(rnn_size):\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\n",
    "        return lstm_cell\n",
    "\n",
    "    cell =  tf.contrib.rnn.MultiRNNCell([get_lstm_cell(rnn_size) for _ in range(num_layers)])\n",
    "\n",
    "    encoder_output , encoder_state = tf.nn.dynamic_rnn(cell,encoder_embed_input,sequence_length=source_sequence_length,dtype=tf.float32)\n",
    "\n",
    "    return encoder_output,encoder_state\n",
    "\n",
    "\n",
    "\n",
    "def process_decoder_input(data,vocab_to_int,batch_size):\n",
    "\n",
    "    ending = tf.strided_slice(data,[0,0],[batch_size,-1],[1,1])\n",
    "    decoder_input = tf.concat([tf.fill([batch_size,1],vocab_to_int['<GO>']),ending],1)\n",
    "\n",
    "    return decoder_input\n",
    "\n",
    "\n",
    "def decoding_layer(target_letter_to_int,decoding_embedding_size,num_layers,rnn_size,\n",
    "                   target_sequence_length,max_target_sequence_length,encoder_state,decoder_input):\n",
    "    '''\n",
    "    构造Decoder层\n",
    "\n",
    "    参数：\n",
    "    - target_letter_to_int: target数据的映射表\n",
    "    - decoding_embedding_size: embed向量大小\n",
    "    - num_layers: 堆叠的RNN单元数量\n",
    "    - rnn_size: RNN单元的隐层结点数量\n",
    "    - target_sequence_length: target数据序列长度\n",
    "    - max_target_sequence_length: target数据序列最大长度\n",
    "    - encoder_state: encoder端编码的状态向量\n",
    "    - decoder_input: decoder端输入\n",
    "    '''\n",
    "\n",
    "    # 1. Embedding\n",
    "    target_vocab_size = len(target_letter_to_int)\n",
    "    decoder_embeddings = tf.Variable(tf.random_uniform([target_vocab_size,decoding_embedding_size]))\n",
    "    decoder_embed_input = tf.nn.embedding_lookup(decoder_embeddings,decoder_input)\n",
    "\n",
    "    # 构造Decoder中的RNN单元\n",
    "    def get_decoder_cell(rnn_size):\n",
    "        decoder_cell = tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\n",
    "        return decoder_cell\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([get_decoder_cell(rnn_size) for _ in range(num_layers)])\n",
    "\n",
    "    # Output全连接层\n",
    "    # target_vocab_size定义了输出层的大小\n",
    "    output_layer = Dense(target_vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.1,stddev=0.1))\n",
    "\n",
    "\n",
    "    # 4. Training decoder\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs = decoder_embed_input,\n",
    "                                                            sequence_length = target_sequence_length,\n",
    "                                                            time_major = False)\n",
    "\n",
    "\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(cell,training_helper,encoder_state,output_layer)\n",
    "        training_decoder_output,_,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,impute_finished=True,\n",
    "                                                                        maximum_iterations = max_target_sequence_length)\n",
    "\n",
    "\n",
    "    # 5. Predicting decoder\n",
    "    # 与training共享参数\n",
    "\n",
    "    with tf.variable_scope(\"decode\",reuse=True):\n",
    "        # 创建一个常量tensor并复制为batch_size的大小\n",
    "        start_tokens = tf.tile(tf.constant([target_letter_to_int['<GO>']],dtype=tf.int32),[batch_size],name='start_token')\n",
    "        predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings,start_tokens,target_letter_to_int['<EOS>'])\n",
    "\n",
    "        predicting_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n",
    "                                                             predicting_helper,\n",
    "                                                             encoder_state,\n",
    "                                                             output_layer)\n",
    "        predicting_decoder_output,_,_ = tf.contrib.seq2seq.dynamic_decode(predicting_decoder,impute_finished = True,\n",
    "                                                                          maximum_iterations = max_target_sequence_length)\n",
    "\n",
    "\n",
    "    return training_decoder_output,predicting_decoder_output\n",
    "\n",
    "# 上面已经构建完成Encoder和Decoder，下面将这两部分连接起来，构建seq2seq模型\n",
    "def seq2seq_model(input_data,targets,lr,target_sequence_length,max_target_sequence_length,\n",
    "                  source_sequence_length,source_vocab_size,target_vocab_size,encoder_embedding_size,\n",
    "                  decoder_embedding_size,rnn_size,num_layers):\n",
    "\n",
    "    _,encoder_state = get_encoder_layer(input_data,\n",
    "                                        rnn_size,\n",
    "                                        num_layers,\n",
    "                                        source_sequence_length,\n",
    "                                        source_vocab_size,\n",
    "                                        encoding_embedding_size)\n",
    "\n",
    "    decoder_input = process_decoder_input(targets,target_letter_to_int,batch_size)\n",
    "\n",
    "    training_decoder_output,predicting_decoder_output = decoding_layer(target_letter_to_int,\n",
    "                                                                       decoding_embedding_size,\n",
    "                                                                       num_layers,\n",
    "                                                                       rnn_size,\n",
    "                                                                       target_sequence_length,\n",
    "                                                                       max_target_sequence_length,\n",
    "                                                                       encoder_state,\n",
    "                                                                       decoder_input)\n",
    "\n",
    "    return training_decoder_output,predicting_decoder_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 构造graph\n",
    "train_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, lr, target_sequence_length, max_target_sequence_length, source_sequence_length = get_inputs()\n",
    "\n",
    "    training_decoder_output, predicting_decoder_output = seq2seq_model(input_data,\n",
    "                                                                       targets,\n",
    "                                                                       lr,\n",
    "                                                                       target_sequence_length,\n",
    "                                                                       max_target_sequence_length,\n",
    "                                                                       source_sequence_length,\n",
    "                                                                       len(source_letter_to_int),\n",
    "                                                                       len(target_letter_to_int),\n",
    "                                                                       encoding_embedding_size,\n",
    "                                                                       decoding_embedding_size,\n",
    "                                                                       rnn_size,\n",
    "                                                                       num_layers)\n",
    "\n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output,'logits')\n",
    "    predicting_logits = tf.identity(predicting_decoder_output.sample_id,name='predictions')\n",
    "\n",
    "    #mask是权重的意思\n",
    "    #tf.sequence_mask([1, 3, 2], 5)  # [[True, False, False, False, False],\n",
    "                                #  [True, True, True, False, False],\n",
    "                                #  [True, True, False, False, False]]\n",
    "    masks = tf.sequence_mask(target_sequence_length,max_target_sequence_length,dtype=tf.float32,name=\"masks\")\n",
    "\n",
    "    # logits: A Tensor of shape [batch_size, sequence_length, num_decoder_symbols] and dtype float.\n",
    "    # The logits correspond to the prediction across all classes at each timestep.\n",
    "    #targets: A Tensor of shape [batch_size, sequence_length] and dtype int.\n",
    "    # The target represents the true class at each timestep.\n",
    "    #weights: A Tensor of shape [batch_size, sequence_length] and dtype float.\n",
    "    # weights constitutes the weighting of each prediction in the sequence. When using weights as masking,\n",
    "    # set all valid timesteps to 1 and all padded timesteps to 0, e.g. a mask returned by tf.sequence_mask.\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks\n",
    "        )\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # minimize函数用于添加操作节点，用于最小化loss，并更新var_list.\n",
    "        # 该函数是简单的合并了compute_gradients()与apply_gradients()函数返回为一个优化更新后的var_list，\n",
    "        # 如果global_step非None，该操作还会为global_step做自增操作\n",
    "\n",
    "        #这里将minimize拆解为了以下两个部分：\n",
    "\n",
    "        # 对var_list中的变量计算loss的梯度 该函数为函数minimize()的第一部分，返回一个以元组(gradient, variable)组成的列表\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        # 将计算出的梯度应用到变量上，是函数minimize()的第二部分，返回一个应用指定的梯度的操作Operation，对global_step做自增操作\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "\n",
    "\n",
    "def pad_sentence_batch(sentence_batch,pad_int):\n",
    "    '''\n",
    "    对batch中的序列进行补全，保证batch中的每行都有相同的sequence_length\n",
    "\n",
    "    参数：\n",
    "    - sentence batch\n",
    "    - pad_int: <PAD>对应索引号\n",
    "    '''\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "\n",
    "def get_batches(targets,sources,batch_size,source_pad_int,target_pad_int):\n",
    "\n",
    "    for batch_i in range(0,len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        sources_batch = sources[start_i : start_i + batch_size]\n",
    "        targets_batch = targets[start_i : start_i + batch_size]\n",
    "\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch,source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch,target_pad_int))\n",
    "\n",
    "        targets_lengths = []\n",
    "        for target in targets_batch:\n",
    "            targets_lengths.append(len(target))\n",
    "\n",
    "        source_lengths = []\n",
    "        for source in sources_batch:\n",
    "            source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_targets_batch,pad_sources_batch,targets_lengths,source_lengths\n",
    "\n",
    "\n",
    "\n",
    "# Train\n",
    "train_source = source_int[batch_size:]\n",
    "train_target = target_int[batch_size:]\n",
    "\n",
    "# 留出一个batch进行验证\n",
    "valid_source = source_int[:batch_size]\n",
    "valid_target = target_int[:batch_size]\n",
    "\n",
    "(valid_targets_batch, valid_sources_batch, valid_targets_lengths, valid_sources_lengths) = next(get_batches(valid_target, valid_source, batch_size,\n",
    "                           source_letter_to_int['<PAD>'],\n",
    "                           target_letter_to_int['<PAD>']))\n",
    "\n",
    "display_step = 50\n",
    "\n",
    "checkpoint = \"data/trained_model.ckpt\"\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print()\n",
    "    for epoch_i in range(1,epochs+1):\n",
    "        for batch_i,(targets_batch, sources_batch, targets_lengths, sources_lengths) in enumerate(get_batches(\n",
    "            train_target,train_source,batch_size,source_letter_to_int['<PAD>'],\n",
    "                           target_letter_to_int['<PAD>']\n",
    "        )):\n",
    "            _,loss = sess.run([train_op,cost],feed_dict={\n",
    "                input_data:sources_batch,\n",
    "                targets:targets_batch,\n",
    "                lr:learning_rate,\n",
    "                target_sequence_length:targets_lengths,\n",
    "                source_sequence_length:sources_lengths\n",
    "            })\n",
    "\n",
    "            if batch_i % display_step == 0:\n",
    "                # 计算validation loss\n",
    "                validation_loss = sess.run(\n",
    "                    [cost],\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     targets: valid_targets_batch,\n",
    "                     lr: learning_rate,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     source_sequence_length: valid_sources_lengths})\n",
    "\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Training Loss: {:>6.3f}  - Validation loss: {:>6.3f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs,\n",
    "                              batch_i,\n",
    "                              len(train_source) // batch_size,\n",
    "                              loss,\n",
    "                              validation_loss[0]))\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, checkpoint)\n",
    "    print('Model Trained and Saved')\n",
    "\n",
    "\n",
    "# 预测\n",
    "def source_to_seq(text):\n",
    "    sequence_length = 7\n",
    "    return [source_letter_to_int.get(word,source_letter_to_int['<UNK>']) for word in text] + [source_letter_to_int['<PAD>']] * (sequence_length - len(text))\n",
    "\n",
    "\n",
    "input_word = '戌岁祝福万事顺'\n",
    "text = source_to_seq(input_word)\n",
    "\n",
    "checkpoint = \"data/trained_model.ckpt\"\n",
    "loaded_graph = tf.Graph()\n",
    "\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph(checkpoint+'.meta')\n",
    "    loader.restore(sess,checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "\n",
    "    answer_logits = sess.run(logits, {input_data: [text] * batch_size,\n",
    "                                      target_sequence_length: [len(input_word)] * batch_size,\n",
    "                                      source_sequence_length: [len(input_word)] * batch_size})[0]\n",
    "\n",
    "    pad = source_letter_to_int[\"<PAD>\"]\n",
    "\n",
    "    print('原始输入:', input_word)\n",
    "\n",
    "    print('\\nSource')\n",
    "    print('  Word 编号:    {}'.format([i for i in text]))\n",
    "    print('  Input Words: {}'.format(\" \".join([source_int_to_letter[i] for i in text])))\n",
    "\n",
    "    print('\\nTarget')\n",
    "    print('  Word 编号:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "    print('  Response Words: {}'.format(\" \".join([target_int_to_letter[i] for i in answer_logits if i != pad])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
