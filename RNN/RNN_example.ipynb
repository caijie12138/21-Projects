{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_steps = 10\n",
    "num_classes = 2\n",
    "state_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "` 参数意思分别 是从a 中以概率P，随机选择3个, p没有指定的时候相当于是一致的分布\n",
    "a1 = np.random.choice(a=5, size=3, replace=False, p=None)\n",
    "print(a1)\n",
    "非一致的分布，会以多少的概率提出来\n",
    "a2 = np.random.choice(a=5, size=3, replace=False, p=[0.2, 0.1, 0.3, 0.4, 0.0])\n",
    "print(a2)\n",
    "replacement 代表的意思是抽样之后还放不放回去，如果是False的话，那么出来的三个数都不一样，如果是\n",
    "True的话， 有可能会出现重复的，因为前面的抽的放回去了。 `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2,size=(size,)))\n",
    "    Y = []\n",
    "    #例子中给的Y是基于X生成的 就是在时间t生成0或者1的概率分别为50%，\n",
    "    #如果X[i-3]=1则产生1的概率增加50% 如果X[i-8]=1则产生1的概率增加25% 如果都满足 则产生1的概率增加75% \n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3]==1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8]==1: \n",
    "            threshold += 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X,np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(raw_data,batch_size,num_steps):\n",
    "    raw_x,raw_y = raw_data\n",
    "    data_x = raw_x.reshape((-1,batch_size,num_steps))\n",
    "    data_y = raw_y.reshape((-1,batch_size,num_steps))\n",
    "    for i in range(data_x.shape[0]):\n",
    "        yield (data_x[i],data_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_epochs(n):\n",
    "    for i in range(n):\n",
    "        yield (get_batch(gen_data(),batch_size,num_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(4), Dimension(10), Dimension(2)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.placeholder(tf.int32,[batch_size,num_steps],name = 'input_placeholder')\n",
    "y = tf.placeholder(tf.int32,[batch_size,num_steps],name = 'output_placeholder')\n",
    "init_state = tf.zeros([batch_size,state_size])\n",
    "x_one_hot = tf.one_hot(x,num_classes)\n",
    "x_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(4), Dimension(2)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_inputs = tf.unstack(x_one_hot,axis=1)\n",
    "rnn_inputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.unstack()　　\n",
    "将给定的R维张量拆分成R-1维张量\n",
    "将value根据axis分解成num个张量，返回的值是list类型，如果没有指定num则根据axis推断出！`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义变量空间\n",
    "with tf.variable_scope('rnn_variable') as scope:\n",
    "    W = tf.get_variable('W',[num_classes+state_size,state_size])\n",
    "    b = tf.get_variable('b',[state_size])\n",
    "\n",
    "def rnn_cell(rnn_input,state):\n",
    "    with tf.variable_scope('rnn_variable',reuse=True) as scope:\n",
    "        W = tf.get_variable('W',[num_classes+state_size,state_size])\n",
    "        b = tf.get_variable('b',[state_size])\n",
    "    return tf.nn.tanh(tf.matmul(tf.concat((rnn_input,state),1),W)+b)#tf.concat(((4，4),(4,2)),axis = 1)->(4,6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = init_state\n",
    "#state <tf.Tensor 'zeros_1:0' shape=(4, 4) dtype=float32> \n",
    "#rnn_inputs10个<tf.Tensor 'zeros_1:0' shape=(4, 2) dtype=float32>\n",
    "#tf.concat拼接的是除了axis之外都相同的两个张量 tf.stack是会增加一个维度\n",
    "rnn_outputs = []\n",
    "for rnn_input in rnn_inputs:#rnn_inputs 10个<tf.Tensor 'zeros_1:0' shape=(4, 2) dtype=float32>\n",
    "    state = rnn_cell(rnn_input,state)\n",
    "    rnn_outputs.append(state)\n",
    "    \n",
    "final_state = rnn_outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax',reuse=False):\n",
    "    W = tf.get_variable('W',[state_size,num_classes])\n",
    "    b = tf.get_variable('b',[num_classes],initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "logits = [tf.matmul(rnn_output,W)+b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "y_as_list = tf.unstack(y,num=num_steps,axis=1)#10个<tf.Tensor 'unstack_6:0' shape=(4,) dtype=int32>\n",
    "\n",
    "#loss = [tf.nn.softmax_cross_entropy_with_logits(labels = y,logits = logit) for (y,logit) in zip(y_as_list,predictions)]\n",
    "loss = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label,logits=logit)for (logit,label) in zip(predictions,y_as_list)]\n",
    "total_loss = tf.reduce_mean(loss)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs,num_steps,state_size=4,verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for idx,epoch in enumerate(get_epochs(num_epochs)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size,state_size))\n",
    "            if verbose:\n",
    "                print('\\n Epoch',idx)\n",
    "            for step,(X,Y) in enumerate(epoch):\n",
    "                #print(step)\n",
    "                training_loss_,training_state,_ = sess.run([total_loss,final_state,train_step],feed_dict={x:X,y:Y})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\",step,\"for last 100 steps:\",training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                training_loss = 0\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 0\n",
      "Average loss at step 100 for last 100 steps: 0.00472203314304\n",
      "Average loss at step 200 for last 100 steps: 0.00442314624786\n",
      "Average loss at step 300 for last 100 steps: 0.00441837131977\n",
      "Average loss at step 400 for last 100 steps: 0.00540114045143\n",
      "Average loss at step 500 for last 100 steps: 0.00463677495718\n",
      "Average loss at step 600 for last 100 steps: 0.00440136432648\n",
      "Average loss at step 700 for last 100 steps: 0.00463926643133\n",
      "Average loss at step 800 for last 100 steps: 0.00637379765511\n",
      "Average loss at step 900 for last 100 steps: 0.00562801659107\n",
      "Average loss at step 1000 for last 100 steps: 0.00439410835505\n",
      "Average loss at step 1100 for last 100 steps: 0.00463803529739\n",
      "Average loss at step 1200 for last 100 steps: 0.00439261615276\n",
      "Average loss at step 1300 for last 100 steps: 0.00488939940929\n",
      "Average loss at step 1400 for last 100 steps: 0.00563286483288\n",
      "Average loss at step 1500 for last 100 steps: 0.00488897413015\n",
      "Average loss at step 1600 for last 100 steps: 0.00439012438059\n",
      "Average loss at step 1700 for last 100 steps: 0.00488837152719\n",
      "Average loss at step 1800 for last 100 steps: 0.00463872045279\n",
      "Average loss at step 1900 for last 100 steps: 0.00488519996405\n",
      "Average loss at step 2000 for last 100 steps: 0.00513481318951\n",
      "Average loss at step 2100 for last 100 steps: 0.00513428032398\n",
      "Average loss at step 2200 for last 100 steps: 0.00538551807404\n",
      "Average loss at step 2300 for last 100 steps: 0.0041344076395\n",
      "Average loss at step 2400 for last 100 steps: 0.00638102889061\n",
      "Average loss at step 2500 for last 100 steps: 0.00513556957245\n",
      "Average loss at step 2600 for last 100 steps: 0.00613097548485\n",
      "Average loss at step 2700 for last 100 steps: 0.00488595545292\n",
      "Average loss at step 2800 for last 100 steps: 0.00438718616962\n",
      "Average loss at step 2900 for last 100 steps: 0.00588284790516\n",
      "Average loss at step 3000 for last 100 steps: 0.0043848991394\n",
      "Average loss at step 3100 for last 100 steps: 0.00513522028923\n",
      "Average loss at step 3200 for last 100 steps: 0.00538381695747\n",
      "Average loss at step 3300 for last 100 steps: 0.00438626199961\n",
      "Average loss at step 3400 for last 100 steps: 0.0058822542429\n",
      "Average loss at step 3500 for last 100 steps: 0.00563317477703\n",
      "Average loss at step 3600 for last 100 steps: 0.00438462823629\n",
      "Average loss at step 3700 for last 100 steps: 0.0041349363327\n",
      "Average loss at step 3800 for last 100 steps: 0.0051329267025\n",
      "Average loss at step 3900 for last 100 steps: 0.00513250052929\n",
      "Average loss at step 4000 for last 100 steps: 0.00388641893864\n",
      "Average loss at step 4100 for last 100 steps: 0.00513256669044\n",
      "Average loss at step 4200 for last 100 steps: 0.00538315415382\n",
      "Average loss at step 4300 for last 100 steps: 0.00513351857662\n",
      "Average loss at step 4400 for last 100 steps: 0.00388566404581\n",
      "Average loss at step 4500 for last 100 steps: 0.00613345980644\n",
      "Average loss at step 4600 for last 100 steps: 0.0056322145462\n",
      "Average loss at step 4700 for last 100 steps: 0.00438513815403\n",
      "Average loss at step 4800 for last 100 steps: 0.00438506692648\n",
      "Average loss at step 4900 for last 100 steps: 0.00388549983501\n",
      "Average loss at step 5000 for last 100 steps: 0.00513317644596\n",
      "Average loss at step 5100 for last 100 steps: 0.00463375657797\n",
      "Average loss at step 5200 for last 100 steps: 0.00588172793388\n",
      "Average loss at step 5300 for last 100 steps: 0.00463461101055\n",
      "Average loss at step 5400 for last 100 steps: 0.00588325381279\n",
      "Average loss at step 5500 for last 100 steps: 0.00513323068619\n",
      "Average loss at step 5600 for last 100 steps: 0.00513304531574\n",
      "Average loss at step 5700 for last 100 steps: 0.00413505256176\n",
      "Average loss at step 5800 for last 100 steps: 0.00413485705853\n",
      "Average loss at step 5900 for last 100 steps: 0.00438476473093\n",
      "Average loss at step 6000 for last 100 steps: 0.00463426977396\n",
      "Average loss at step 6100 for last 100 steps: 0.00588194012642\n",
      "Average loss at step 6200 for last 100 steps: 0.00463430702686\n",
      "Average loss at step 6300 for last 100 steps: 0.00638163566589\n",
      "Average loss at step 6400 for last 100 steps: 0.0046337223053\n",
      "Average loss at step 6500 for last 100 steps: 0.00613247990608\n",
      "Average loss at step 6600 for last 100 steps: 0.00438436359167\n",
      "Average loss at step 6700 for last 100 steps: 0.00463297367096\n",
      "Average loss at step 6800 for last 100 steps: 0.0053823441267\n",
      "Average loss at step 6900 for last 100 steps: 0.00563260674477\n",
      "Average loss at step 7000 for last 100 steps: 0.00513303458691\n",
      "Average loss at step 7100 for last 100 steps: 0.00413448810577\n",
      "Average loss at step 7200 for last 100 steps: 0.00538291871548\n",
      "Average loss at step 7300 for last 100 steps: 0.00463352024555\n",
      "Average loss at step 7400 for last 100 steps: 0.00588204681873\n",
      "Average loss at step 7500 for last 100 steps: 0.0048831063509\n",
      "Average loss at step 7600 for last 100 steps: 0.00513366580009\n",
      "Average loss at step 7700 for last 100 steps: 0.00463398069143\n",
      "Average loss at step 7800 for last 100 steps: 0.00613251149654\n",
      "Average loss at step 7900 for last 100 steps: 0.00388441205025\n",
      "Average loss at step 8000 for last 100 steps: 0.00513292610645\n",
      "Average loss at step 8100 for last 100 steps: 0.00638227462769\n",
      "Average loss at step 8200 for last 100 steps: 0.00463390260935\n",
      "Average loss at step 8300 for last 100 steps: 0.00438414156437\n",
      "Average loss at step 8400 for last 100 steps: 0.00513301193714\n",
      "Average loss at step 8500 for last 100 steps: 0.00538284480572\n",
      "Average loss at step 8600 for last 100 steps: 0.00463379532099\n",
      "Average loss at step 8700 for last 100 steps: 0.00488314568996\n",
      "Average loss at step 8800 for last 100 steps: 0.00463370501995\n",
      "Average loss at step 8900 for last 100 steps: 0.00438384354115\n",
      "Average loss at step 9000 for last 100 steps: 0.00538293004036\n",
      "Average loss at step 9100 for last 100 steps: 0.00438385784626\n",
      "Average loss at step 9200 for last 100 steps: 0.00513311266899\n",
      "Average loss at step 9300 for last 100 steps: 0.00513287782669\n",
      "Average loss at step 9400 for last 100 steps: 0.00388370752335\n",
      "Average loss at step 9500 for last 100 steps: 0.00613249480724\n",
      "Average loss at step 9600 for last 100 steps: 0.00438285499811\n",
      "Average loss at step 9700 for last 100 steps: 0.00538332819939\n",
      "Average loss at step 9800 for last 100 steps: 0.0056326174736\n",
      "Average loss at step 9900 for last 100 steps: 0.0068822067976\n",
      "Average loss at step 10000 for last 100 steps: 0.00438370943069\n",
      "Average loss at step 10100 for last 100 steps: 0.00413338810205\n",
      "Average loss at step 10200 for last 100 steps: 0.00488309055567\n",
      "Average loss at step 10300 for last 100 steps: 0.00588219940662\n",
      "Average loss at step 10400 for last 100 steps: 0.0053831923008\n",
      "Average loss at step 10500 for last 100 steps: 0.00463361918926\n",
      "Average loss at step 10600 for last 100 steps: 0.0046332731843\n",
      "Average loss at step 10700 for last 100 steps: 0.00488304048777\n",
      "Average loss at step 10800 for last 100 steps: 0.00513249635696\n",
      "Average loss at step 10900 for last 100 steps: 0.00388391882181\n",
      "Average loss at step 11000 for last 100 steps: 0.00588261067867\n",
      "Average loss at step 11100 for last 100 steps: 0.00463310003281\n",
      "Average loss at step 11200 for last 100 steps: 0.00438364416361\n",
      "Average loss at step 11300 for last 100 steps: 0.0038839507103\n",
      "Average loss at step 11400 for last 100 steps: 0.00563306868076\n",
      "Average loss at step 11500 for last 100 steps: 0.00538286805153\n",
      "Average loss at step 11600 for last 100 steps: 0.00463354974985\n",
      "Average loss at step 11700 for last 100 steps: 0.00613286197186\n",
      "Average loss at step 11800 for last 100 steps: 0.00638161480427\n",
      "Average loss at step 11900 for last 100 steps: 0.00513285517693\n",
      "Average loss at step 12000 for last 100 steps: 0.00513299822807\n",
      "Average loss at step 12100 for last 100 steps: 0.00438320219517\n",
      "Average loss at step 12200 for last 100 steps: 0.00563204526901\n",
      "Average loss at step 12300 for last 100 steps: 0.00588269770145\n",
      "Average loss at step 12400 for last 100 steps: 0.00563265025616\n",
      "Average loss at step 12500 for last 100 steps: 0.00563279509544\n",
      "Average loss at step 12600 for last 100 steps: 0.00513329207897\n",
      "Average loss at step 12700 for last 100 steps: 0.00588294386864\n",
      "Average loss at step 12800 for last 100 steps: 0.00513325154781\n",
      "Average loss at step 12900 for last 100 steps: 0.00513324260712\n",
      "Average loss at step 13000 for last 100 steps: 0.00538313984871\n",
      "Average loss at step 13100 for last 100 steps: 0.00463279783726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 13200 for last 100 steps: 0.00463312149048\n",
      "Average loss at step 13300 for last 100 steps: 0.00463312149048\n",
      "Average loss at step 13400 for last 100 steps: 0.00563273966312\n",
      "Average loss at step 13500 for last 100 steps: 0.00463266670704\n",
      "Average loss at step 13600 for last 100 steps: 0.00638212561607\n",
      "Average loss at step 13700 for last 100 steps: 0.00463312625885\n",
      "Average loss at step 13800 for last 100 steps: 0.00563267409801\n",
      "Average loss at step 13900 for last 100 steps: 0.00463308155537\n",
      "Average loss at step 14000 for last 100 steps: 0.00488291084766\n",
      "Average loss at step 14100 for last 100 steps: 0.00488330304623\n",
      "Average loss at step 14200 for last 100 steps: 0.0056329792738\n",
      "Average loss at step 14300 for last 100 steps: 0.00488329827785\n",
      "Average loss at step 14400 for last 100 steps: 0.00538287639618\n",
      "Average loss at step 14500 for last 100 steps: 0.00463334560394\n",
      "Average loss at step 14600 for last 100 steps: 0.00488266289234\n",
      "Average loss at step 14700 for last 100 steps: 0.00388360440731\n",
      "Average loss at step 14800 for last 100 steps: 0.00488321125507\n",
      "Average loss at step 14900 for last 100 steps: 0.00588255524635\n",
      "Average loss at step 15000 for last 100 steps: 0.00513294041157\n",
      "Average loss at step 15100 for last 100 steps: 0.00538249492645\n",
      "Average loss at step 15200 for last 100 steps: 0.00463333755732\n",
      "Average loss at step 15300 for last 100 steps: 0.00463302046061\n",
      "Average loss at step 15400 for last 100 steps: 0.00588259875774\n",
      "Average loss at step 15500 for last 100 steps: 0.00463295608759\n",
      "Average loss at step 15600 for last 100 steps: 0.00513309895992\n",
      "Average loss at step 15700 for last 100 steps: 0.00463306248188\n",
      "Average loss at step 15800 for last 100 steps: 0.00463331401348\n",
      "Average loss at step 15900 for last 100 steps: 0.00538279950619\n",
      "Average loss at step 16000 for last 100 steps: 0.00538239836693\n",
      "Average loss at step 16100 for last 100 steps: 0.00513283133507\n",
      "Average loss at step 16200 for last 100 steps: 0.00513310432434\n",
      "Average loss at step 16300 for last 100 steps: 0.00538243114948\n",
      "Average loss at step 16400 for last 100 steps: 0.0041333770752\n",
      "Average loss at step 16500 for last 100 steps: 0.00488288879395\n",
      "Average loss at step 16600 for last 100 steps: 0.00463324964046\n",
      "Average loss at step 16700 for last 100 steps: 0.00513290882111\n",
      "Average loss at step 16800 for last 100 steps: 0.00588285446167\n",
      "Average loss at step 16900 for last 100 steps: 0.00638268113136\n",
      "Average loss at step 17000 for last 100 steps: 0.0056327009201\n",
      "Average loss at step 17100 for last 100 steps: 0.00488289773464\n",
      "Average loss at step 17200 for last 100 steps: 0.0051330769062\n",
      "Average loss at step 17300 for last 100 steps: 0.00538255572319\n",
      "Average loss at step 17400 for last 100 steps: 0.00438328355551\n",
      "Average loss at step 17500 for last 100 steps: 0.005382822752\n",
      "Average loss at step 17600 for last 100 steps: 0.00563295245171\n",
      "Average loss at step 17700 for last 100 steps: 0.00388324379921\n",
      "Average loss at step 17800 for last 100 steps: 0.00538277029991\n",
      "Average loss at step 17900 for last 100 steps: 0.00488311290741\n",
      "Average loss at step 18000 for last 100 steps: 0.00488309860229\n",
      "Average loss at step 18100 for last 100 steps: 0.00438309043646\n",
      "Average loss at step 18200 for last 100 steps: 0.00388338983059\n",
      "Average loss at step 18300 for last 100 steps: 0.00463317960501\n",
      "Average loss at step 18400 for last 100 steps: 0.00513281464577\n",
      "Average loss at step 18500 for last 100 steps: 0.00513267219067\n",
      "Average loss at step 18600 for last 100 steps: 0.00538298010826\n",
      "Average loss at step 18700 for last 100 steps: 0.00488310903311\n",
      "Average loss at step 18800 for last 100 steps: 0.00538278758526\n",
      "Average loss at step 18900 for last 100 steps: 0.00588268458843\n",
      "Average loss at step 19000 for last 100 steps: 0.00413329422474\n",
      "Average loss at step 19100 for last 100 steps: 0.00463289409876\n",
      "Average loss at step 19200 for last 100 steps: 0.0056327265501\n",
      "Average loss at step 19300 for last 100 steps: 0.00488311111927\n",
      "Average loss at step 19400 for last 100 steps: 0.00513282299042\n",
      "Average loss at step 19500 for last 100 steps: 0.00538270115852\n",
      "Average loss at step 19600 for last 100 steps: 0.0043831834197\n",
      "Average loss at step 19700 for last 100 steps: 0.00438318729401\n",
      "Average loss at step 19800 for last 100 steps: 0.00538248896599\n",
      "Average loss at step 19900 for last 100 steps: 0.00513238728046\n",
      "Average loss at step 20000 for last 100 steps: 0.00488307237625\n",
      "Average loss at step 20100 for last 100 steps: 0.00513281702995\n",
      "Average loss at step 20200 for last 100 steps: 0.00488287746906\n",
      "Average loss at step 20300 for last 100 steps: 0.00463289797306\n",
      "Average loss at step 20400 for last 100 steps: 0.00438316494226\n",
      "Average loss at step 20500 for last 100 steps: 0.00513302505016\n",
      "Average loss at step 20600 for last 100 steps: 0.00588283061981\n",
      "Average loss at step 20700 for last 100 steps: 0.00588246941566\n",
      "Average loss at step 20800 for last 100 steps: 0.00588240742683\n",
      "Average loss at step 20900 for last 100 steps: 0.00438314974308\n",
      "Average loss at step 21000 for last 100 steps: 0.00563269853592\n",
      "Average loss at step 21100 for last 100 steps: 0.00563224971294\n",
      "Average loss at step 21200 for last 100 steps: 0.00563286960125\n",
      "Average loss at step 21300 for last 100 steps: 0.00538294374943\n",
      "Average loss at step 21400 for last 100 steps: 0.00513298153877\n",
      "Average loss at step 21500 for last 100 steps: 0.00488303810358\n",
      "Average loss at step 21600 for last 100 steps: 0.00513276934624\n",
      "Average loss at step 21700 for last 100 steps: 0.00488301903009\n",
      "Average loss at step 21800 for last 100 steps: 0.00563263297081\n",
      "Average loss at step 21900 for last 100 steps: 0.00488285779953\n",
      "Average loss at step 22000 for last 100 steps: 0.00638238787651\n",
      "Average loss at step 22100 for last 100 steps: 0.00438313782215\n",
      "Average loss at step 22200 for last 100 steps: 0.00488286882639\n",
      "Average loss at step 22300 for last 100 steps: 0.00538291573524\n",
      "Average loss at step 22400 for last 100 steps: 0.00413318574429\n",
      "Average loss at step 22500 for last 100 steps: 0.0048830229044\n",
      "Average loss at step 22600 for last 100 steps: 0.00563268661499\n",
      "Average loss at step 22700 for last 100 steps: 0.00438292503357\n",
      "Average loss at step 22800 for last 100 steps: 0.00563246428967\n",
      "Average loss at step 22900 for last 100 steps: 0.00413316905499\n",
      "Average loss at step 23000 for last 100 steps: 0.00538233399391\n",
      "Average loss at step 23100 for last 100 steps: 0.00538288772106\n",
      "Average loss at step 23200 for last 100 steps: 0.00463302671909\n",
      "Average loss at step 23300 for last 100 steps: 0.003883215487\n",
      "Average loss at step 23400 for last 100 steps: 0.00463290780783\n",
      "Average loss at step 23500 for last 100 steps: 0.00663245201111\n",
      "Average loss at step 23600 for last 100 steps: 0.00413319200277\n",
      "Average loss at step 23700 for last 100 steps: 0.00438310444355\n",
      "Average loss at step 23800 for last 100 steps: 0.00588279604912\n",
      "Average loss at step 23900 for last 100 steps: 0.0043829536438\n",
      "Average loss at step 24000 for last 100 steps: 0.00388319551945\n",
      "Average loss at step 24100 for last 100 steps: 0.00488299369812\n",
      "Average loss at step 24200 for last 100 steps: 0.00488299667835\n",
      "Average loss at step 24300 for last 100 steps: 0.00563271641731\n",
      "Average loss at step 24400 for last 100 steps: 0.00613251507282\n",
      "Average loss at step 24500 for last 100 steps: 0.00513271927834\n",
      "Average loss at step 24600 for last 100 steps: 0.00438291460276\n",
      "Average loss at step 24700 for last 100 steps: 0.00613273262978\n",
      "Average loss at step 24800 for last 100 steps: 0.00538260161877\n",
      "Average loss at step 24900 for last 100 steps: 0.0051329189539\n",
      "\n",
      " Epoch 1\n",
      "Average loss at step 100 for last 100 steps: 0.00363321840763\n",
      "Average loss at step 200 for last 100 steps: 0.00638270676136\n",
      "Average loss at step 300 for last 100 steps: 0.00538274586201\n",
      "Average loss at step 400 for last 100 steps: 0.00538231790066\n",
      "Average loss at step 500 for last 100 steps: 0.00438286960125\n",
      "Average loss at step 600 for last 100 steps: 0.00563245415688\n",
      "Average loss at step 700 for last 100 steps: 0.00538241028786\n",
      "Average loss at step 800 for last 100 steps: 0.0053828895092\n",
      "Average loss at step 900 for last 100 steps: 0.00488296598196\n",
      "Average loss at step 1000 for last 100 steps: 0.00388300031424\n",
      "Average loss at step 1100 for last 100 steps: 0.00413283914328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 1200 for last 100 steps: 0.00488276869059\n",
      "Average loss at step 1300 for last 100 steps: 0.00613273441792\n",
      "Average loss at step 1400 for last 100 steps: 0.00513291418552\n",
      "Average loss at step 1500 for last 100 steps: 0.00588264763355\n",
      "Average loss at step 1600 for last 100 steps: 0.00663265347481\n",
      "Average loss at step 1700 for last 100 steps: 0.00513271212578\n",
      "Average loss at step 1800 for last 100 steps: 0.00563265681267\n",
      "Average loss at step 1900 for last 100 steps: 0.00563247084618\n",
      "Average loss at step 2000 for last 100 steps: 0.00563281655312\n",
      "Average loss at step 2100 for last 100 steps: 0.00438289701939\n",
      "Average loss at step 2200 for last 100 steps: 0.0056326675415\n",
      "Average loss at step 2300 for last 100 steps: 0.0043830537796\n",
      "Average loss at step 2400 for last 100 steps: 0.00488296121359\n",
      "Average loss at step 2500 for last 100 steps: 0.00388309389353\n",
      "Average loss at step 2600 for last 100 steps: 0.00538269221783\n",
      "Average loss at step 2700 for last 100 steps: 0.00513253808022\n",
      "Average loss at step 2800 for last 100 steps: 0.0046328791976\n",
      "Average loss at step 2900 for last 100 steps: 0.00463287442923\n",
      "Average loss at step 3000 for last 100 steps: 0.00538241028786\n",
      "Average loss at step 3100 for last 100 steps: 0.00463299810886\n",
      "Average loss at step 3200 for last 100 steps: 0.00563264489174\n",
      "Average loss at step 3300 for last 100 steps: 0.00513264834881\n",
      "Average loss at step 3400 for last 100 steps: 0.00438305199146\n",
      "Average loss at step 3500 for last 100 steps: 0.00513289451599\n",
      "Average loss at step 3600 for last 100 steps: 0.00488293230534\n",
      "Average loss at step 3700 for last 100 steps: 0.0043829870224\n",
      "Average loss at step 3800 for last 100 steps: 0.00638242542744\n",
      "Average loss at step 3900 for last 100 steps: 0.00438302606344\n",
      "Average loss at step 4000 for last 100 steps: 0.00463279902935\n",
      "Average loss at step 4100 for last 100 steps: 0.00463279664516\n",
      "Average loss at step 4200 for last 100 steps: 0.00588277697563\n",
      "Average loss at step 4300 for last 100 steps: 0.00438299328089\n",
      "Average loss at step 4400 for last 100 steps: 0.00438272058964\n",
      "Average loss at step 4500 for last 100 steps: 0.00538272559643\n",
      "Average loss at step 4600 for last 100 steps: 0.00488292038441\n",
      "Average loss at step 4700 for last 100 steps: 0.00438300311565\n",
      "Average loss at step 4800 for last 100 steps: 0.0051327008009\n",
      "Average loss at step 4900 for last 100 steps: 0.00463283210993\n",
      "Average loss at step 5000 for last 100 steps: 0.00613260388374\n",
      "Average loss at step 5100 for last 100 steps: 0.00488280296326\n",
      "Average loss at step 5200 for last 100 steps: 0.00563279747963\n",
      "Average loss at step 5300 for last 100 steps: 0.00438300937414\n",
      "Average loss at step 5400 for last 100 steps: 0.00563264489174\n",
      "Average loss at step 5500 for last 100 steps: 0.00563261926174\n",
      "Average loss at step 5600 for last 100 steps: 0.00388308346272\n",
      "Average loss at step 5700 for last 100 steps: 0.00563264369965\n",
      "Average loss at step 5800 for last 100 steps: 0.00538280367851\n",
      "Average loss at step 5900 for last 100 steps: 0.00463296115398\n",
      "Average loss at step 6000 for last 100 steps: 0.00488289922476\n",
      "Average loss at step 6100 for last 100 steps: 0.00538282990456\n",
      "Average loss at step 6200 for last 100 steps: 0.00488281190395\n",
      "Average loss at step 6300 for last 100 steps: 0.00463280051947\n",
      "Average loss at step 6400 for last 100 steps: 0.00513286232948\n",
      "Average loss at step 6500 for last 100 steps: 0.00538281321526\n",
      "Average loss at step 6600 for last 100 steps: 0.00488291680813\n",
      "Average loss at step 6700 for last 100 steps: 0.00438287079334\n",
      "Average loss at step 6800 for last 100 steps: 0.00538281083107\n",
      "Average loss at step 6900 for last 100 steps: 0.0051327085495\n",
      "Average loss at step 7000 for last 100 steps: 0.00538269221783\n",
      "Average loss at step 7100 for last 100 steps: 0.00588263869286\n",
      "Average loss at step 7200 for last 100 steps: 0.00463290780783\n",
      "Average loss at step 7300 for last 100 steps: 0.00638265967369\n",
      "Average loss at step 7400 for last 100 steps: 0.00488262176514\n",
      "Average loss at step 7500 for last 100 steps: 0.00588274657726\n",
      "Average loss at step 7600 for last 100 steps: 0.00638240277767\n",
      "Average loss at step 7700 for last 100 steps: 0.00488287866116\n",
      "Average loss at step 7800 for last 100 steps: 0.00513270378113\n",
      "Average loss at step 7900 for last 100 steps: 0.00463290512562\n",
      "Average loss at step 8000 for last 100 steps: 0.00538252711296\n",
      "Average loss at step 8100 for last 100 steps: 0.0043829497695\n",
      "Average loss at step 8200 for last 100 steps: 0.00538280367851\n",
      "Average loss at step 8300 for last 100 steps: 0.00488286107779\n",
      "Average loss at step 8400 for last 100 steps: 0.00538268744946\n",
      "Average loss at step 8500 for last 100 steps: 0.00438281148672\n",
      "Average loss at step 8600 for last 100 steps: 0.00513271212578\n",
      "Average loss at step 8700 for last 100 steps: 0.0046328958869\n",
      "Average loss at step 8800 for last 100 steps: 0.00638241112232\n",
      "Average loss at step 8900 for last 100 steps: 0.00588274121284\n",
      "Average loss at step 9000 for last 100 steps: 0.00438296496868\n",
      "Average loss at step 9100 for last 100 steps: 0.00438281834126\n",
      "Average loss at step 9200 for last 100 steps: 0.00488285541534\n",
      "Average loss at step 9300 for last 100 steps: 0.00488289058208\n",
      "Average loss at step 9400 for last 100 steps: 0.00413299411535\n",
      "Average loss at step 9500 for last 100 steps: 0.00463281065226\n",
      "Average loss at step 9600 for last 100 steps: 0.00413296878338\n",
      "Average loss at step 9700 for last 100 steps: 0.0053826969862\n",
      "Average loss at step 9800 for last 100 steps: 0.0056327599287\n",
      "Average loss at step 9900 for last 100 steps: 0.00513285517693\n",
      "Average loss at step 10000 for last 100 steps: 0.00513255834579\n",
      "Average loss at step 10100 for last 100 steps: 0.00488252311945\n",
      "Average loss at step 10200 for last 100 steps: 0.0048828792572\n",
      "Average loss at step 10300 for last 100 steps: 0.004632820189\n",
      "Average loss at step 10400 for last 100 steps: 0.00513284862041\n",
      "Average loss at step 10500 for last 100 steps: 0.00438295930624\n",
      "Average loss at step 10600 for last 100 steps: 0.00488276153803\n",
      "Average loss at step 10700 for last 100 steps: 0.00463276386261\n",
      "Average loss at step 10800 for last 100 steps: 0.00488286882639\n",
      "Average loss at step 10900 for last 100 steps: 0.00663237094879\n",
      "Average loss at step 11000 for last 100 steps: 0.00538267493248\n",
      "Average loss at step 11100 for last 100 steps: 0.0048828753829\n",
      "Average loss at step 11200 for last 100 steps: 0.00388299643993\n",
      "Average loss at step 11300 for last 100 steps: 0.00388300925493\n",
      "Average loss at step 11400 for last 100 steps: 0.00538282036781\n",
      "Average loss at step 11500 for last 100 steps: 0.00488287448883\n",
      "Average loss at step 11600 for last 100 steps: 0.0051327085495\n",
      "Average loss at step 11700 for last 100 steps: 0.00663227319717\n",
      "Average loss at step 11800 for last 100 steps: 0.0051327341795\n",
      "Average loss at step 11900 for last 100 steps: 0.00438291251659\n",
      "Average loss at step 12000 for last 100 steps: 0.00638267636299\n",
      "Average loss at step 12100 for last 100 steps: 0.00488275438547\n",
      "Average loss at step 12200 for last 100 steps: 0.00538278400898\n",
      "Average loss at step 12300 for last 100 steps: 0.00388297438622\n",
      "Average loss at step 12400 for last 100 steps: 0.00538267016411\n",
      "Average loss at step 12500 for last 100 steps: 0.00513281524181\n",
      "Average loss at step 12600 for last 100 steps: 0.00588249385357\n",
      "Average loss at step 12700 for last 100 steps: 0.00413293838501\n",
      "Average loss at step 12800 for last 100 steps: 0.00588275015354\n",
      "Average loss at step 12900 for last 100 steps: 0.00488264322281\n",
      "Average loss at step 13000 for last 100 steps: 0.0046328958869\n",
      "Average loss at step 13100 for last 100 steps: 0.0056326675415\n",
      "Average loss at step 13200 for last 100 steps: 0.00463289439678\n",
      "Average loss at step 13300 for last 100 steps: 0.00413295507431\n",
      "Average loss at step 13400 for last 100 steps: 0.00438292503357\n",
      "Average loss at step 13500 for last 100 steps: 0.00413294315338\n",
      "Average loss at step 13600 for last 100 steps: 0.00563274741173\n",
      "Average loss at step 13700 for last 100 steps: 0.00563267767429\n",
      "Average loss at step 13800 for last 100 steps: 0.00513280808926\n",
      "Average loss at step 13900 for last 100 steps: 0.00463279098272\n",
      "Average loss at step 14000 for last 100 steps: 0.00563253998756\n",
      "Average loss at step 14100 for last 100 steps: 0.00488274753094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 14200 for last 100 steps: 0.00688233017921\n",
      "Average loss at step 14300 for last 100 steps: 0.00513281702995\n",
      "Average loss at step 14400 for last 100 steps: 0.00513272583485\n",
      "Average loss at step 14500 for last 100 steps: 0.00488285601139\n",
      "Average loss at step 14600 for last 100 steps: 0.00538264930248\n",
      "Average loss at step 14700 for last 100 steps: 0.00463287264109\n",
      "Average loss at step 14800 for last 100 steps: 0.00438278764486\n",
      "Average loss at step 14900 for last 100 steps: 0.00588259875774\n",
      "Average loss at step 15000 for last 100 steps: 0.00613249182701\n",
      "Average loss at step 15100 for last 100 steps: 0.00463286966085\n",
      "Average loss at step 15200 for last 100 steps: 0.00438290983438\n",
      "Average loss at step 15300 for last 100 steps: 0.00588272333145\n",
      "Average loss at step 15400 for last 100 steps: 0.00488275527954\n",
      "Average loss at step 15500 for last 100 steps: 0.00463286787271\n",
      "Average loss at step 15600 for last 100 steps: 0.00513261437416\n",
      "Average loss at step 15700 for last 100 steps: 0.00688248038292\n",
      "Average loss at step 15800 for last 100 steps: 0.00413292586803\n",
      "Average loss at step 15900 for last 100 steps: 0.00538265585899\n",
      "Average loss at step 16000 for last 100 steps: 0.00488261640072\n",
      "Average loss at step 16100 for last 100 steps: 0.00438289254904\n",
      "Average loss at step 16200 for last 100 steps: 0.00488284289837\n",
      "Average loss at step 16300 for last 100 steps: 0.00488274276257\n",
      "Average loss at step 16400 for last 100 steps: 0.00463287115097\n",
      "Average loss at step 16500 for last 100 steps: 0.00488285154104\n",
      "Average loss at step 16600 for last 100 steps: 0.00513279438019\n",
      "Average loss at step 16700 for last 100 steps: 0.0056327432394\n",
      "Average loss at step 16800 for last 100 steps: 0.00388294756413\n",
      "Average loss at step 16900 for last 100 steps: 0.00488282114267\n",
      "Average loss at step 17000 for last 100 steps: 0.00538260817528\n",
      "Average loss at step 17100 for last 100 steps: 0.00463286697865\n",
      "Average loss at step 17200 for last 100 steps: 0.00513272047043\n",
      "Average loss at step 17300 for last 100 steps: 0.00438287645578\n",
      "Average loss at step 17400 for last 100 steps: 0.00488274991512\n",
      "Average loss at step 17500 for last 100 steps: 0.00388294905424\n",
      "Average loss at step 17600 for last 100 steps: 0.00438278853893\n",
      "Average loss at step 17700 for last 100 steps: 0.00413290411234\n",
      "Average loss at step 17800 for last 100 steps: 0.00513277888298\n",
      "Average loss at step 17900 for last 100 steps: 0.00463285446167\n",
      "Average loss at step 18000 for last 100 steps: 0.00563254952431\n",
      "Average loss at step 18100 for last 100 steps: 0.00413282006979\n",
      "Average loss at step 18200 for last 100 steps: 0.0056327432394\n",
      "Average loss at step 18300 for last 100 steps: 0.00463284194469\n",
      "Average loss at step 18400 for last 100 steps: 0.00488272845745\n",
      "Average loss at step 18500 for last 100 steps: 0.00538256406784\n",
      "Average loss at step 18600 for last 100 steps: 0.00388285577297\n",
      "Average loss at step 18700 for last 100 steps: 0.00538255393505\n",
      "Average loss at step 18800 for last 100 steps: 0.00488259702921\n",
      "Average loss at step 18900 for last 100 steps: 0.0058825224638\n",
      "Average loss at step 19000 for last 100 steps: 0.00538268208504\n",
      "Average loss at step 19100 for last 100 steps: 0.00388293892145\n",
      "Average loss at step 19200 for last 100 steps: 0.00463275492191\n",
      "Average loss at step 19300 for last 100 steps: 0.00438267797232\n",
      "Average loss at step 19400 for last 100 steps: 0.00388284981251\n",
      "Average loss at step 19500 for last 100 steps: 0.00513280212879\n",
      "Average loss at step 19600 for last 100 steps: 0.00563273668289\n",
      "Average loss at step 19700 for last 100 steps: 0.00463284492493\n",
      "Average loss at step 19800 for last 100 steps: 0.0043826597929\n",
      "Average loss at step 19900 for last 100 steps: 0.0053826713562\n",
      "Average loss at step 20000 for last 100 steps: 0.00563264191151\n",
      "Average loss at step 20100 for last 100 steps: 0.00588249206543\n",
      "Average loss at step 20200 for last 100 steps: 0.00713240027428\n",
      "Average loss at step 20300 for last 100 steps: 0.00563265144825\n",
      "Average loss at step 20400 for last 100 steps: 0.00513268887997\n",
      "Average loss at step 20500 for last 100 steps: 0.00413289248943\n",
      "Average loss at step 20600 for last 100 steps: 0.00563255906105\n",
      "Average loss at step 20700 for last 100 steps: 0.00463283240795\n",
      "Average loss at step 20800 for last 100 steps: 0.00413289070129\n",
      "Average loss at step 20900 for last 100 steps: 0.00663262844086\n",
      "Average loss at step 21000 for last 100 steps: 0.00538267493248\n",
      "Average loss at step 21100 for last 100 steps: 0.00513270974159\n",
      "Average loss at step 21200 for last 100 steps: 0.00513277828693\n",
      "Average loss at step 21300 for last 100 steps: 0.00388280928135\n",
      "Average loss at step 21400 for last 100 steps: 0.00488271474838\n",
      "Average loss at step 21500 for last 100 steps: 0.00363293081522\n",
      "Average loss at step 21600 for last 100 steps: 0.00488263040781\n",
      "Average loss at step 21700 for last 100 steps: 0.00563263654709\n",
      "Average loss at step 21800 for last 100 steps: 0.00488272041082\n",
      "Average loss at step 21900 for last 100 steps: 0.0048827034235\n",
      "Average loss at step 22000 for last 100 steps: 0.00513270974159\n",
      "Average loss at step 22100 for last 100 steps: 0.00488270461559\n",
      "Average loss at step 22200 for last 100 steps: 0.00513260483742\n",
      "Average loss at step 22300 for last 100 steps: 0.00563271939754\n",
      "Average loss at step 22400 for last 100 steps: 0.00488271147013\n",
      "Average loss at step 22500 for last 100 steps: 0.00563265144825\n",
      "Average loss at step 22600 for last 100 steps: 0.00488263040781\n",
      "Average loss at step 22700 for last 100 steps: 0.00563264846802\n",
      "Average loss at step 22800 for last 100 steps: 0.00438286393881\n",
      "Average loss at step 22900 for last 100 steps: 0.00538267433643\n",
      "Average loss at step 23000 for last 100 steps: 0.00588271260262\n",
      "Average loss at step 23100 for last 100 steps: 0.00463283777237\n",
      "Average loss at step 23200 for last 100 steps: 0.00488270193338\n",
      "Average loss at step 23300 for last 100 steps: 0.00563272178173\n",
      "Average loss at step 23400 for last 100 steps: 0.00563264846802\n",
      "Average loss at step 23500 for last 100 steps: 0.00513277053833\n",
      "Average loss at step 23600 for last 100 steps: 0.00513270497322\n",
      "Average loss at step 23700 for last 100 steps: 0.00438266575336\n",
      "Average loss at step 23800 for last 100 steps: 0.00613250672817\n",
      "Average loss at step 23900 for last 100 steps: 0.00488280296326\n",
      "Average loss at step 24000 for last 100 steps: 0.00488262176514\n",
      "Average loss at step 24100 for last 100 steps: 0.00588255465031\n",
      "Average loss at step 24200 for last 100 steps: 0.00588270366192\n",
      "Average loss at step 24300 for last 100 steps: 0.0036328125\n",
      "Average loss at step 24400 for last 100 steps: 0.00513269424438\n",
      "Average loss at step 24500 for last 100 steps: 0.00538275003433\n",
      "Average loss at step 24600 for last 100 steps: 0.00563272595406\n",
      "Average loss at step 24700 for last 100 steps: 0.00513268351555\n",
      "Average loss at step 24800 for last 100 steps: 0.0048827290535\n",
      "Average loss at step 24900 for last 100 steps: 0.00613248944283\n",
      "\n",
      " Epoch 2\n",
      "Average loss at step 100 for last 100 steps: 0.00413286983967\n",
      "Average loss at step 200 for last 100 steps: 0.00488269865513\n",
      "Average loss at step 300 for last 100 steps: 0.00488269746304\n",
      "Average loss at step 400 for last 100 steps: 0.00438275396824\n",
      "Average loss at step 500 for last 100 steps: 0.00463282495737\n",
      "Average loss at step 600 for last 100 steps: 0.00488271892071\n",
      "Average loss at step 700 for last 100 steps: 0.00488271802664\n",
      "Average loss at step 800 for last 100 steps: 0.00563271403313\n",
      "Average loss at step 900 for last 100 steps: 0.00613266170025\n",
      "Average loss at step 1000 for last 100 steps: 0.00438283979893\n",
      "Average loss at step 1100 for last 100 steps: 0.00538267731667\n",
      "Average loss at step 1200 for last 100 steps: 0.0043827599287\n",
      "Average loss at step 1300 for last 100 steps: 0.00488262742758\n",
      "Average loss at step 1400 for last 100 steps: 0.00438283830881\n",
      "Average loss at step 1500 for last 100 steps: 0.00563264727592\n",
      "Average loss at step 1600 for last 100 steps: 0.00613258719444\n",
      "Average loss at step 1700 for last 100 steps: 0.00438283056021\n",
      "Average loss at step 1800 for last 100 steps: 0.00538266479969\n",
      "Average loss at step 1900 for last 100 steps: 0.00538259029388\n",
      "Average loss at step 2000 for last 100 steps: 0.00563257396221\n",
      "Average loss at step 2100 for last 100 steps: 0.00463280379772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 2200 for last 100 steps: 0.004882555902\n",
      "Average loss at step 2300 for last 100 steps: 0.00613250017166\n",
      "Average loss at step 2400 for last 100 steps: 0.00538260340691\n",
      "Average loss at step 2500 for last 100 steps: 0.00463280290365\n",
      "Average loss at step 2600 for last 100 steps: 0.0048827791214\n",
      "Average loss at step 2700 for last 100 steps: 0.00513276576996\n",
      "Average loss at step 2800 for last 100 steps: 0.00563256263733\n",
      "Average loss at step 2900 for last 100 steps: 0.00563271939754\n",
      "Average loss at step 3000 for last 100 steps: 0.00463281065226\n",
      "Average loss at step 3100 for last 100 steps: 0.00388287603855\n",
      "Average loss at step 3200 for last 100 steps: 0.00488277733326\n",
      "Average loss at step 3300 for last 100 steps: 0.00463265895844\n",
      "Average loss at step 3400 for last 100 steps: 0.00513275980949\n",
      "Average loss at step 3500 for last 100 steps: 0.00488270133734\n",
      "Average loss at step 3600 for last 100 steps: 0.00588262379169\n",
      "Average loss at step 3700 for last 100 steps: 0.00513268828392\n",
      "Average loss at step 3800 for last 100 steps: 0.00438282877207\n",
      "Average loss at step 3900 for last 100 steps: 0.00563265502453\n",
      "Average loss at step 4000 for last 100 steps: 0.00563258171082\n",
      "Average loss at step 4100 for last 100 steps: 0.00513253688812\n",
      "Average loss at step 4200 for last 100 steps: 0.00438275635242\n",
      "Average loss at step 4300 for last 100 steps: 0.00413285642862\n",
      "Average loss at step 4400 for last 100 steps: 0.00438273817301\n",
      "Average loss at step 4500 for last 100 steps: 0.00463274002075\n",
      "Average loss at step 4600 for last 100 steps: 0.00638251841068\n",
      "Average loss at step 4700 for last 100 steps: 0.00463280856609\n",
      "Average loss at step 4800 for last 100 steps: 0.0043827277422\n",
      "Average loss at step 4900 for last 100 steps: 0.00688253045082\n",
      "Average loss at step 5000 for last 100 steps: 0.00388277977705\n",
      "Average loss at step 5100 for last 100 steps: 0.00488269805908\n",
      "Average loss at step 5200 for last 100 steps: 0.00413285076618\n",
      "Average loss at step 5300 for last 100 steps: 0.00488263607025\n",
      "Average loss at step 5400 for last 100 steps: 0.00463272422552\n",
      "Average loss at step 5500 for last 100 steps: 0.00438275337219\n",
      "Average loss at step 5600 for last 100 steps: 0.00513268768787\n",
      "Average loss at step 5700 for last 100 steps: 0.00363288462162\n",
      "Average loss at step 5800 for last 100 steps: 0.00513261437416\n",
      "Average loss at step 5900 for last 100 steps: 0.00513270199299\n",
      "Average loss at step 6000 for last 100 steps: 0.00513267695904\n",
      "Average loss at step 6100 for last 100 steps: 0.00463279336691\n",
      "Average loss at step 6200 for last 100 steps: 0.00488269627094\n",
      "Average loss at step 6300 for last 100 steps: 0.00488269984722\n",
      "Average loss at step 6400 for last 100 steps: 0.00463279724121\n",
      "Average loss at step 6500 for last 100 steps: 0.00413283824921\n",
      "Average loss at step 6600 for last 100 steps: 0.00563269734383\n",
      "Average loss at step 6700 for last 100 steps: 0.00613251686096\n",
      "Average loss at step 6800 for last 100 steps: 0.00538266003132\n",
      "Average loss at step 6900 for last 100 steps: 0.00413276672363\n",
      "Average loss at step 7000 for last 100 steps: 0.00388284474611\n",
      "Average loss at step 7100 for last 100 steps: 0.00438275754452\n",
      "Average loss at step 7200 for last 100 steps: 0.0051326996088\n",
      "Average loss at step 7300 for last 100 steps: 0.00463278859854\n",
      "Average loss at step 7400 for last 100 steps: 0.00513275325298\n",
      "Average loss at step 7500 for last 100 steps: 0.00563264548779\n",
      "Average loss at step 7600 for last 100 steps: 0.00488276481628\n",
      "Average loss at step 7700 for last 100 steps: 0.00438281357288\n",
      "Average loss at step 7800 for last 100 steps: 0.00588268816471\n",
      "Average loss at step 7900 for last 100 steps: 0.00438280582428\n",
      "Average loss at step 8000 for last 100 steps: 0.00513269305229\n",
      "Average loss at step 8100 for last 100 steps: 0.00463279098272\n",
      "Average loss at step 8200 for last 100 steps: 0.00588255107403\n",
      "Average loss at step 8300 for last 100 steps: 0.00488267958164\n",
      "Average loss at step 8400 for last 100 steps: 0.005882563591\n",
      "Average loss at step 8500 for last 100 steps: 0.00538263916969\n",
      "Average loss at step 8600 for last 100 steps: 0.0048827701807\n",
      "Average loss at step 8700 for last 100 steps: 0.00413277238607\n",
      "Average loss at step 8800 for last 100 steps: 0.00438274472952\n",
      "Average loss at step 8900 for last 100 steps: 0.0058825969696\n",
      "Average loss at step 9000 for last 100 steps: 0.00463262945414\n",
      "Average loss at step 9100 for last 100 steps: 0.00513274669647\n",
      "Average loss at step 9200 for last 100 steps: 0.00563261628151\n",
      "Average loss at step 9300 for last 100 steps: 0.00438280969858\n",
      "Average loss at step 9400 for last 100 steps: 0.00463278055191\n",
      "Average loss at step 9500 for last 100 steps: 0.00463278144598\n",
      "Average loss at step 9600 for last 100 steps: 0.0051327419281\n",
      "Average loss at step 9700 for last 100 steps: 0.00538266956806\n",
      "Average loss at step 9800 for last 100 steps: 0.00488263309002\n",
      "Average loss at step 9900 for last 100 steps: 0.00438266843557\n",
      "Average loss at step 10000 for last 100 steps: 0.00388277858496\n",
      "Average loss at step 10100 for last 100 steps: 0.00563271164894\n",
      "Average loss at step 10200 for last 100 steps: 0.00613266646862\n",
      "Average loss at step 10300 for last 100 steps: 0.00638264000416\n",
      "Average loss at step 10400 for last 100 steps: 0.00488261044025\n",
      "Average loss at step 10500 for last 100 steps: 0.00563264846802\n",
      "Average loss at step 10600 for last 100 steps: 0.00438280969858\n",
      "Average loss at step 10700 for last 100 steps: 0.00488276392221\n",
      "Average loss at step 10800 for last 100 steps: 0.00413276672363\n",
      "Average loss at step 10900 for last 100 steps: 0.00613245785236\n",
      "Average loss at step 11000 for last 100 steps: 0.0061325442791\n",
      "Average loss at step 11100 for last 100 steps: 0.0056326431036\n",
      "Average loss at step 11200 for last 100 steps: 0.00538271844387\n",
      "Average loss at step 11300 for last 100 steps: 0.00513259708881\n",
      "Average loss at step 11400 for last 100 steps: 0.00538257241249\n",
      "Average loss at step 11500 for last 100 steps: 0.00488269031048\n",
      "Average loss at step 11600 for last 100 steps: 0.00488268941641\n",
      "Average loss at step 11700 for last 100 steps: 0.00438272953033\n",
      "Average loss at step 11800 for last 100 steps: 0.00563255131245\n",
      "Average loss at step 11900 for last 100 steps: 0.00488256633282\n",
      "Average loss at step 12000 for last 100 steps: 0.00588255763054\n",
      "Average loss at step 12100 for last 100 steps: 0.00513269305229\n",
      "Average loss at step 12200 for last 100 steps: 0.00413281917572\n",
      "Average loss at step 12300 for last 100 steps: 0.00363286405802\n",
      "Average loss at step 12400 for last 100 steps: 0.00563261687756\n",
      "Average loss at step 12500 for last 100 steps: 0.00488275945187\n",
      "Average loss at step 12600 for last 100 steps: 0.00488276094198\n",
      "Average loss at step 12700 for last 100 steps: 0.00413274675608\n",
      "Average loss at step 12800 for last 100 steps: 0.00538271903992\n",
      "Average loss at step 12900 for last 100 steps: 0.00538266181946\n",
      "Average loss at step 13000 for last 100 steps: 0.00513266801834\n",
      "Average loss at step 13100 for last 100 steps: 0.00438279688358\n",
      "Average loss at step 13200 for last 100 steps: 0.00513274550438\n",
      "Average loss at step 13300 for last 100 steps: 0.00488267511129\n",
      "Average loss at step 13400 for last 100 steps: 0.00463270008564\n",
      "Average loss at step 13500 for last 100 steps: 0.00488274484873\n",
      "Average loss at step 13600 for last 100 steps: 0.00488267511129\n",
      "Average loss at step 13700 for last 100 steps: 0.00513274431229\n",
      "Average loss at step 13800 for last 100 steps: 0.00638249754906\n",
      "Average loss at step 13900 for last 100 steps: 0.00488276392221\n",
      "Average loss at step 14000 for last 100 steps: 0.00413273990154\n",
      "Average loss at step 14100 for last 100 steps: 0.00563264071941\n",
      "Average loss at step 14200 for last 100 steps: 0.00613255023956\n",
      "Average loss at step 14300 for last 100 steps: 0.0051326841116\n",
      "Average loss at step 14400 for last 100 steps: 0.00463278591633\n",
      "Average loss at step 14500 for last 100 steps: 0.00563250243664\n",
      "Average loss at step 14600 for last 100 steps: 0.00413276493549\n",
      "Average loss at step 14700 for last 100 steps: 0.00488276779652\n",
      "Average loss at step 14800 for last 100 steps: 0.00413281619549\n",
      "Average loss at step 14900 for last 100 steps: 0.00563264131546\n",
      "Average loss at step 15000 for last 100 steps: 0.00413280963898\n",
      "Average loss at step 15100 for last 100 steps: 0.00588252902031\n",
      "Average loss at step 15200 for last 100 steps: 0.00438279807568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 15300 for last 100 steps: 0.00663255214691\n",
      "Average loss at step 15400 for last 100 steps: 0.0048827457428\n",
      "Average loss at step 15500 for last 100 steps: 0.00413280904293\n",
      "Average loss at step 15600 for last 100 steps: 0.00588261663914\n",
      "Average loss at step 15700 for last 100 steps: 0.00488275915384\n",
      "Average loss at step 15800 for last 100 steps: 0.00513265967369\n",
      "Average loss at step 15900 for last 100 steps: 0.00513260364532\n",
      "Average loss at step 16000 for last 100 steps: 0.00563264667988\n",
      "Average loss at step 16100 for last 100 steps: 0.00438272416592\n",
      "Average loss at step 16200 for last 100 steps: 0.00463271915913\n",
      "Average loss at step 16300 for last 100 steps: 0.00438273668289\n",
      "Average loss at step 16400 for last 100 steps: 0.00563264548779\n",
      "Average loss at step 16500 for last 100 steps: 0.00538258433342\n",
      "Average loss at step 16600 for last 100 steps: 0.00463276475668\n",
      "Average loss at step 16700 for last 100 steps: 0.00513265907764\n",
      "Average loss at step 16800 for last 100 steps: 0.00438279300928\n",
      "Average loss at step 16900 for last 100 steps: 0.00588257193565\n",
      "Average loss at step 17000 for last 100 steps: 0.00438266277313\n",
      "Average loss at step 17100 for last 100 steps: 0.00513273119926\n",
      "Average loss at step 17200 for last 100 steps: 0.00438271343708\n",
      "Average loss at step 17300 for last 100 steps: 0.00388283342123\n",
      "Average loss at step 17400 for last 100 steps: 0.00488273799419\n",
      "Average loss at step 17500 for last 100 steps: 0.00513267576694\n",
      "Average loss at step 17600 for last 100 steps: 0.00513267457485\n",
      "Average loss at step 17700 for last 100 steps: 0.0053826636076\n",
      "Average loss at step 17800 for last 100 steps: 0.0048826828599\n",
      "Average loss at step 17900 for last 100 steps: 0.00563264608383\n",
      "Average loss at step 18000 for last 100 steps: 0.0051326239109\n",
      "Average loss at step 18100 for last 100 steps: 0.00463264465332\n",
      "Average loss at step 18200 for last 100 steps: 0.00463275998831\n",
      "Average loss at step 18300 for last 100 steps: 0.00438279390335\n",
      "Average loss at step 18400 for last 100 steps: 0.00513274073601\n",
      "Average loss at step 18500 for last 100 steps: 0.00463276535273\n",
      "Average loss at step 18600 for last 100 steps: 0.00388281822205\n",
      "Average loss at step 18700 for last 100 steps: 0.00438278198242\n",
      "Average loss at step 18800 for last 100 steps: 0.00413280397654\n",
      "Average loss at step 18900 for last 100 steps: 0.0048827418685\n",
      "Average loss at step 19000 for last 100 steps: 0.00488275140524\n",
      "Average loss at step 19100 for last 100 steps: 0.00438271045685\n",
      "Average loss at step 19200 for last 100 steps: 0.00463277429342\n",
      "Average loss at step 19300 for last 100 steps: 0.00388268768787\n",
      "Average loss at step 19400 for last 100 steps: 0.0048826956749\n",
      "Average loss at step 19500 for last 100 steps: 0.00488269090652\n",
      "Average loss at step 19600 for last 100 steps: 0.0056326264143\n",
      "Average loss at step 19700 for last 100 steps: 0.00488270103931\n",
      "Average loss at step 19800 for last 100 steps: 0.00488268136978\n",
      "Average loss at step 19900 for last 100 steps: 0.00388274371624\n",
      "Average loss at step 20000 for last 100 steps: 0.00638264000416\n",
      "Average loss at step 20100 for last 100 steps: 0.00463276684284\n",
      "Average loss at step 20200 for last 100 steps: 0.00463270276785\n",
      "Average loss at step 20300 for last 100 steps: 0.00513265252113\n",
      "Average loss at step 20400 for last 100 steps: 0.00438277721405\n",
      "Average loss at step 20500 for last 100 steps: 0.00538264155388\n",
      "Average loss at step 20600 for last 100 steps: 0.00638246178627\n",
      "Average loss at step 20700 for last 100 steps: 0.00513271570206\n",
      "Average loss at step 20800 for last 100 steps: 0.00438266187906\n",
      "Average loss at step 20900 for last 100 steps: 0.00488262653351\n",
      "Average loss at step 21000 for last 100 steps: 0.00463271141052\n",
      "Average loss at step 21100 for last 100 steps: 0.00563263058662\n",
      "Average loss at step 21200 for last 100 steps: 0.00488267421722\n",
      "Average loss at step 21300 for last 100 steps: 0.00438270658255\n",
      "Average loss at step 21400 for last 100 steps: 0.00538260638714\n",
      "Average loss at step 21500 for last 100 steps: 0.00438277959824\n",
      "Average loss at step 21600 for last 100 steps: 0.00563262999058\n",
      "Average loss at step 21700 for last 100 steps: 0.00463275283575\n",
      "Average loss at step 21800 for last 100 steps: 0.00563264250755\n",
      "Average loss at step 21900 for last 100 steps: 0.00463262945414\n",
      "Average loss at step 22000 for last 100 steps: 0.00538264453411\n",
      "Average loss at step 22100 for last 100 steps: 0.00588262200356\n",
      "Average loss at step 22200 for last 100 steps: 0.0038828125596\n",
      "Average loss at step 22300 for last 100 steps: 0.0053825968504\n",
      "Average loss at step 22400 for last 100 steps: 0.00513272404671\n",
      "Average loss at step 22500 for last 100 steps: 0.00588261902332\n",
      "Average loss at step 22600 for last 100 steps: 0.0046327495575\n",
      "Average loss at step 22700 for last 100 steps: 0.00463276088238\n",
      "Average loss at step 22800 for last 100 steps: 0.00413274198771\n",
      "Average loss at step 22900 for last 100 steps: 0.00438277333975\n",
      "Average loss at step 23000 for last 100 steps: 0.00488273948431\n",
      "Average loss at step 23100 for last 100 steps: 0.00488273918629\n",
      "Average loss at step 23200 for last 100 steps: 0.00438276201487\n",
      "Average loss at step 23300 for last 100 steps: 0.00363282471895\n",
      "Average loss at step 23400 for last 100 steps: 0.00513272702694\n",
      "Average loss at step 23500 for last 100 steps: 0.00463275998831\n",
      "Average loss at step 23600 for last 100 steps: 0.00588262677193\n",
      "Average loss at step 23700 for last 100 steps: 0.00513266205788\n",
      "Average loss at step 23800 for last 100 steps: 0.0048827418685\n",
      "Average loss at step 23900 for last 100 steps: 0.0053827059269\n",
      "Average loss at step 24000 for last 100 steps: 0.00563268780708\n",
      "Average loss at step 24100 for last 100 steps: 0.00438276469707\n",
      "Average loss at step 24200 for last 100 steps: 0.00463274300098\n",
      "Average loss at step 24300 for last 100 steps: 0.00588266789913\n",
      "Average loss at step 24400 for last 100 steps: 0.00488274663687\n",
      "Average loss at step 24500 for last 100 steps: 0.00463270187378\n",
      "Average loss at step 24600 for last 100 steps: 0.00413273096085\n",
      "Average loss at step 24700 for last 100 steps: 0.00463268756866\n",
      "Average loss at step 24800 for last 100 steps: 0.00563268721104\n",
      "Average loss at step 24900 for last 100 steps: 0.00538270354271\n",
      "\n",
      " Epoch 3\n",
      "Average loss at step 100 for last 100 steps: 0.00538271069527\n",
      "Average loss at step 200 for last 100 steps: 0.00588261544704\n",
      "Average loss at step 300 for last 100 steps: 0.00488273531199\n",
      "Average loss at step 400 for last 100 steps: 0.00563268184662\n",
      "Average loss at step 500 for last 100 steps: 0.00588256239891\n",
      "Average loss at step 600 for last 100 steps: 0.00563264250755\n",
      "Average loss at step 700 for last 100 steps: 0.00438276678324\n",
      "Average loss at step 800 for last 100 steps: 0.00413273721933\n",
      "Average loss at step 900 for last 100 steps: 0.00388275772333\n",
      "Average loss at step 1000 for last 100 steps: 0.00438275963068\n",
      "Average loss at step 1100 for last 100 steps: 0.0041327804327\n",
      "Average loss at step 1200 for last 100 steps: 0.00488267660141\n",
      "Average loss at step 1300 for last 100 steps: 0.00463275432587\n",
      "Average loss at step 1400 for last 100 steps: 0.00388280153275\n",
      "Average loss at step 1500 for last 100 steps: 0.00413278400898\n",
      "Average loss at step 1600 for last 100 steps: 0.00463268578053\n",
      "Average loss at step 1700 for last 100 steps: 0.00438275903463\n",
      "Average loss at step 1800 for last 100 steps: 0.00513265967369\n",
      "Average loss at step 1900 for last 100 steps: 0.00413278400898\n",
      "Average loss at step 2000 for last 100 steps: 0.00513265490532\n",
      "Average loss at step 2100 for last 100 steps: 0.00563268184662\n",
      "Average loss at step 2200 for last 100 steps: 0.00338283538818\n",
      "Average loss at step 2300 for last 100 steps: 0.00538264393806\n",
      "Average loss at step 2400 for last 100 steps: 0.00488273233175\n",
      "Average loss at step 2500 for last 100 steps: 0.00538265585899\n",
      "Average loss at step 2600 for last 100 steps: 0.0051327162981\n",
      "Average loss at step 2700 for last 100 steps: 0.00488266855478\n",
      "Average loss at step 2800 for last 100 steps: 0.00563268065453\n",
      "Average loss at step 2900 for last 100 steps: 0.00438275665045\n",
      "Average loss at step 3000 for last 100 steps: 0.0051326584816\n",
      "Average loss at step 3100 for last 100 steps: 0.00513265788555\n",
      "Average loss at step 3200 for last 100 steps: 0.00488255560398\n",
      "Average loss at step 3300 for last 100 steps: 0.00438270479441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 3400 for last 100 steps: 0.00538258731365\n",
      "Average loss at step 3500 for last 100 steps: 0.00638248980045\n",
      "Average loss at step 3600 for last 100 steps: 0.00513261675835\n",
      "Average loss at step 3700 for last 100 steps: 0.00438276678324\n",
      "Average loss at step 3800 for last 100 steps: 0.00638259530067\n",
      "Average loss at step 3900 for last 100 steps: 0.00513270914555\n",
      "Average loss at step 4000 for last 100 steps: 0.00588262557983\n",
      "Average loss at step 4100 for last 100 steps: 0.00513266682625\n",
      "Average loss at step 4200 for last 100 steps: 0.00438270390034\n",
      "Average loss at step 4300 for last 100 steps: 0.00438265144825\n",
      "Average loss at step 4400 for last 100 steps: 0.00513266563416\n",
      "Average loss at step 4500 for last 100 steps: 0.00538265705109\n",
      "Average loss at step 4600 for last 100 steps: 0.00438274919987\n",
      "Average loss at step 4700 for last 100 steps: 0.00463269323111\n",
      "Average loss at step 4800 for last 100 steps: 0.00438276916742\n",
      "Average loss at step 4900 for last 100 steps: 0.00488273799419\n",
      "Average loss at step 5000 for last 100 steps: 0.00513272166252\n",
      "Average loss at step 5100 for last 100 steps: 0.00388279736042\n",
      "Average loss at step 5200 for last 100 steps: 0.00613264918327\n",
      "Average loss at step 5300 for last 100 steps: 0.00413272619247\n",
      "Average loss at step 5400 for last 100 steps: 0.00463274776936\n",
      "Average loss at step 5500 for last 100 steps: 0.00388278305531\n",
      "Average loss at step 5600 for last 100 steps: 0.00588256001472\n",
      "Average loss at step 5700 for last 100 steps: 0.00538264632225\n",
      "Average loss at step 5800 for last 100 steps: 0.00463270008564\n",
      "Average loss at step 5900 for last 100 steps: 0.00563258171082\n",
      "Average loss at step 6000 for last 100 steps: 0.0051327008009\n",
      "Average loss at step 6100 for last 100 steps: 0.0051326161623\n",
      "Average loss at step 6200 for last 100 steps: 0.00638258695602\n",
      "Average loss at step 6300 for last 100 steps: 0.0046326905489\n",
      "Average loss at step 6400 for last 100 steps: 0.00413271605968\n",
      "Average loss at step 6500 for last 100 steps: 0.0053826469183\n",
      "Average loss at step 6600 for last 100 steps: 0.00538254141808\n",
      "Average loss at step 6700 for last 100 steps: 0.00488272130489\n",
      "Average loss at step 6800 for last 100 steps: 0.00588266730309\n",
      "Average loss at step 6900 for last 100 steps: 0.00588256299496\n",
      "Average loss at step 7000 for last 100 steps: 0.00538264989853\n",
      "Average loss at step 7100 for last 100 steps: 0.00713249504566\n",
      "Average loss at step 7200 for last 100 steps: 0.00613249897957\n",
      "Average loss at step 7300 for last 100 steps: 0.00538265109062\n",
      "Average loss at step 7400 for last 100 steps: 0.0048826828599\n",
      "Average loss at step 7500 for last 100 steps: 0.00513271152973\n",
      "Average loss at step 7600 for last 100 steps: 0.00413271963596\n",
      "Average loss at step 7700 for last 100 steps: 0.00538265347481\n",
      "Average loss at step 7800 for last 100 steps: 0.00463273465633\n",
      "Average loss at step 7900 for last 100 steps: 0.00513271033764\n",
      "Average loss at step 8000 for last 100 steps: 0.00513264775276\n",
      "Average loss at step 8100 for last 100 steps: 0.00513265609741\n",
      "Average loss at step 8200 for last 100 steps: 0.00513270974159\n",
      "Average loss at step 8300 for last 100 steps: 0.00463269710541\n",
      "Average loss at step 8400 for last 100 steps: 0.0068825596571\n",
      "Average loss at step 8500 for last 100 steps: 0.00438275337219\n",
      "Average loss at step 8600 for last 100 steps: 0.00488271892071\n",
      "Average loss at step 8700 for last 100 steps: 0.00588262438774\n",
      "Average loss at step 8800 for last 100 steps: 0.00663253724575\n",
      "Average loss at step 8900 for last 100 steps: 0.00413276523352\n",
      "Average loss at step 9000 for last 100 steps: 0.00463273346424\n",
      "Average loss at step 9100 for last 100 steps: 0.00463269323111\n",
      "Average loss at step 9200 for last 100 steps: 0.00463268369436\n",
      "Average loss at step 9300 for last 100 steps: 0.00563267529011\n",
      "Average loss at step 9400 for last 100 steps: 0.00463273614645\n",
      "Average loss at step 9500 for last 100 steps: 0.00538263678551\n",
      "Average loss at step 9600 for last 100 steps: 0.00588262617588\n",
      "Average loss at step 9700 for last 100 steps: 0.00438275039196\n",
      "Average loss at step 9800 for last 100 steps: 0.00463268846273\n",
      "Average loss at step 9900 for last 100 steps: 0.00463274091482\n",
      "Average loss at step 10000 for last 100 steps: 0.0056326264143\n",
      "Average loss at step 10100 for last 100 steps: 0.00438275396824\n",
      "Average loss at step 10200 for last 100 steps: 0.00563267409801\n",
      "Average loss at step 10300 for last 100 steps: 0.00488261699677\n",
      "Average loss at step 10400 for last 100 steps: 0.00488263696432\n",
      "Average loss at step 10500 for last 100 steps: 0.00438274919987\n",
      "Average loss at step 10600 for last 100 steps: 0.00488271534443\n",
      "Average loss at step 10700 for last 100 steps: 0.00413276284933\n",
      "Average loss at step 10800 for last 100 steps: 0.00563258767128\n",
      "Average loss at step 10900 for last 100 steps: 0.00488272279501\n",
      "Average loss at step 11000 for last 100 steps: 0.00663257479668\n",
      "Average loss at step 11100 for last 100 steps: 0.00488272368908\n",
      "Average loss at step 11200 for last 100 steps: 0.00538264513016\n",
      "Average loss at step 11300 for last 100 steps: 0.00538260877132\n",
      "Average loss at step 11400 for last 100 steps: 0.00538264274597\n",
      "Average loss at step 11500 for last 100 steps: 0.00513269722462\n",
      "Average loss at step 11600 for last 100 steps: 0.00463273346424\n",
      "Average loss at step 11700 for last 100 steps: 0.00638263225555\n",
      "Average loss at step 11800 for last 100 steps: 0.00538263678551\n",
      "Average loss at step 11900 for last 100 steps: 0.00488272368908\n",
      "Average loss at step 12000 for last 100 steps: 0.00463273346424\n",
      "Average loss at step 12100 for last 100 steps: 0.00538269639015\n",
      "Average loss at step 12200 for last 100 steps: 0.00538269519806\n",
      "Average loss at step 12300 for last 100 steps: 0.00413271605968\n",
      "Average loss at step 12400 for last 100 steps: 0.00388277292252\n",
      "Average loss at step 12500 for last 100 steps: 0.00413275897503\n",
      "Average loss at step 12600 for last 100 steps: 0.00463272750378\n",
      "Average loss at step 12700 for last 100 steps: 0.00513270497322\n",
      "Average loss at step 12800 for last 100 steps: 0.00463272243738\n",
      "Average loss at step 12900 for last 100 steps: 0.00438269913197\n",
      "Average loss at step 13000 for last 100 steps: 0.00513270020485\n",
      "Average loss at step 13100 for last 100 steps: 0.00613254547119\n",
      "Average loss at step 13200 for last 100 steps: 0.00563257932663\n",
      "Average loss at step 13300 for last 100 steps: 0.00463273465633\n",
      "Average loss at step 13400 for last 100 steps: 0.00463268190622\n",
      "Average loss at step 13500 for last 100 steps: 0.00463267624378\n",
      "Average loss at step 13600 for last 100 steps: 0.00613260567188\n",
      "Average loss at step 13700 for last 100 steps: 0.00488262087107\n",
      "Average loss at step 13800 for last 100 steps: 0.0056326764822\n",
      "Average loss at step 13900 for last 100 steps: 0.00463272958994\n",
      "Average loss at step 14000 for last 100 steps: 0.00488266706467\n",
      "Average loss at step 14100 for last 100 steps: 0.00463272958994\n",
      "Average loss at step 14200 for last 100 steps: 0.00363278359175\n",
      "Average loss at step 14300 for last 100 steps: 0.00538265526295\n",
      "Average loss at step 14400 for last 100 steps: 0.00463273048401\n",
      "Average loss at step 14500 for last 100 steps: 0.00538268446922\n",
      "Average loss at step 14600 for last 100 steps: 0.00613252043724\n",
      "Average loss at step 14700 for last 100 steps: 0.00313281476498\n",
      "Average loss at step 14800 for last 100 steps: 0.00538268864155\n",
      "Average loss at step 14900 for last 100 steps: 0.00488267034292\n",
      "Average loss at step 15000 for last 100 steps: 0.00463272482157\n",
      "Average loss at step 15100 for last 100 steps: 0.00513270378113\n",
      "Average loss at step 15200 for last 100 steps: 0.00413271039724\n",
      "Average loss at step 15300 for last 100 steps: 0.00488271057606\n",
      "Average loss at step 15400 for last 100 steps: 0.00488268196583\n",
      "Average loss at step 15500 for last 100 steps: 0.00463267624378\n",
      "Average loss at step 15600 for last 100 steps: 0.00438274145126\n",
      "Average loss at step 15700 for last 100 steps: 0.00638259410858\n",
      "Average loss at step 15800 for last 100 steps: 0.00488267421722\n",
      "Average loss at step 15900 for last 100 steps: 0.00538264632225\n",
      "Average loss at step 16000 for last 100 steps: 0.00513265967369\n",
      "Average loss at step 16100 for last 100 steps: 0.00513265728951\n",
      "Average loss at step 16200 for last 100 steps: 0.00538267970085\n",
      "Average loss at step 16300 for last 100 steps: 0.00388276189566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 16400 for last 100 steps: 0.00438269197941\n",
      "Average loss at step 16500 for last 100 steps: 0.00563267886639\n",
      "Average loss at step 16600 for last 100 steps: 0.00463272750378\n",
      "Average loss at step 16700 for last 100 steps: 0.00463272631168\n",
      "Average loss at step 16800 for last 100 steps: 0.00538264632225\n",
      "Average loss at step 16900 for last 100 steps: 0.00538263380527\n",
      "Average loss at step 17000 for last 100 steps: 0.00463272571564\n",
      "Average loss at step 17100 for last 100 steps: 0.00563263773918\n",
      "Average loss at step 17200 for last 100 steps: 0.00488271802664\n",
      "Average loss at step 17300 for last 100 steps: 0.00438273847103\n",
      "Average loss at step 17400 for last 100 steps: 0.0036327368021\n",
      "Average loss at step 17500 for last 100 steps: 0.00438274383545\n",
      "Average loss at step 17600 for last 100 steps: 0.00463272571564\n",
      "Average loss at step 17700 for last 100 steps: 0.00563262283802\n",
      "Average loss at step 17800 for last 100 steps: 0.0048827123642\n",
      "Average loss at step 17900 for last 100 steps: 0.00388276815414\n",
      "Average loss at step 18000 for last 100 steps: 0.00413275420666\n",
      "Average loss at step 18100 for last 100 steps: 0.00588260948658\n",
      "Average loss at step 18200 for last 100 steps: 0.0038827636838\n",
      "Average loss at step 18300 for last 100 steps: 0.00438273370266\n",
      "Average loss at step 18400 for last 100 steps: 0.00638262808323\n",
      "Average loss at step 18500 for last 100 steps: 0.00538260161877\n",
      "Average loss at step 18600 for last 100 steps: 0.00488270759583\n",
      "Average loss at step 18700 for last 100 steps: 0.00488267570734\n",
      "Average loss at step 18800 for last 100 steps: 0.00388277053833\n",
      "Average loss at step 18900 for last 100 steps: 0.0046326893568\n",
      "Average loss at step 19000 for last 100 steps: 0.00538263440132\n",
      "Average loss at step 19100 for last 100 steps: 0.00388272583485\n",
      "Average loss at step 19200 for last 100 steps: 0.003882766366\n",
      "Average loss at step 19300 for last 100 steps: 0.00488270670176\n",
      "Average loss at step 19400 for last 100 steps: 0.00513269722462\n",
      "Average loss at step 19500 for last 100 steps: 0.00463267654181\n",
      "Average loss at step 19600 for last 100 steps: 0.00463272750378\n",
      "Average loss at step 19700 for last 100 steps: 0.00488267421722\n",
      "Average loss at step 19800 for last 100 steps: 0.00638259589672\n",
      "Average loss at step 19900 for last 100 steps: 0.0056326687336\n",
      "Average loss at step 20000 for last 100 steps: 0.00563263654709\n",
      "Average loss at step 20100 for last 100 steps: 0.00513269066811\n",
      "Average loss at step 20200 for last 100 steps: 0.00513261139393\n",
      "Average loss at step 20300 for last 100 steps: 0.00538268446922\n",
      "Average loss at step 20400 for last 100 steps: 0.00588253974915\n",
      "Average loss at step 20500 for last 100 steps: 0.00588266313076\n",
      "Average loss at step 20600 for last 100 steps: 0.00463271766901\n",
      "Average loss at step 20700 for last 100 steps: 0.00463268607855\n",
      "Average loss at step 20800 for last 100 steps: 0.00438273429871\n",
      "Average loss at step 20900 for last 100 steps: 0.00463272094727\n",
      "Average loss at step 21000 for last 100 steps: 0.00413275063038\n",
      "Average loss at step 21100 for last 100 steps: 0.0053826379776\n",
      "Average loss at step 21200 for last 100 steps: 0.00438274204731\n",
      "Average loss at step 21300 for last 100 steps: 0.00588265836239\n",
      "Average loss at step 21400 for last 100 steps: 0.00463272631168\n",
      "Average loss at step 21500 for last 100 steps: 0.00613256156445\n",
      "Average loss at step 21600 for last 100 steps: 0.00513265728951\n",
      "Average loss at step 21700 for last 100 steps: 0.0051326161623\n",
      "Average loss at step 21800 for last 100 steps: 0.00563257396221\n",
      "Average loss at step 21900 for last 100 steps: 0.00438269436359\n",
      "Average loss at step 22000 for last 100 steps: 0.00513262629509\n",
      "Average loss at step 22100 for last 100 steps: 0.00488271147013\n",
      "Average loss at step 22200 for last 100 steps: 0.00463268190622\n",
      "Average loss at step 22300 for last 100 steps: 0.00463268220425\n",
      "Average loss at step 22400 for last 100 steps: 0.00513266026974\n",
      "Average loss at step 22500 for last 100 steps: 0.00413274914026\n",
      "Average loss at step 22600 for last 100 steps: 0.00413274466991\n",
      "Average loss at step 22700 for last 100 steps: 0.00538264870644\n",
      "Average loss at step 22800 for last 100 steps: 0.00588265478611\n",
      "Average loss at step 22900 for last 100 steps: 0.00463268607855\n",
      "Average loss at step 23000 for last 100 steps: 0.00463268190622\n",
      "Average loss at step 23100 for last 100 steps: 0.00513265728951\n",
      "Average loss at step 23200 for last 100 steps: 0.00513261198997\n",
      "Average loss at step 23300 for last 100 steps: 0.00538260340691\n",
      "Average loss at step 23400 for last 100 steps: 0.00513260781765\n",
      "Average loss at step 23500 for last 100 steps: 0.00513268828392\n",
      "Average loss at step 23600 for last 100 steps: 0.00488269895315\n",
      "Average loss at step 23700 for last 100 steps: 0.00563255310059\n",
      "Average loss at step 23800 for last 100 steps: 0.00563258469105\n",
      "Average loss at step 23900 for last 100 steps: 0.00513265013695\n",
      "Average loss at step 24000 for last 100 steps: 0.00563266694546\n",
      "Average loss at step 24100 for last 100 steps: 0.00463271051645\n",
      "Average loss at step 24200 for last 100 steps: 0.0061325699091\n",
      "Average loss at step 24300 for last 100 steps: 0.00613263845444\n",
      "Average loss at step 24400 for last 100 steps: 0.00338277816772\n",
      "Average loss at step 24500 for last 100 steps: 0.00513264656067\n",
      "Average loss at step 24600 for last 100 steps: 0.00463271856308\n",
      "Average loss at step 24700 for last 100 steps: 0.00388275355101\n",
      "Average loss at step 24800 for last 100 steps: 0.00538263440132\n",
      "Average loss at step 24900 for last 100 steps: 0.00463271707296\n",
      "\n",
      " Epoch 4\n",
      "Average loss at step 100 for last 100 steps: 0.00438268184662\n",
      "Average loss at step 200 for last 100 steps: 0.00563259541988\n",
      "Average loss at step 300 for last 100 steps: 0.00588257193565\n",
      "Average loss at step 400 for last 100 steps: 0.00513269066811\n",
      "Average loss at step 500 for last 100 steps: 0.00513265252113\n",
      "Average loss at step 600 for last 100 steps: 0.00463272243738\n",
      "Average loss at step 700 for last 100 steps: 0.00513269007206\n",
      "Average loss at step 800 for last 100 steps: 0.00488267093897\n",
      "Average loss at step 900 for last 100 steps: 0.00538261055946\n",
      "Average loss at step 1000 for last 100 steps: 0.00488267511129\n",
      "Average loss at step 1100 for last 100 steps: 0.00563257813454\n",
      "Average loss at step 1200 for last 100 steps: 0.00463271617889\n",
      "Average loss at step 1300 for last 100 steps: 0.00513269603252\n",
      "Average loss at step 1400 for last 100 steps: 0.00488270759583\n",
      "Average loss at step 1500 for last 100 steps: 0.00513266205788\n",
      "Average loss at step 1600 for last 100 steps: 0.00488266646862\n",
      "Average loss at step 1700 for last 100 steps: 0.00588258743286\n",
      "Average loss at step 1800 for last 100 steps: 0.00613259613514\n",
      "Average loss at step 1900 for last 100 steps: 0.00488265365362\n",
      "Average loss at step 2000 for last 100 steps: 0.00463267624378\n",
      "Average loss at step 2100 for last 100 steps: 0.00488265991211\n",
      "Average loss at step 2200 for last 100 steps: 0.00463268667459\n",
      "Average loss at step 2300 for last 100 steps: 0.00563267350197\n",
      "Average loss at step 2400 for last 100 steps: 0.0043826842308\n",
      "Average loss at step 2500 for last 100 steps: 0.0041327381134\n",
      "Average loss at step 2600 for last 100 steps: 0.00488270759583\n",
      "Average loss at step 2700 for last 100 steps: 0.00513269484043\n",
      "Average loss at step 2800 for last 100 steps: 0.00363276600838\n",
      "Average loss at step 2900 for last 100 steps: 0.00463258326054\n",
      "Average loss at step 3000 for last 100 steps: 0.0043826803565\n",
      "Average loss at step 3100 for last 100 steps: 0.00588258624077\n",
      "Average loss at step 3200 for last 100 steps: 0.00438272893429\n",
      "Average loss at step 3300 for last 100 steps: 0.00463271051645\n",
      "Average loss at step 3400 for last 100 steps: 0.00488261282444\n",
      "Average loss at step 3500 for last 100 steps: 0.00588257730007\n",
      "Average loss at step 3600 for last 100 steps: 0.00463259875774\n",
      "Average loss at step 3700 for last 100 steps: 0.0058825647831\n",
      "Average loss at step 3800 for last 100 steps: 0.00463262975216\n",
      "Average loss at step 3900 for last 100 steps: 0.00463267147541\n",
      "Average loss at step 4000 for last 100 steps: 0.00413273721933\n",
      "Average loss at step 4100 for last 100 steps: 0.00588262021542\n",
      "Average loss at step 4200 for last 100 steps: 0.00563262581825\n",
      "Average loss at step 4300 for last 100 steps: 0.00513261198997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4400 for last 100 steps: 0.00463270664215\n",
      "Average loss at step 4500 for last 100 steps: 0.00463271051645\n",
      "Average loss at step 4600 for last 100 steps: 0.00563262283802\n",
      "Average loss at step 4700 for last 100 steps: 0.00513264477253\n",
      "Average loss at step 4800 for last 100 steps: 0.00463263839483\n",
      "Average loss at step 4900 for last 100 steps: 0.00538264632225\n",
      "Average loss at step 5000 for last 100 steps: 0.00563259780407\n",
      "Average loss at step 5100 for last 100 steps: 0.00413269132376\n",
      "Average loss at step 5200 for last 100 steps: 0.00413273900747\n",
      "Average loss at step 5300 for last 100 steps: 0.00488270461559\n",
      "Average loss at step 5400 for last 100 steps: 0.00538267850876\n",
      "Average loss at step 5500 for last 100 steps: 0.00463266760111\n",
      "Average loss at step 5600 for last 100 steps: 0.00463271230459\n",
      "Average loss at step 5700 for last 100 steps: 0.00488270461559\n",
      "Average loss at step 5800 for last 100 steps: 0.00538264632225\n",
      "Average loss at step 5900 for last 100 steps: 0.00563267052174\n",
      "Average loss at step 6000 for last 100 steps: 0.00538264274597\n",
      "Average loss at step 6100 for last 100 steps: 0.00388274282217\n",
      "Average loss at step 6200 for last 100 steps: 0.00463268369436\n",
      "Average loss at step 6300 for last 100 steps: 0.00438272386789\n",
      "Average loss at step 6400 for last 100 steps: 0.00563266396523\n",
      "Average loss at step 6500 for last 100 steps: 0.00488265514374\n",
      "Average loss at step 6600 for last 100 steps: 0.00463263511658\n",
      "Average loss at step 6700 for last 100 steps: 0.00538264274597\n",
      "Average loss at step 6800 for last 100 steps: 0.00513268828392\n",
      "Average loss at step 6900 for last 100 steps: 0.00513268589973\n",
      "Average loss at step 7000 for last 100 steps: 0.00638263344765\n",
      "Average loss at step 7100 for last 100 steps: 0.00538263320923\n",
      "Average loss at step 7200 for last 100 steps: 0.00463271051645\n",
      "Average loss at step 7300 for last 100 steps: 0.00638263583183\n",
      "Average loss at step 7400 for last 100 steps: 0.00563266158104\n",
      "Average loss at step 7500 for last 100 steps: 0.00438271850348\n",
      "Average loss at step 7600 for last 100 steps: 0.00388271182775\n",
      "Average loss at step 7700 for last 100 steps: 0.00513260245323\n",
      "Average loss at step 7800 for last 100 steps: 0.0038827034831\n",
      "Average loss at step 7900 for last 100 steps: 0.00488266378641\n",
      "Average loss at step 8000 for last 100 steps: 0.00588265478611\n",
      "Average loss at step 8100 for last 100 steps: 0.00438268989325\n",
      "Average loss at step 8200 for last 100 steps: 0.0043826842308\n",
      "Average loss at step 8300 for last 100 steps: 0.00613259911537\n",
      "Average loss at step 8400 for last 100 steps: 0.00388270556927\n",
      "Average loss at step 8500 for last 100 steps: 0.00513264715672\n",
      "Average loss at step 8600 for last 100 steps: 0.00463266938925\n",
      "Average loss at step 8700 for last 100 steps: 0.00513268351555\n",
      "Average loss at step 8800 for last 100 steps: 0.00613264083862\n",
      "Average loss at step 8900 for last 100 steps: 0.00463267236948\n",
      "Average loss at step 9000 for last 100 steps: 0.0058825802803\n",
      "Average loss at step 9100 for last 100 steps: 0.0051326495409\n",
      "Average loss at step 9200 for last 100 steps: 0.00538267970085\n",
      "Average loss at step 9300 for last 100 steps: 0.0053826791048\n",
      "Average loss at step 9400 for last 100 steps: 0.00513269007206\n",
      "Average loss at step 9500 for last 100 steps: 0.00463267803192\n",
      "Average loss at step 9600 for last 100 steps: 0.00438267618418\n",
      "Average loss at step 9700 for last 100 steps: 0.00563262581825\n",
      "Average loss at step 9800 for last 100 steps: 0.00463270485401\n",
      "Average loss at step 9900 for last 100 steps: 0.00488270103931\n",
      "Average loss at step 10000 for last 100 steps: 0.00388273775578\n",
      "Average loss at step 10100 for last 100 steps: 0.00613255679607\n",
      "Average loss at step 10200 for last 100 steps: 0.00488266766071\n",
      "Average loss at step 10300 for last 100 steps: 0.00538267493248\n",
      "Average loss at step 10400 for last 100 steps: 0.00513268232346\n",
      "Average loss at step 10500 for last 100 steps: 0.00463270097971\n",
      "Average loss at step 10600 for last 100 steps: 0.0051326829195\n",
      "Average loss at step 10700 for last 100 steps: 0.00463267326355\n",
      "Average loss at step 10800 for last 100 steps: 0.00463267713785\n",
      "Average loss at step 10900 for last 100 steps: 0.00588262677193\n",
      "Average loss at step 11000 for last 100 steps: 0.00638263583183\n",
      "Average loss at step 11100 for last 100 steps: 0.00513265252113\n",
      "Average loss at step 11200 for last 100 steps: 0.00488269984722\n",
      "Average loss at step 11300 for last 100 steps: 0.00463267982006\n",
      "Average loss at step 11400 for last 100 steps: 0.00413273274899\n",
      "Average loss at step 11500 for last 100 steps: 0.00513264298439\n",
      "Average loss at step 11600 for last 100 steps: 0.00688251316547\n",
      "Average loss at step 11700 for last 100 steps: 0.00638262569904\n",
      "Average loss at step 11800 for last 100 steps: 0.0058826225996\n",
      "Average loss at step 11900 for last 100 steps: 0.00563266396523\n",
      "Average loss at step 12000 for last 100 steps: 0.00613260388374\n",
      "Average loss at step 12100 for last 100 steps: 0.00463270962238\n",
      "Average loss at step 12200 for last 100 steps: 0.00513265252113\n",
      "Average loss at step 12300 for last 100 steps: 0.00613257586956\n",
      "Average loss at step 12400 for last 100 steps: 0.0048826956749\n",
      "Average loss at step 12500 for last 100 steps: 0.00638263165951\n",
      "Average loss at step 12600 for last 100 steps: 0.00513264656067\n",
      "Average loss at step 12700 for last 100 steps: 0.00463267564774\n",
      "Average loss at step 12800 for last 100 steps: 0.00513268470764\n",
      "Average loss at step 12900 for last 100 steps: 0.0056326597929\n",
      "Average loss at step 13000 for last 100 steps: 0.00538263559341\n",
      "Average loss at step 13100 for last 100 steps: 0.00438268989325\n",
      "Average loss at step 13200 for last 100 steps: 0.00538264095783\n",
      "Average loss at step 13300 for last 100 steps: 0.00438271611929\n",
      "Average loss at step 13400 for last 100 steps: 0.00613257884979\n",
      "Average loss at step 13500 for last 100 steps: 0.00413268864155\n",
      "Average loss at step 13600 for last 100 steps: 0.00363275438547\n",
      "Average loss at step 13700 for last 100 steps: 0.00463262498379\n",
      "Average loss at step 13800 for last 100 steps: 0.00638256371021\n",
      "Average loss at step 13900 for last 100 steps: 0.00463267564774\n",
      "Average loss at step 14000 for last 100 steps: 0.00463266700506\n",
      "Average loss at step 14100 for last 100 steps: 0.00388274282217\n",
      "Average loss at step 14200 for last 100 steps: 0.00438267409801\n",
      "Average loss at step 14300 for last 100 steps: 0.00413272291422\n",
      "Average loss at step 14400 for last 100 steps: 0.00538263320923\n",
      "Average loss at step 14500 for last 100 steps: 0.00413272857666\n",
      "Average loss at step 14600 for last 100 steps: 0.00438271820545\n",
      "Average loss at step 14700 for last 100 steps: 0.0038827419281\n",
      "Average loss at step 14800 for last 100 steps: 0.00488265603781\n",
      "Average loss at step 14900 for last 100 steps: 0.00463266670704\n",
      "Average loss at step 15000 for last 100 steps: 0.00513268589973\n",
      "Average loss at step 15100 for last 100 steps: 0.00438264369965\n",
      "Average loss at step 15200 for last 100 steps: 0.00363275229931\n",
      "Average loss at step 15300 for last 100 steps: 0.00613260626793\n",
      "Average loss at step 15400 for last 100 steps: 0.00538267016411\n",
      "Average loss at step 15500 for last 100 steps: 0.00613260447979\n",
      "Average loss at step 15600 for last 100 steps: 0.00513260960579\n",
      "Average loss at step 15700 for last 100 steps: 0.00488266080618\n",
      "Average loss at step 15800 for last 100 steps: 0.00513265371323\n",
      "Average loss at step 15900 for last 100 steps: 0.00463266611099\n",
      "Average loss at step 16000 for last 100 steps: 0.0048826494813\n",
      "Average loss at step 16100 for last 100 steps: 0.0051326417923\n",
      "Average loss at step 16200 for last 100 steps: 0.00563259780407\n",
      "Average loss at step 16300 for last 100 steps: 0.00463267087936\n",
      "Average loss at step 16400 for last 100 steps: 0.00463265955448\n",
      "Average loss at step 16500 for last 100 steps: 0.00488266080618\n",
      "Average loss at step 16600 for last 100 steps: 0.00463269948959\n",
      "Average loss at step 16700 for last 100 steps: 0.00513267874718\n",
      "Average loss at step 16800 for last 100 steps: 0.00438271582127\n",
      "Average loss at step 16900 for last 100 steps: 0.00413272291422\n",
      "Average loss at step 17000 for last 100 steps: 0.00538263142109\n",
      "Average loss at step 17100 for last 100 steps: 0.00363274365664\n",
      "Average loss at step 17200 for last 100 steps: 0.00463266551495\n",
      "Average loss at step 17300 for last 100 steps: 0.00438271939754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 17400 for last 100 steps: 0.00538263082504\n",
      "Average loss at step 17500 for last 100 steps: 0.00513260960579\n",
      "Average loss at step 17600 for last 100 steps: 0.00638255417347\n",
      "Average loss at step 17700 for last 100 steps: 0.00438271611929\n",
      "Average loss at step 17800 for last 100 steps: 0.00538259983063\n",
      "Average loss at step 17900 for last 100 steps: 0.00563262403011\n",
      "Average loss at step 18000 for last 100 steps: 0.00638259410858\n",
      "Average loss at step 18100 for last 100 steps: 0.00463269859552\n",
      "Average loss at step 18200 for last 100 steps: 0.0051326161623\n",
      "Average loss at step 18300 for last 100 steps: 0.00463266193867\n",
      "Average loss at step 18400 for last 100 steps: 0.00438271135092\n",
      "Average loss at step 18500 for last 100 steps: 0.00563258230686\n",
      "Average loss at step 18600 for last 100 steps: 0.00438271135092\n",
      "Average loss at step 18700 for last 100 steps: 0.00513267993927\n",
      "Average loss at step 18800 for last 100 steps: 0.00413271844387\n",
      "Average loss at step 18900 for last 100 steps: 0.00488268852234\n",
      "Average loss at step 19000 for last 100 steps: 0.00663262069225\n",
      "Average loss at step 19100 for last 100 steps: 0.00613260686398\n",
      "Average loss at step 19200 for last 100 steps: 0.00463266938925\n",
      "Average loss at step 19300 for last 100 steps: 0.00563265740871\n",
      "Average loss at step 19400 for last 100 steps: 0.00488265424967\n",
      "Average loss at step 19500 for last 100 steps: 0.00463269710541\n",
      "Average loss at step 19600 for last 100 steps: 0.00538266777992\n",
      "Average loss at step 19700 for last 100 steps: 0.00463269472122\n",
      "Average loss at step 19800 for last 100 steps: 0.00463262140751\n",
      "Average loss at step 19900 for last 100 steps: 0.006382599473\n",
      "Average loss at step 20000 for last 100 steps: 0.00463269770145\n",
      "Average loss at step 20100 for last 100 steps: 0.00613264381886\n",
      "Average loss at step 20200 for last 100 steps: 0.0038827341795\n",
      "Average loss at step 20300 for last 100 steps: 0.00613260626793\n",
      "Average loss at step 20400 for last 100 steps: 0.00488264739513\n",
      "Average loss at step 20500 for last 100 steps: 0.00538263678551\n",
      "Average loss at step 20600 for last 100 steps: 0.00588264465332\n",
      "Average loss at step 20700 for last 100 steps: 0.00463267505169\n",
      "Average loss at step 20800 for last 100 steps: 0.00513267338276\n",
      "Average loss at step 20900 for last 100 steps: 0.0048826867342\n",
      "Average loss at step 21000 for last 100 steps: 0.00413271605968\n",
      "Average loss at step 21100 for last 100 steps: 0.00513267397881\n",
      "Average loss at step 21200 for last 100 steps: 0.00588258385658\n",
      "Average loss at step 21300 for last 100 steps: 0.00513267934322\n",
      "Average loss at step 21400 for last 100 steps: 0.00463269621134\n",
      "Average loss at step 21500 for last 100 steps: 0.0048826867342\n",
      "Average loss at step 21600 for last 100 steps: 0.00438270717859\n",
      "Average loss at step 21700 for last 100 steps: 0.00413268268108\n",
      "Average loss at step 21800 for last 100 steps: 0.00463269650936\n",
      "Average loss at step 21900 for last 100 steps: 0.0051325750351\n",
      "Average loss at step 22000 for last 100 steps: 0.0058826225996\n",
      "Average loss at step 22100 for last 100 steps: 0.00413269102573\n",
      "Average loss at step 22200 for last 100 steps: 0.00513267874718\n",
      "Average loss at step 22300 for last 100 steps: 0.00388273149729\n",
      "Average loss at step 22400 for last 100 steps: 0.00563263237476\n",
      "Average loss at step 22500 for last 100 steps: 0.00438267767429\n",
      "Average loss at step 22600 for last 100 steps: 0.00638262808323\n",
      "Average loss at step 22700 for last 100 steps: 0.00638256907463\n",
      "Average loss at step 22800 for last 100 steps: 0.00438271194696\n",
      "Average loss at step 22900 for last 100 steps: 0.00413271903992\n",
      "Average loss at step 23000 for last 100 steps: 0.00563258528709\n",
      "Average loss at step 23100 for last 100 steps: 0.00613261282444\n",
      "Average loss at step 23200 for last 100 steps: 0.00563266277313\n",
      "Average loss at step 23300 for last 100 steps: 0.0046326571703\n",
      "Average loss at step 23400 for last 100 steps: 0.00438264280558\n",
      "Average loss at step 23500 for last 100 steps: 0.00513264834881\n",
      "Average loss at step 23600 for last 100 steps: 0.00463270097971\n",
      "Average loss at step 23700 for last 100 steps: 0.00563265919685\n",
      "Average loss at step 23800 for last 100 steps: 0.00613252878189\n",
      "Average loss at step 23900 for last 100 steps: 0.00513264417648\n",
      "Average loss at step 24000 for last 100 steps: 0.00538263916969\n",
      "Average loss at step 24100 for last 100 steps: 0.00388272702694\n",
      "Average loss at step 24200 for last 100 steps: 0.00413264423609\n",
      "Average loss at step 24300 for last 100 steps: 0.00663261532784\n",
      "Average loss at step 24400 for last 100 steps: 0.00538256525993\n",
      "Average loss at step 24500 for last 100 steps: 0.0048826533556\n",
      "Average loss at step 24600 for last 100 steps: 0.00713256955147\n",
      "Average loss at step 24700 for last 100 steps: 0.00638262808323\n",
      "Average loss at step 24800 for last 100 steps: 0.00438268005848\n",
      "Average loss at step 24900 for last 100 steps: 0.00538259327412\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD6CAYAAABOIFvoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nouzRqHAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_losses = train(5,num_steps)\n",
    "plt.plot(training_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/Users/caijie/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EPOCH 0\n",
      "Average loss at step 100 for last 100 steps: 0.00612698674202\n",
      "Average loss at step 200 for last 100 steps: 0.00611189961433\n",
      "Average loss at step 300 for last 100 steps: 0.00636170685291\n",
      "Average loss at step 400 for last 100 steps: 0.00676086306572\n",
      "Average loss at step 500 for last 100 steps: 0.00696432411671\n",
      "Average loss at step 600 for last 100 steps: 0.00676354706287\n",
      "Average loss at step 700 for last 100 steps: 0.00658714413643\n",
      "Average loss at step 800 for last 100 steps: 0.00675344228745\n",
      "Average loss at step 900 for last 100 steps: 0.00606065869331\n",
      "Average loss at step 1000 for last 100 steps: 0.00669437408447\n",
      "Average loss at step 1100 for last 100 steps: 0.00640906095505\n",
      "Average loss at step 1200 for last 100 steps: 0.00669491767883\n",
      "Average loss at step 1300 for last 100 steps: 0.00658537089825\n",
      "Average loss at step 1400 for last 100 steps: 0.00499543577433\n",
      "Average loss at step 1500 for last 100 steps: 0.00582120478153\n",
      "Average loss at step 1600 for last 100 steps: 0.00535673737526\n",
      "Average loss at step 1700 for last 100 steps: 0.00533659338951\n",
      "Average loss at step 1800 for last 100 steps: 0.00618882417679\n",
      "Average loss at step 1900 for last 100 steps: 0.0055581176281\n",
      "Average loss at step 2000 for last 100 steps: 0.00582221984863\n",
      "Average loss at step 2100 for last 100 steps: 0.00635502576828\n",
      "Average loss at step 2200 for last 100 steps: 0.00611360669136\n",
      "Average loss at step 2300 for last 100 steps: 0.00502805054188\n",
      "Average loss at step 2400 for last 100 steps: 0.0059973013401\n",
      "Average loss at step 2500 for last 100 steps: 0.0060131919384\n",
      "Average loss at step 2600 for last 100 steps: 0.00619261562824\n",
      "Average loss at step 2700 for last 100 steps: 0.00619066238403\n",
      "Average loss at step 2800 for last 100 steps: 0.00613609194756\n",
      "Average loss at step 2900 for last 100 steps: 0.00703214585781\n",
      "Average loss at step 3000 for last 100 steps: 0.0060047185421\n",
      "Average loss at step 3100 for last 100 steps: 0.00544282734394\n",
      "Average loss at step 3200 for last 100 steps: 0.00599391818047\n",
      "Average loss at step 3300 for last 100 steps: 0.00634511530399\n",
      "Average loss at step 3400 for last 100 steps: 0.00711957156658\n",
      "Average loss at step 3500 for last 100 steps: 0.0061650288105\n",
      "Average loss at step 3600 for last 100 steps: 0.00596802413464\n",
      "Average loss at step 3700 for last 100 steps: 0.00581426143646\n",
      "Average loss at step 3800 for last 100 steps: 0.00580676078796\n",
      "Average loss at step 3900 for last 100 steps: 0.00584787011147\n",
      "Average loss at step 4000 for last 100 steps: 0.00570769667625\n",
      "Average loss at step 4100 for last 100 steps: 0.00583094120026\n",
      "Average loss at step 4200 for last 100 steps: 0.00682579696178\n",
      "Average loss at step 4300 for last 100 steps: 0.00621235191822\n",
      "Average loss at step 4400 for last 100 steps: 0.0059646064043\n",
      "Average loss at step 4500 for last 100 steps: 0.0057161962986\n",
      "Average loss at step 4600 for last 100 steps: 0.00641967475414\n",
      "Average loss at step 4700 for last 100 steps: 0.00612182378769\n",
      "Average loss at step 4800 for last 100 steps: 0.00629192888737\n",
      "Average loss at step 4900 for last 100 steps: 0.00539389908314\n",
      "Average loss at step 5000 for last 100 steps: 0.00606746196747\n",
      "Average loss at step 5100 for last 100 steps: 0.00600021481514\n",
      "Average loss at step 5200 for last 100 steps: 0.00571380972862\n",
      "Average loss at step 5300 for last 100 steps: 0.0063720792532\n",
      "Average loss at step 5400 for last 100 steps: 0.00563738644123\n",
      "Average loss at step 5500 for last 100 steps: 0.00633011996746\n",
      "Average loss at step 5600 for last 100 steps: 0.00577872216702\n",
      "Average loss at step 5700 for last 100 steps: 0.00629865050316\n",
      "Average loss at step 5800 for last 100 steps: 0.00604793071747\n",
      "Average loss at step 5900 for last 100 steps: 0.00617136776447\n",
      "Average loss at step 6000 for last 100 steps: 0.00579996109009\n",
      "Average loss at step 6100 for last 100 steps: 0.00676135063171\n",
      "Average loss at step 6200 for last 100 steps: 0.00621536374092\n",
      "Average loss at step 6300 for last 100 steps: 0.00609832406044\n",
      "Average loss at step 6400 for last 100 steps: 0.00716976046562\n",
      "Average loss at step 6500 for last 100 steps: 0.00596205949783\n",
      "Average loss at step 6600 for last 100 steps: 0.00618764877319\n",
      "Average loss at step 6700 for last 100 steps: 0.00581467747688\n",
      "Average loss at step 6800 for last 100 steps: 0.00597425341606\n",
      "Average loss at step 6900 for last 100 steps: 0.00616394460201\n",
      "Average loss at step 7000 for last 100 steps: 0.00655452251434\n",
      "Average loss at step 7100 for last 100 steps: 0.00668188452721\n",
      "Average loss at step 7200 for last 100 steps: 0.0058912217617\n",
      "Average loss at step 7300 for last 100 steps: 0.00633007347584\n",
      "Average loss at step 7400 for last 100 steps: 0.00596321642399\n",
      "Average loss at step 7500 for last 100 steps: 0.00616964459419\n",
      "Average loss at step 7600 for last 100 steps: 0.00668453454971\n",
      "Average loss at step 7700 for last 100 steps: 0.00575367748737\n",
      "Average loss at step 7800 for last 100 steps: 0.00526759386063\n",
      "Average loss at step 7900 for last 100 steps: 0.00589984714985\n",
      "Average loss at step 8000 for last 100 steps: 0.00663644373417\n",
      "Average loss at step 8100 for last 100 steps: 0.00634043037891\n",
      "Average loss at step 8200 for last 100 steps: 0.00525389194489\n",
      "Average loss at step 8300 for last 100 steps: 0.00656786084175\n",
      "Average loss at step 8400 for last 100 steps: 0.00533059716225\n",
      "Average loss at step 8500 for last 100 steps: 0.00581922709942\n",
      "Average loss at step 8600 for last 100 steps: 0.00591219186783\n",
      "Average loss at step 8700 for last 100 steps: 0.00559862792492\n",
      "Average loss at step 8800 for last 100 steps: 0.00618126690388\n",
      "Average loss at step 8900 for last 100 steps: 0.00612554907799\n",
      "Average loss at step 9000 for last 100 steps: 0.00626577377319\n",
      "Average loss at step 9100 for last 100 steps: 0.00558617293835\n",
      "Average loss at step 9200 for last 100 steps: 0.00513404250145\n",
      "Average loss at step 9300 for last 100 steps: 0.00559471428394\n",
      "Average loss at step 9400 for last 100 steps: 0.00635783553123\n",
      "Average loss at step 9500 for last 100 steps: 0.00638649463654\n",
      "Average loss at step 9600 for last 100 steps: 0.00617594718933\n",
      "Average loss at step 9700 for last 100 steps: 0.00572004377842\n",
      "Average loss at step 9800 for last 100 steps: 0.00585783362389\n",
      "Average loss at step 9900 for last 100 steps: 0.00566807568073\n",
      "Average loss at step 10000 for last 100 steps: 0.00659747302532\n",
      "Average loss at step 10100 for last 100 steps: 0.00551352620125\n",
      "Average loss at step 10200 for last 100 steps: 0.00598188221455\n",
      "Average loss at step 10300 for last 100 steps: 0.00587586820126\n",
      "Average loss at step 10400 for last 100 steps: 0.0060995465517\n",
      "Average loss at step 10500 for last 100 steps: 0.00624172687531\n",
      "Average loss at step 10600 for last 100 steps: 0.00585706532001\n",
      "Average loss at step 10700 for last 100 steps: 0.00608189225197\n",
      "Average loss at step 10800 for last 100 steps: 0.00622301220894\n",
      "Average loss at step 10900 for last 100 steps: 0.00570059895515\n",
      "Average loss at step 11000 for last 100 steps: 0.00669432103634\n",
      "Average loss at step 11100 for last 100 steps: 0.00673943340778\n",
      "Average loss at step 11200 for last 100 steps: 0.00617426276207\n",
      "Average loss at step 11300 for last 100 steps: 0.00583751559258\n",
      "Average loss at step 11400 for last 100 steps: 0.00672184467316\n",
      "Average loss at step 11500 for last 100 steps: 0.00607769012451\n",
      "Average loss at step 11600 for last 100 steps: 0.00612613379955\n",
      "Average loss at step 11700 for last 100 steps: 0.00583389639854\n",
      "Average loss at step 11800 for last 100 steps: 0.00635072410107\n",
      "Average loss at step 11900 for last 100 steps: 0.00602286458015\n",
      "Average loss at step 12000 for last 100 steps: 0.00667051494122\n",
      "Average loss at step 12100 for last 100 steps: 0.00650765955448\n",
      "Average loss at step 12200 for last 100 steps: 0.00669524312019\n",
      "Average loss at step 12300 for last 100 steps: 0.00541489601135\n",
      "Average loss at step 12400 for last 100 steps: 0.00644613146782\n",
      "Average loss at step 12500 for last 100 steps: 0.00630869567394\n",
      "Average loss at step 12600 for last 100 steps: 0.0059875780344\n",
      "Average loss at step 12700 for last 100 steps: 0.00549661457539\n",
      "Average loss at step 12800 for last 100 steps: 0.00570097267628\n",
      "Average loss at step 12900 for last 100 steps: 0.00568659603596\n",
      "Average loss at step 13000 for last 100 steps: 0.00660706818104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 13100 for last 100 steps: 0.00598557829857\n",
      "Average loss at step 13200 for last 100 steps: 0.00496373474598\n",
      "Average loss at step 13300 for last 100 steps: 0.00555088222027\n",
      "Average loss at step 13400 for last 100 steps: 0.00579398274422\n",
      "Average loss at step 13500 for last 100 steps: 0.00596034228802\n",
      "Average loss at step 13600 for last 100 steps: 0.00716067314148\n",
      "Average loss at step 13700 for last 100 steps: 0.0063054561615\n",
      "Average loss at step 13800 for last 100 steps: 0.00604588806629\n",
      "Average loss at step 13900 for last 100 steps: 0.00624220490456\n",
      "Average loss at step 14000 for last 100 steps: 0.00604621589184\n",
      "Average loss at step 14100 for last 100 steps: 0.00563490748405\n",
      "Average loss at step 14200 for last 100 steps: 0.0071887499094\n",
      "Average loss at step 14300 for last 100 steps: 0.00578392028809\n",
      "Average loss at step 14400 for last 100 steps: 0.00624545276165\n",
      "Average loss at step 14500 for last 100 steps: 0.00574135184288\n",
      "Average loss at step 14600 for last 100 steps: 0.00569518625736\n",
      "Average loss at step 14700 for last 100 steps: 0.00596587240696\n",
      "Average loss at step 14800 for last 100 steps: 0.00615639090538\n",
      "Average loss at step 14900 for last 100 steps: 0.00598879814148\n",
      "Average loss at step 15000 for last 100 steps: 0.00529023766518\n",
      "Average loss at step 15100 for last 100 steps: 0.00576190471649\n",
      "Average loss at step 15200 for last 100 steps: 0.00625361919403\n",
      "Average loss at step 15300 for last 100 steps: 0.00598271667957\n",
      "Average loss at step 15400 for last 100 steps: 0.00645102918148\n",
      "Average loss at step 15500 for last 100 steps: 0.00640634834766\n",
      "Average loss at step 15600 for last 100 steps: 0.0063121509552\n",
      "Average loss at step 15700 for last 100 steps: 0.00580385386944\n",
      "Average loss at step 15800 for last 100 steps: 0.00588201999664\n",
      "Average loss at step 15900 for last 100 steps: 0.00561919271946\n",
      "Average loss at step 16000 for last 100 steps: 0.00629778265953\n",
      "Average loss at step 16100 for last 100 steps: 0.00648447930813\n",
      "Average loss at step 16200 for last 100 steps: 0.00581625819206\n",
      "Average loss at step 16300 for last 100 steps: 0.00565696358681\n",
      "Average loss at step 16400 for last 100 steps: 0.00504437565804\n",
      "Average loss at step 16500 for last 100 steps: 0.00581571578979\n",
      "Average loss at step 16600 for last 100 steps: 0.00575739502907\n",
      "Average loss at step 16700 for last 100 steps: 0.00641715347767\n",
      "Average loss at step 16800 for last 100 steps: 0.00563462615013\n",
      "Average loss at step 16900 for last 100 steps: 0.00562754511833\n",
      "Average loss at step 17000 for last 100 steps: 0.00586140275002\n",
      "Average loss at step 17100 for last 100 steps: 0.00537364661694\n",
      "Average loss at step 17200 for last 100 steps: 0.00776791870594\n",
      "Average loss at step 17300 for last 100 steps: 0.00634714603424\n",
      "Average loss at step 17400 for last 100 steps: 0.00643115520477\n",
      "Average loss at step 17500 for last 100 steps: 0.00589222550392\n",
      "Average loss at step 17600 for last 100 steps: 0.00595539569855\n",
      "Average loss at step 17700 for last 100 steps: 0.00569558799267\n",
      "Average loss at step 17800 for last 100 steps: 0.00622841536999\n",
      "Average loss at step 17900 for last 100 steps: 0.00608676373959\n",
      "Average loss at step 18000 for last 100 steps: 0.00666597962379\n",
      "Average loss at step 18100 for last 100 steps: 0.00594040572643\n",
      "Average loss at step 18200 for last 100 steps: 0.0059228926897\n",
      "Average loss at step 18300 for last 100 steps: 0.00571514129639\n",
      "Average loss at step 18400 for last 100 steps: 0.00585110247135\n",
      "Average loss at step 18500 for last 100 steps: 0.00580514252186\n",
      "Average loss at step 18600 for last 100 steps: 0.00538547456264\n",
      "Average loss at step 18700 for last 100 steps: 0.00567469239235\n",
      "Average loss at step 18800 for last 100 steps: 0.00636749148369\n",
      "Average loss at step 18900 for last 100 steps: 0.00554121673107\n",
      "Average loss at step 19000 for last 100 steps: 0.00587961852551\n",
      "Average loss at step 19100 for last 100 steps: 0.00573061108589\n",
      "Average loss at step 19200 for last 100 steps: 0.00604238390923\n",
      "Average loss at step 19300 for last 100 steps: 0.00598633766174\n",
      "Average loss at step 19400 for last 100 steps: 0.00602276325226\n",
      "Average loss at step 19500 for last 100 steps: 0.00640219807625\n",
      "Average loss at step 19600 for last 100 steps: 0.0060805439949\n",
      "Average loss at step 19700 for last 100 steps: 0.00591858386993\n",
      "Average loss at step 19800 for last 100 steps: 0.00580966293812\n",
      "Average loss at step 19900 for last 100 steps: 0.00608750343323\n",
      "Average loss at step 20000 for last 100 steps: 0.00686098754406\n",
      "Average loss at step 20100 for last 100 steps: 0.00596341431141\n",
      "Average loss at step 20200 for last 100 steps: 0.00671176075935\n",
      "Average loss at step 20300 for last 100 steps: 0.0065433371067\n",
      "Average loss at step 20400 for last 100 steps: 0.00583699405193\n",
      "Average loss at step 20500 for last 100 steps: 0.00600866317749\n",
      "Average loss at step 20600 for last 100 steps: 0.00662098228931\n",
      "Average loss at step 20700 for last 100 steps: 0.00584651529789\n",
      "Average loss at step 20800 for last 100 steps: 0.00606968224049\n",
      "Average loss at step 20900 for last 100 steps: 0.007362434268\n",
      "Average loss at step 21000 for last 100 steps: 0.00628357768059\n",
      "Average loss at step 21100 for last 100 steps: 0.00610064148903\n",
      "Average loss at step 21200 for last 100 steps: 0.00585326969624\n",
      "Average loss at step 21300 for last 100 steps: 0.00600770115852\n",
      "Average loss at step 21400 for last 100 steps: 0.00593231022358\n",
      "Average loss at step 21500 for last 100 steps: 0.00534579873085\n",
      "Average loss at step 21600 for last 100 steps: 0.00654672026634\n",
      "Average loss at step 21700 for last 100 steps: 0.00507514119148\n",
      "Average loss at step 21800 for last 100 steps: 0.00611783623695\n",
      "Average loss at step 21900 for last 100 steps: 0.00612547636032\n",
      "Average loss at step 22000 for last 100 steps: 0.00626291811466\n",
      "Average loss at step 22100 for last 100 steps: 0.00581830382347\n",
      "Average loss at step 22200 for last 100 steps: 0.0058892506361\n",
      "Average loss at step 22300 for last 100 steps: 0.00564570069313\n",
      "Average loss at step 22400 for last 100 steps: 0.00573714256287\n",
      "Average loss at step 22500 for last 100 steps: 0.00605206131935\n",
      "Average loss at step 22600 for last 100 steps: 0.00583201050758\n",
      "Average loss at step 22700 for last 100 steps: 0.00534557938576\n",
      "Average loss at step 22800 for last 100 steps: 0.00504538059235\n",
      "Average loss at step 22900 for last 100 steps: 0.00656852722168\n",
      "Average loss at step 23000 for last 100 steps: 0.00633678615093\n",
      "Average loss at step 23100 for last 100 steps: 0.00599657893181\n",
      "Average loss at step 23200 for last 100 steps: 0.00602448880672\n",
      "Average loss at step 23300 for last 100 steps: 0.00595353960991\n",
      "Average loss at step 23400 for last 100 steps: 0.00605215191841\n",
      "Average loss at step 23500 for last 100 steps: 0.00617185711861\n",
      "Average loss at step 23600 for last 100 steps: 0.00573580741882\n",
      "Average loss at step 23700 for last 100 steps: 0.0048747959733\n",
      "Average loss at step 23800 for last 100 steps: 0.00609444499016\n",
      "Average loss at step 23900 for last 100 steps: 0.00588003277779\n",
      "Average loss at step 24000 for last 100 steps: 0.00600966691971\n",
      "Average loss at step 24100 for last 100 steps: 0.00664150953293\n",
      "Average loss at step 24200 for last 100 steps: 0.00596642792225\n",
      "Average loss at step 24300 for last 100 steps: 0.00667490363121\n",
      "Average loss at step 24400 for last 100 steps: 0.00560966849327\n",
      "Average loss at step 24500 for last 100 steps: 0.00472290277481\n",
      "Average loss at step 24600 for last 100 steps: 0.00647280573845\n",
      "Average loss at step 24700 for last 100 steps: 0.00547430872917\n",
      "Average loss at step 24800 for last 100 steps: 0.00589341461658\n",
      "Average loss at step 24900 for last 100 steps: 0.00695860922337\n",
      "\n",
      " EPOCH 1\n",
      "Average loss at step 100 for last 100 steps: 0.00590946793556\n",
      "Average loss at step 200 for last 100 steps: 0.00530752182007\n",
      "Average loss at step 300 for last 100 steps: 0.00609793782234\n",
      "Average loss at step 400 for last 100 steps: 0.00574316561222\n",
      "Average loss at step 500 for last 100 steps: 0.00550018012524\n",
      "Average loss at step 600 for last 100 steps: 0.00576154589653\n",
      "Average loss at step 700 for last 100 steps: 0.00581646621227\n",
      "Average loss at step 800 for last 100 steps: 0.00687297463417\n",
      "Average loss at step 900 for last 100 steps: 0.00643552362919\n",
      "Average loss at step 1000 for last 100 steps: 0.00568987369537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 1100 for last 100 steps: 0.00624753952026\n",
      "Average loss at step 1200 for last 100 steps: 0.00581322789192\n",
      "Average loss at step 1300 for last 100 steps: 0.00567038238049\n",
      "Average loss at step 1400 for last 100 steps: 0.00628666222095\n",
      "Average loss at step 1500 for last 100 steps: 0.00634530603886\n",
      "Average loss at step 1600 for last 100 steps: 0.00666896343231\n",
      "Average loss at step 1700 for last 100 steps: 0.00591654002666\n",
      "Average loss at step 1800 for last 100 steps: 0.00578426122665\n",
      "Average loss at step 1900 for last 100 steps: 0.00648014545441\n",
      "Average loss at step 2000 for last 100 steps: 0.00683946788311\n",
      "Average loss at step 2100 for last 100 steps: 0.00621709764004\n",
      "Average loss at step 2200 for last 100 steps: 0.00525670647621\n",
      "Average loss at step 2300 for last 100 steps: 0.00609576702118\n",
      "Average loss at step 2400 for last 100 steps: 0.00695015013218\n",
      "Average loss at step 2500 for last 100 steps: 0.00650997102261\n",
      "Average loss at step 2600 for last 100 steps: 0.00539555847645\n",
      "Average loss at step 2700 for last 100 steps: 0.00645223855972\n",
      "Average loss at step 2800 for last 100 steps: 0.0058453643322\n",
      "Average loss at step 2900 for last 100 steps: 0.00612968742847\n",
      "Average loss at step 3000 for last 100 steps: 0.00592867672443\n",
      "Average loss at step 3100 for last 100 steps: 0.00515459358692\n",
      "Average loss at step 3200 for last 100 steps: 0.00606086790562\n",
      "Average loss at step 3300 for last 100 steps: 0.00532050073147\n",
      "Average loss at step 3400 for last 100 steps: 0.00633767962456\n",
      "Average loss at step 3500 for last 100 steps: 0.00621920585632\n",
      "Average loss at step 3600 for last 100 steps: 0.00580731213093\n",
      "Average loss at step 3700 for last 100 steps: 0.00667624235153\n",
      "Average loss at step 3800 for last 100 steps: 0.00593240857124\n",
      "Average loss at step 3900 for last 100 steps: 0.00543285727501\n",
      "Average loss at step 4000 for last 100 steps: 0.00614421486855\n",
      "Average loss at step 4100 for last 100 steps: 0.00618741035461\n",
      "Average loss at step 4200 for last 100 steps: 0.00636818051338\n",
      "Average loss at step 4300 for last 100 steps: 0.00558680057526\n",
      "Average loss at step 4400 for last 100 steps: 0.0055247938633\n",
      "Average loss at step 4500 for last 100 steps: 0.00629025578499\n",
      "Average loss at step 4600 for last 100 steps: 0.0068223887682\n",
      "Average loss at step 4700 for last 100 steps: 0.00570781886578\n",
      "Average loss at step 4800 for last 100 steps: 0.0063830691576\n",
      "Average loss at step 4900 for last 100 steps: 0.0057138979435\n",
      "Average loss at step 5000 for last 100 steps: 0.00626041293144\n",
      "Average loss at step 5100 for last 100 steps: 0.00632368862629\n",
      "Average loss at step 5200 for last 100 steps: 0.00653878808022\n",
      "Average loss at step 5300 for last 100 steps: 0.00591122984886\n",
      "Average loss at step 5400 for last 100 steps: 0.00521430969238\n",
      "Average loss at step 5500 for last 100 steps: 0.00625306367874\n",
      "Average loss at step 5600 for last 100 steps: 0.00636830627918\n",
      "Average loss at step 5700 for last 100 steps: 0.00698032081127\n",
      "Average loss at step 5800 for last 100 steps: 0.00611807763577\n",
      "Average loss at step 5900 for last 100 steps: 0.0060001629591\n",
      "Average loss at step 6000 for last 100 steps: 0.00665385127068\n",
      "Average loss at step 6100 for last 100 steps: 0.00609119296074\n",
      "Average loss at step 6200 for last 100 steps: 0.0062575429678\n",
      "Average loss at step 6300 for last 100 steps: 0.00669865310192\n",
      "Average loss at step 6400 for last 100 steps: 0.00642005801201\n",
      "Average loss at step 6500 for last 100 steps: 0.00636684417725\n",
      "Average loss at step 6600 for last 100 steps: 0.00641456782818\n",
      "Average loss at step 6700 for last 100 steps: 0.00586507678032\n",
      "Average loss at step 6800 for last 100 steps: 0.00533371567726\n",
      "Average loss at step 6900 for last 100 steps: 0.00604292213917\n",
      "Average loss at step 7000 for last 100 steps: 0.00519178748131\n",
      "Average loss at step 7100 for last 100 steps: 0.00607773959637\n",
      "Average loss at step 7200 for last 100 steps: 0.00532670259476\n",
      "Average loss at step 7300 for last 100 steps: 0.00634620964527\n",
      "Average loss at step 7400 for last 100 steps: 0.00593449234962\n",
      "Average loss at step 7500 for last 100 steps: 0.00578720092773\n",
      "Average loss at step 7600 for last 100 steps: 0.0058065366745\n",
      "Average loss at step 7700 for last 100 steps: 0.00581302940845\n",
      "Average loss at step 7800 for last 100 steps: 0.00645290434361\n",
      "Average loss at step 7900 for last 100 steps: 0.00621405720711\n",
      "Average loss at step 8000 for last 100 steps: 0.00598039627075\n",
      "Average loss at step 8100 for last 100 steps: 0.00647010862827\n",
      "Average loss at step 8200 for last 100 steps: 0.00600858807564\n",
      "Average loss at step 8300 for last 100 steps: 0.00726428508759\n",
      "Average loss at step 8400 for last 100 steps: 0.00598943114281\n",
      "Average loss at step 8500 for last 100 steps: 0.00571474671364\n",
      "Average loss at step 8600 for last 100 steps: 0.0059867399931\n",
      "Average loss at step 8700 for last 100 steps: 0.0060829180479\n",
      "Average loss at step 8800 for last 100 steps: 0.00630937457085\n",
      "Average loss at step 8900 for last 100 steps: 0.0059649002552\n",
      "Average loss at step 9000 for last 100 steps: 0.00556237816811\n",
      "Average loss at step 9100 for last 100 steps: 0.00607488274574\n",
      "Average loss at step 9200 for last 100 steps: 0.00576152980328\n",
      "Average loss at step 9300 for last 100 steps: 0.00616307079792\n",
      "Average loss at step 9400 for last 100 steps: 0.0064466381073\n",
      "Average loss at step 9500 for last 100 steps: 0.00566442131996\n",
      "Average loss at step 9600 for last 100 steps: 0.00659320950508\n",
      "Average loss at step 9700 for last 100 steps: 0.00626787364483\n",
      "Average loss at step 9800 for last 100 steps: 0.00646499037743\n",
      "Average loss at step 9900 for last 100 steps: 0.00667545795441\n",
      "Average loss at step 10000 for last 100 steps: 0.00615092277527\n",
      "Average loss at step 10100 for last 100 steps: 0.00583908736706\n",
      "Average loss at step 10200 for last 100 steps: 0.00655797302723\n",
      "Average loss at step 10300 for last 100 steps: 0.0056190508604\n",
      "Average loss at step 10400 for last 100 steps: 0.00592119753361\n",
      "Average loss at step 10500 for last 100 steps: 0.00607300281525\n",
      "Average loss at step 10600 for last 100 steps: 0.00590183436871\n",
      "Average loss at step 10700 for last 100 steps: 0.00607771992683\n",
      "Average loss at step 10800 for last 100 steps: 0.00609693408012\n",
      "Average loss at step 10900 for last 100 steps: 0.00682947218418\n",
      "Average loss at step 11000 for last 100 steps: 0.00568515956402\n",
      "Average loss at step 11100 for last 100 steps: 0.00594607949257\n",
      "Average loss at step 11200 for last 100 steps: 0.00603477120399\n",
      "Average loss at step 11300 for last 100 steps: 0.00583507657051\n",
      "Average loss at step 11400 for last 100 steps: 0.00554986774921\n",
      "Average loss at step 11500 for last 100 steps: 0.00576407551765\n",
      "Average loss at step 11600 for last 100 steps: 0.00583258748055\n",
      "Average loss at step 11700 for last 100 steps: 0.006223269701\n",
      "Average loss at step 11800 for last 100 steps: 0.00644673109055\n",
      "Average loss at step 11900 for last 100 steps: 0.00579401254654\n",
      "Average loss at step 12000 for last 100 steps: 0.00588987231255\n",
      "Average loss at step 12100 for last 100 steps: 0.00572769999504\n",
      "Average loss at step 12200 for last 100 steps: 0.00638039886951\n",
      "Average loss at step 12300 for last 100 steps: 0.00517767429352\n",
      "Average loss at step 12400 for last 100 steps: 0.00654572606087\n",
      "Average loss at step 12500 for last 100 steps: 0.00656075239182\n",
      "Average loss at step 12600 for last 100 steps: 0.00510895371437\n",
      "Average loss at step 12700 for last 100 steps: 0.00614745378494\n",
      "Average loss at step 12800 for last 100 steps: 0.00589099287987\n",
      "Average loss at step 12900 for last 100 steps: 0.00567560076714\n",
      "Average loss at step 13000 for last 100 steps: 0.00582981109619\n",
      "Average loss at step 13100 for last 100 steps: 0.00649903595448\n",
      "Average loss at step 13200 for last 100 steps: 0.0060675996542\n",
      "Average loss at step 13300 for last 100 steps: 0.00562082588673\n",
      "Average loss at step 13400 for last 100 steps: 0.00578200221062\n",
      "Average loss at step 13500 for last 100 steps: 0.00658415138721\n",
      "Average loss at step 13600 for last 100 steps: 0.00657411158085\n",
      "Average loss at step 13700 for last 100 steps: 0.00597216248512\n",
      "Average loss at step 13800 for last 100 steps: 0.00565963745117\n",
      "Average loss at step 13900 for last 100 steps: 0.00602237582207\n",
      "Average loss at step 14000 for last 100 steps: 0.00625989079475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 14100 for last 100 steps: 0.00578272342682\n",
      "Average loss at step 14200 for last 100 steps: 0.00649541974068\n",
      "Average loss at step 14300 for last 100 steps: 0.00640415787697\n",
      "Average loss at step 14400 for last 100 steps: 0.00515988111496\n",
      "Average loss at step 14500 for last 100 steps: 0.00695792198181\n",
      "Average loss at step 14600 for last 100 steps: 0.00655750334263\n",
      "Average loss at step 14700 for last 100 steps: 0.00652529120445\n",
      "Average loss at step 14800 for last 100 steps: 0.00581945061684\n",
      "Average loss at step 14900 for last 100 steps: 0.00635095000267\n",
      "Average loss at step 15000 for last 100 steps: 0.00603799939156\n",
      "Average loss at step 15100 for last 100 steps: 0.00622637271881\n",
      "Average loss at step 15200 for last 100 steps: 0.00621214747429\n",
      "Average loss at step 15300 for last 100 steps: 0.00641227543354\n",
      "Average loss at step 15400 for last 100 steps: 0.00599910497665\n",
      "Average loss at step 15500 for last 100 steps: 0.00526887595654\n",
      "Average loss at step 15600 for last 100 steps: 0.0056928217411\n",
      "Average loss at step 15700 for last 100 steps: 0.00500948965549\n",
      "Average loss at step 15800 for last 100 steps: 0.00639833569527\n",
      "Average loss at step 15900 for last 100 steps: 0.00666820645332\n",
      "Average loss at step 16000 for last 100 steps: 0.00620298266411\n",
      "Average loss at step 16100 for last 100 steps: 0.00629598975182\n",
      "Average loss at step 16200 for last 100 steps: 0.00584706187248\n",
      "Average loss at step 16300 for last 100 steps: 0.00519500851631\n",
      "Average loss at step 16400 for last 100 steps: 0.00541031956673\n",
      "Average loss at step 16500 for last 100 steps: 0.005805965662\n",
      "Average loss at step 16600 for last 100 steps: 0.00522006094456\n",
      "Average loss at step 16700 for last 100 steps: 0.00653493285179\n",
      "Average loss at step 16800 for last 100 steps: 0.00589327931404\n",
      "Average loss at step 16900 for last 100 steps: 0.00677144110203\n",
      "Average loss at step 17000 for last 100 steps: 0.00580291628838\n",
      "Average loss at step 17100 for last 100 steps: 0.00623193860054\n",
      "Average loss at step 17200 for last 100 steps: 0.0062651604414\n",
      "Average loss at step 17300 for last 100 steps: 0.0047716742754\n",
      "Average loss at step 17400 for last 100 steps: 0.00644932687283\n",
      "Average loss at step 17500 for last 100 steps: 0.00477385222912\n",
      "Average loss at step 17600 for last 100 steps: 0.00588941514492\n",
      "Average loss at step 17700 for last 100 steps: 0.0059165585041\n",
      "Average loss at step 17800 for last 100 steps: 0.00658638000488\n",
      "Average loss at step 17900 for last 100 steps: 0.00568064391613\n",
      "Average loss at step 18000 for last 100 steps: 0.00554153084755\n",
      "Average loss at step 18100 for last 100 steps: 0.00579182446003\n",
      "Average loss at step 18200 for last 100 steps: 0.00626985371113\n",
      "Average loss at step 18300 for last 100 steps: 0.00555659890175\n",
      "Average loss at step 18400 for last 100 steps: 0.00594995141029\n",
      "Average loss at step 18500 for last 100 steps: 0.00574851930141\n",
      "Average loss at step 18600 for last 100 steps: 0.00569169640541\n",
      "Average loss at step 18700 for last 100 steps: 0.00624037504196\n",
      "Average loss at step 18800 for last 100 steps: 0.00677638292313\n",
      "Average loss at step 18900 for last 100 steps: 0.00608212828636\n",
      "Average loss at step 19000 for last 100 steps: 0.00624318003654\n",
      "Average loss at step 19100 for last 100 steps: 0.00549965143204\n",
      "Average loss at step 19200 for last 100 steps: 0.0058237183094\n",
      "Average loss at step 19300 for last 100 steps: 0.0067891818285\n",
      "Average loss at step 19400 for last 100 steps: 0.00600877344608\n",
      "Average loss at step 19500 for last 100 steps: 0.00646860778332\n",
      "Average loss at step 19600 for last 100 steps: 0.00584110081196\n",
      "Average loss at step 19700 for last 100 steps: 0.00682638168335\n",
      "Average loss at step 19800 for last 100 steps: 0.00587921082973\n",
      "Average loss at step 19900 for last 100 steps: 0.00639226555824\n",
      "Average loss at step 20000 for last 100 steps: 0.00666963756084\n",
      "Average loss at step 20100 for last 100 steps: 0.00601119101048\n",
      "Average loss at step 20200 for last 100 steps: 0.00648814558983\n",
      "Average loss at step 20300 for last 100 steps: 0.00575167536736\n",
      "Average loss at step 20400 for last 100 steps: 0.00634900927544\n",
      "Average loss at step 20500 for last 100 steps: 0.0058019143343\n",
      "Average loss at step 20600 for last 100 steps: 0.00651862025261\n",
      "Average loss at step 20700 for last 100 steps: 0.00586797177792\n",
      "Average loss at step 20800 for last 100 steps: 0.00551137447357\n",
      "Average loss at step 20900 for last 100 steps: 0.00587376236916\n",
      "Average loss at step 21000 for last 100 steps: 0.00602676272392\n",
      "Average loss at step 21100 for last 100 steps: 0.00649974167347\n",
      "Average loss at step 21200 for last 100 steps: 0.00577220201492\n",
      "Average loss at step 21300 for last 100 steps: 0.00589378535748\n",
      "Average loss at step 21400 for last 100 steps: 0.0059146296978\n",
      "Average loss at step 21500 for last 100 steps: 0.00629201889038\n",
      "Average loss at step 21600 for last 100 steps: 0.00636828780174\n",
      "Average loss at step 21700 for last 100 steps: 0.00539780080318\n",
      "Average loss at step 21800 for last 100 steps: 0.00606141924858\n",
      "Average loss at step 21900 for last 100 steps: 0.00522055149078\n",
      "Average loss at step 22000 for last 100 steps: 0.00612239480019\n",
      "Average loss at step 22100 for last 100 steps: 0.0058334928751\n",
      "Average loss at step 22200 for last 100 steps: 0.00613910377026\n",
      "Average loss at step 22300 for last 100 steps: 0.00623965740204\n",
      "Average loss at step 22400 for last 100 steps: 0.00623758554459\n",
      "Average loss at step 22500 for last 100 steps: 0.00580682575703\n",
      "Average loss at step 22600 for last 100 steps: 0.00582035064697\n",
      "Average loss at step 22700 for last 100 steps: 0.00554515242577\n",
      "Average loss at step 22800 for last 100 steps: 0.00636311352253\n",
      "Average loss at step 22900 for last 100 steps: 0.0059477430582\n",
      "Average loss at step 23000 for last 100 steps: 0.00522239089012\n",
      "Average loss at step 23100 for last 100 steps: 0.00635156989098\n",
      "Average loss at step 23200 for last 100 steps: 0.00598907530308\n",
      "Average loss at step 23300 for last 100 steps: 0.005950294137\n",
      "Average loss at step 23400 for last 100 steps: 0.00521508336067\n",
      "Average loss at step 23500 for last 100 steps: 0.00602314770222\n",
      "Average loss at step 23600 for last 100 steps: 0.00520139217377\n",
      "Average loss at step 23700 for last 100 steps: 0.00679333806038\n",
      "Average loss at step 23800 for last 100 steps: 0.00624464809895\n",
      "Average loss at step 23900 for last 100 steps: 0.00649396181107\n",
      "Average loss at step 24000 for last 100 steps: 0.00589554131031\n",
      "Average loss at step 24100 for last 100 steps: 0.00630935192108\n",
      "Average loss at step 24200 for last 100 steps: 0.00529943346977\n",
      "Average loss at step 24300 for last 100 steps: 0.00588797688484\n",
      "Average loss at step 24400 for last 100 steps: 0.00569558978081\n",
      "Average loss at step 24500 for last 100 steps: 0.0057680028677\n",
      "Average loss at step 24600 for last 100 steps: 0.00492328315973\n",
      "Average loss at step 24700 for last 100 steps: 0.00647594094276\n",
      "Average loss at step 24800 for last 100 steps: 0.00561743080616\n",
      "Average loss at step 24900 for last 100 steps: 0.00554462492466\n",
      "\n",
      " EPOCH 2\n",
      "Average loss at step 100 for last 100 steps: 0.00629475176334\n",
      "Average loss at step 200 for last 100 steps: 0.00581509232521\n",
      "Average loss at step 300 for last 100 steps: 0.00600772559643\n",
      "Average loss at step 400 for last 100 steps: 0.00651048362255\n",
      "Average loss at step 500 for last 100 steps: 0.00621763288975\n",
      "Average loss at step 600 for last 100 steps: 0.00620119512081\n",
      "Average loss at step 700 for last 100 steps: 0.00591621518135\n",
      "Average loss at step 800 for last 100 steps: 0.00568333685398\n",
      "Average loss at step 900 for last 100 steps: 0.00648653864861\n",
      "Average loss at step 1000 for last 100 steps: 0.00524446129799\n",
      "Average loss at step 1100 for last 100 steps: 0.0059477519989\n",
      "Average loss at step 1200 for last 100 steps: 0.00617480814457\n",
      "Average loss at step 1300 for last 100 steps: 0.00608802497387\n",
      "Average loss at step 1400 for last 100 steps: 0.00568727552891\n",
      "Average loss at step 1500 for last 100 steps: 0.00535761475563\n",
      "Average loss at step 1600 for last 100 steps: 0.0060199201107\n",
      "Average loss at step 1700 for last 100 steps: 0.00619143128395\n",
      "Average loss at step 1800 for last 100 steps: 0.00591585934162\n",
      "Average loss at step 1900 for last 100 steps: 0.00615215659142\n",
      "Average loss at step 2000 for last 100 steps: 0.0058947122097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 2100 for last 100 steps: 0.00670546531677\n",
      "Average loss at step 2200 for last 100 steps: 0.00678084075451\n",
      "Average loss at step 2300 for last 100 steps: 0.00503089487553\n",
      "Average loss at step 2400 for last 100 steps: 0.00646460533142\n",
      "Average loss at step 2500 for last 100 steps: 0.0053579223156\n",
      "Average loss at step 2600 for last 100 steps: 0.00563588142395\n",
      "Average loss at step 2700 for last 100 steps: 0.00576939284801\n",
      "Average loss at step 2800 for last 100 steps: 0.00592771351337\n",
      "Average loss at step 2900 for last 100 steps: 0.00562668025494\n",
      "Average loss at step 3000 for last 100 steps: 0.00614705145359\n",
      "Average loss at step 3100 for last 100 steps: 0.00595337212086\n",
      "Average loss at step 3200 for last 100 steps: 0.0057149285078\n",
      "Average loss at step 3300 for last 100 steps: 0.00626499474049\n",
      "Average loss at step 3400 for last 100 steps: 0.00631878495216\n",
      "Average loss at step 3500 for last 100 steps: 0.00602763533592\n",
      "Average loss at step 3600 for last 100 steps: 0.00573796153069\n",
      "Average loss at step 3700 for last 100 steps: 0.00631482481956\n",
      "Average loss at step 3800 for last 100 steps: 0.00550880670547\n",
      "Average loss at step 3900 for last 100 steps: 0.00611858725548\n",
      "Average loss at step 4000 for last 100 steps: 0.00502649903297\n",
      "Average loss at step 4100 for last 100 steps: 0.00681173682213\n",
      "Average loss at step 4200 for last 100 steps: 0.00590472817421\n",
      "Average loss at step 4300 for last 100 steps: 0.00530866503716\n",
      "Average loss at step 4400 for last 100 steps: 0.00595699191093\n",
      "Average loss at step 4500 for last 100 steps: 0.00576364159584\n",
      "Average loss at step 4600 for last 100 steps: 0.00608316540718\n",
      "Average loss at step 4700 for last 100 steps: 0.00598959088326\n",
      "Average loss at step 4800 for last 100 steps: 0.00588065326214\n",
      "Average loss at step 4900 for last 100 steps: 0.00636793017387\n",
      "Average loss at step 5000 for last 100 steps: 0.00599127650261\n",
      "Average loss at step 5100 for last 100 steps: 0.00604514837265\n",
      "Average loss at step 5200 for last 100 steps: 0.00559626996517\n",
      "Average loss at step 5300 for last 100 steps: 0.00563953876495\n",
      "Average loss at step 5400 for last 100 steps: 0.00564499676228\n",
      "Average loss at step 5500 for last 100 steps: 0.00601051867008\n",
      "Average loss at step 5600 for last 100 steps: 0.00707324624062\n",
      "Average loss at step 5700 for last 100 steps: 0.00554299175739\n",
      "Average loss at step 5800 for last 100 steps: 0.00565129578114\n",
      "Average loss at step 5900 for last 100 steps: 0.00602715790272\n",
      "Average loss at step 6000 for last 100 steps: 0.00606398403645\n",
      "Average loss at step 6100 for last 100 steps: 0.00551696240902\n",
      "Average loss at step 6200 for last 100 steps: 0.00582583665848\n",
      "Average loss at step 6300 for last 100 steps: 0.0056193959713\n",
      "Average loss at step 6400 for last 100 steps: 0.00572860181332\n",
      "Average loss at step 6500 for last 100 steps: 0.00621657848358\n",
      "Average loss at step 6600 for last 100 steps: 0.0064819842577\n",
      "Average loss at step 6700 for last 100 steps: 0.00606422543526\n",
      "Average loss at step 6800 for last 100 steps: 0.00655417621136\n",
      "Average loss at step 6900 for last 100 steps: 0.00677144050598\n",
      "Average loss at step 7000 for last 100 steps: 0.00634018659592\n",
      "Average loss at step 7100 for last 100 steps: 0.0054179251194\n",
      "Average loss at step 7200 for last 100 steps: 0.00616759598255\n",
      "Average loss at step 7300 for last 100 steps: 0.00609153866768\n",
      "Average loss at step 7400 for last 100 steps: 0.00632985472679\n",
      "Average loss at step 7500 for last 100 steps: 0.00559598922729\n",
      "Average loss at step 7600 for last 100 steps: 0.00634982705116\n",
      "Average loss at step 7700 for last 100 steps: 0.0061122405529\n",
      "Average loss at step 7800 for last 100 steps: 0.00623653769493\n",
      "Average loss at step 7900 for last 100 steps: 0.0066788816452\n",
      "Average loss at step 8000 for last 100 steps: 0.00633252322674\n",
      "Average loss at step 8100 for last 100 steps: 0.00602388978004\n",
      "Average loss at step 8200 for last 100 steps: 0.00733829855919\n",
      "Average loss at step 8300 for last 100 steps: 0.00607760727406\n",
      "Average loss at step 8400 for last 100 steps: 0.00558329761028\n",
      "Average loss at step 8500 for last 100 steps: 0.00612858295441\n",
      "Average loss at step 8600 for last 100 steps: 0.00658678174019\n",
      "Average loss at step 8700 for last 100 steps: 0.0061735022068\n",
      "Average loss at step 8800 for last 100 steps: 0.00570384681225\n",
      "Average loss at step 8900 for last 100 steps: 0.00588996767998\n",
      "Average loss at step 9000 for last 100 steps: 0.00609445929527\n",
      "Average loss at step 9100 for last 100 steps: 0.00556548297405\n",
      "Average loss at step 9200 for last 100 steps: 0.00568255066872\n",
      "Average loss at step 9300 for last 100 steps: 0.00580183684826\n",
      "Average loss at step 9400 for last 100 steps: 0.0055666577816\n",
      "Average loss at step 9500 for last 100 steps: 0.00541795432568\n",
      "Average loss at step 9600 for last 100 steps: 0.00656981885433\n",
      "Average loss at step 9700 for last 100 steps: 0.00520281016827\n",
      "Average loss at step 9800 for last 100 steps: 0.00501785635948\n",
      "Average loss at step 9900 for last 100 steps: 0.00629693031311\n",
      "Average loss at step 10000 for last 100 steps: 0.00565243661404\n",
      "Average loss at step 10100 for last 100 steps: 0.00637634754181\n",
      "Average loss at step 10200 for last 100 steps: 0.00540635108948\n",
      "Average loss at step 10300 for last 100 steps: 0.00608915686607\n",
      "Average loss at step 10400 for last 100 steps: 0.00567771553993\n",
      "Average loss at step 10500 for last 100 steps: 0.0059850358963\n",
      "Average loss at step 10600 for last 100 steps: 0.00632891058922\n",
      "Average loss at step 10700 for last 100 steps: 0.00597467601299\n",
      "Average loss at step 10800 for last 100 steps: 0.00604415655136\n",
      "Average loss at step 10900 for last 100 steps: 0.00619713187218\n",
      "Average loss at step 11000 for last 100 steps: 0.00610346138477\n",
      "Average loss at step 11100 for last 100 steps: 0.00554156601429\n",
      "Average loss at step 11200 for last 100 steps: 0.00570615887642\n",
      "Average loss at step 11300 for last 100 steps: 0.00595265269279\n",
      "Average loss at step 11400 for last 100 steps: 0.00584138810635\n",
      "Average loss at step 11500 for last 100 steps: 0.00607430100441\n",
      "Average loss at step 11600 for last 100 steps: 0.00637721538544\n",
      "Average loss at step 11700 for last 100 steps: 0.00633675038815\n",
      "Average loss at step 11800 for last 100 steps: 0.00632992446423\n",
      "Average loss at step 11900 for last 100 steps: 0.00594654023647\n",
      "Average loss at step 12000 for last 100 steps: 0.00673093020916\n",
      "Average loss at step 12100 for last 100 steps: 0.00578817784786\n",
      "Average loss at step 12200 for last 100 steps: 0.00545495510101\n",
      "Average loss at step 12300 for last 100 steps: 0.00533053040504\n",
      "Average loss at step 12400 for last 100 steps: 0.00574955105782\n",
      "Average loss at step 12500 for last 100 steps: 0.00622923254967\n",
      "Average loss at step 12600 for last 100 steps: 0.00637222290039\n",
      "Average loss at step 12700 for last 100 steps: 0.00589041650295\n",
      "Average loss at step 12800 for last 100 steps: 0.00597834587097\n",
      "Average loss at step 12900 for last 100 steps: 0.00646852314472\n",
      "Average loss at step 13000 for last 100 steps: 0.00641089737415\n",
      "Average loss at step 13100 for last 100 steps: 0.00598574757576\n",
      "Average loss at step 13200 for last 100 steps: 0.00557494580746\n",
      "Average loss at step 13300 for last 100 steps: 0.00605448544025\n",
      "Average loss at step 13400 for last 100 steps: 0.00629796504974\n",
      "Average loss at step 13500 for last 100 steps: 0.00602179646492\n",
      "Average loss at step 13600 for last 100 steps: 0.00616055607796\n",
      "Average loss at step 13700 for last 100 steps: 0.00588588237762\n",
      "Average loss at step 13800 for last 100 steps: 0.00529588878155\n",
      "Average loss at step 13900 for last 100 steps: 0.00577620327473\n",
      "Average loss at step 14000 for last 100 steps: 0.00640202403069\n",
      "Average loss at step 14100 for last 100 steps: 0.00571074128151\n",
      "Average loss at step 14200 for last 100 steps: 0.00640920877457\n",
      "Average loss at step 14300 for last 100 steps: 0.00553355872631\n",
      "Average loss at step 14400 for last 100 steps: 0.00561078548431\n",
      "Average loss at step 14500 for last 100 steps: 0.00634717941284\n",
      "Average loss at step 14600 for last 100 steps: 0.00618377447128\n",
      "Average loss at step 14700 for last 100 steps: 0.00547059237957\n",
      "Average loss at step 14800 for last 100 steps: 0.00588310837746\n",
      "Average loss at step 14900 for last 100 steps: 0.00608659863472\n",
      "Average loss at step 15000 for last 100 steps: 0.00655248343945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 15100 for last 100 steps: 0.0051387912035\n",
      "Average loss at step 15200 for last 100 steps: 0.0055816757679\n",
      "Average loss at step 15300 for last 100 steps: 0.00594570994377\n",
      "Average loss at step 15400 for last 100 steps: 0.00625442743301\n",
      "Average loss at step 15500 for last 100 steps: 0.00630955576897\n",
      "Average loss at step 15600 for last 100 steps: 0.00565168738365\n",
      "Average loss at step 15700 for last 100 steps: 0.00593162000179\n",
      "Average loss at step 15800 for last 100 steps: 0.00641726016998\n",
      "Average loss at step 15900 for last 100 steps: 0.00574580252171\n",
      "Average loss at step 16000 for last 100 steps: 0.00613997936249\n",
      "Average loss at step 16100 for last 100 steps: 0.00527814507484\n",
      "Average loss at step 16200 for last 100 steps: 0.00682639718056\n",
      "Average loss at step 16300 for last 100 steps: 0.006256275177\n",
      "Average loss at step 16400 for last 100 steps: 0.00647194623947\n",
      "Average loss at step 16500 for last 100 steps: 0.00568771243095\n",
      "Average loss at step 16600 for last 100 steps: 0.00546768963337\n",
      "Average loss at step 16700 for last 100 steps: 0.00569613456726\n",
      "Average loss at step 16800 for last 100 steps: 0.00590432465076\n",
      "Average loss at step 16900 for last 100 steps: 0.00576446175575\n",
      "Average loss at step 17000 for last 100 steps: 0.00620901942253\n",
      "Average loss at step 17100 for last 100 steps: 0.00530234873295\n",
      "Average loss at step 17200 for last 100 steps: 0.00631967067719\n",
      "Average loss at step 17300 for last 100 steps: 0.00599947333336\n",
      "Average loss at step 17400 for last 100 steps: 0.00580134928226\n",
      "Average loss at step 17500 for last 100 steps: 0.00567803382874\n",
      "Average loss at step 17600 for last 100 steps: 0.00560535311699\n",
      "Average loss at step 17700 for last 100 steps: 0.00595506310463\n",
      "Average loss at step 17800 for last 100 steps: 0.0053046643734\n",
      "Average loss at step 17900 for last 100 steps: 0.00594898998737\n",
      "Average loss at step 18000 for last 100 steps: 0.00659062564373\n",
      "Average loss at step 18100 for last 100 steps: 0.00638323485851\n",
      "Average loss at step 18200 for last 100 steps: 0.00610549330711\n",
      "Average loss at step 18300 for last 100 steps: 0.00568007290363\n",
      "Average loss at step 18400 for last 100 steps: 0.00689485430717\n",
      "Average loss at step 18500 for last 100 steps: 0.0063682103157\n",
      "Average loss at step 18600 for last 100 steps: 0.00598859429359\n",
      "Average loss at step 18700 for last 100 steps: 0.00587407231331\n",
      "Average loss at step 18800 for last 100 steps: 0.00478702843189\n",
      "Average loss at step 18900 for last 100 steps: 0.00660193741322\n",
      "Average loss at step 19000 for last 100 steps: 0.00609400570393\n",
      "Average loss at step 19100 for last 100 steps: 0.00649460792542\n",
      "Average loss at step 19200 for last 100 steps: 0.00568303227425\n",
      "Average loss at step 19300 for last 100 steps: 0.00609403252602\n",
      "Average loss at step 19400 for last 100 steps: 0.0057139647007\n",
      "Average loss at step 19500 for last 100 steps: 0.00641171693802\n",
      "Average loss at step 19600 for last 100 steps: 0.0067965221405\n",
      "Average loss at step 19700 for last 100 steps: 0.00612201571465\n",
      "Average loss at step 19800 for last 100 steps: 0.00576704978943\n",
      "Average loss at step 19900 for last 100 steps: 0.00574901401997\n",
      "Average loss at step 20000 for last 100 steps: 0.00576455295086\n",
      "Average loss at step 20100 for last 100 steps: 0.00634378790855\n",
      "Average loss at step 20200 for last 100 steps: 0.0057968711853\n",
      "Average loss at step 20300 for last 100 steps: 0.0055013024807\n",
      "Average loss at step 20400 for last 100 steps: 0.00586238443851\n",
      "Average loss at step 20500 for last 100 steps: 0.00533862471581\n",
      "Average loss at step 20600 for last 100 steps: 0.00635976910591\n",
      "Average loss at step 20700 for last 100 steps: 0.00611094117165\n",
      "Average loss at step 20800 for last 100 steps: 0.00661724448204\n",
      "Average loss at step 20900 for last 100 steps: 0.0056215685606\n",
      "Average loss at step 21000 for last 100 steps: 0.00603603243828\n",
      "Average loss at step 21100 for last 100 steps: 0.00652101159096\n",
      "Average loss at step 21200 for last 100 steps: 0.00595052242279\n",
      "Average loss at step 21300 for last 100 steps: 0.00607653617859\n",
      "Average loss at step 21400 for last 100 steps: 0.0059422647953\n",
      "Average loss at step 21500 for last 100 steps: 0.00654743552208\n",
      "Average loss at step 21600 for last 100 steps: 0.0061699783802\n",
      "Average loss at step 21700 for last 100 steps: 0.00673579037189\n",
      "Average loss at step 21800 for last 100 steps: 0.00567232489586\n",
      "Average loss at step 21900 for last 100 steps: 0.00643646597862\n",
      "Average loss at step 22000 for last 100 steps: 0.0067787694931\n",
      "Average loss at step 22100 for last 100 steps: 0.00632571578026\n",
      "Average loss at step 22200 for last 100 steps: 0.00555323302746\n",
      "Average loss at step 22300 for last 100 steps: 0.006526876688\n",
      "Average loss at step 22400 for last 100 steps: 0.00508661031723\n",
      "Average loss at step 22500 for last 100 steps: 0.00595746636391\n",
      "Average loss at step 22600 for last 100 steps: 0.00626654982567\n",
      "Average loss at step 22700 for last 100 steps: 0.00590404689312\n",
      "Average loss at step 22800 for last 100 steps: 0.00575308680534\n",
      "Average loss at step 22900 for last 100 steps: 0.00486929506063\n",
      "Average loss at step 23000 for last 100 steps: 0.0055168390274\n",
      "Average loss at step 23100 for last 100 steps: 0.00566829621792\n",
      "Average loss at step 23200 for last 100 steps: 0.00561201393604\n",
      "Average loss at step 23300 for last 100 steps: 0.00590188026428\n",
      "Average loss at step 23400 for last 100 steps: 0.00613681018353\n",
      "Average loss at step 23500 for last 100 steps: 0.00545756936073\n",
      "Average loss at step 23600 for last 100 steps: 0.00601039946079\n",
      "Average loss at step 23700 for last 100 steps: 0.00660993218422\n",
      "Average loss at step 23800 for last 100 steps: 0.00487936884165\n",
      "Average loss at step 23900 for last 100 steps: 0.00688688457012\n",
      "Average loss at step 24000 for last 100 steps: 0.00530878603458\n",
      "Average loss at step 24100 for last 100 steps: 0.00558871924877\n",
      "Average loss at step 24200 for last 100 steps: 0.00628594756126\n",
      "Average loss at step 24300 for last 100 steps: 0.00688921451569\n",
      "Average loss at step 24400 for last 100 steps: 0.00636389374733\n",
      "Average loss at step 24500 for last 100 steps: 0.00587353348732\n",
      "Average loss at step 24600 for last 100 steps: 0.00602387309074\n",
      "Average loss at step 24700 for last 100 steps: 0.00643182456493\n",
      "Average loss at step 24800 for last 100 steps: 0.00605838179588\n",
      "Average loss at step 24900 for last 100 steps: 0.00550313830376\n",
      "\n",
      " EPOCH 3\n",
      "Average loss at step 100 for last 100 steps: 0.0063392239809\n",
      "Average loss at step 200 for last 100 steps: 0.00622612655163\n",
      "Average loss at step 300 for last 100 steps: 0.00640254735947\n",
      "Average loss at step 400 for last 100 steps: 0.00602915585041\n",
      "Average loss at step 500 for last 100 steps: 0.00533440470695\n",
      "Average loss at step 600 for last 100 steps: 0.00612328827381\n",
      "Average loss at step 700 for last 100 steps: 0.00628714382648\n",
      "Average loss at step 800 for last 100 steps: 0.00594029426575\n",
      "Average loss at step 900 for last 100 steps: 0.00570628583431\n",
      "Average loss at step 1000 for last 100 steps: 0.00632873415947\n",
      "Average loss at step 1100 for last 100 steps: 0.00591467261314\n",
      "Average loss at step 1200 for last 100 steps: 0.00612244844437\n",
      "Average loss at step 1300 for last 100 steps: 0.0057707041502\n",
      "Average loss at step 1400 for last 100 steps: 0.00607119202614\n",
      "Average loss at step 1500 for last 100 steps: 0.00609804213047\n",
      "Average loss at step 1600 for last 100 steps: 0.00605384230614\n",
      "Average loss at step 1700 for last 100 steps: 0.00645017981529\n",
      "Average loss at step 1800 for last 100 steps: 0.00578457176685\n",
      "Average loss at step 1900 for last 100 steps: 0.00593384623528\n",
      "Average loss at step 2000 for last 100 steps: 0.00628094911575\n",
      "Average loss at step 2100 for last 100 steps: 0.00567409753799\n",
      "Average loss at step 2200 for last 100 steps: 0.00552872300148\n",
      "Average loss at step 2300 for last 100 steps: 0.00618516623974\n",
      "Average loss at step 2400 for last 100 steps: 0.00540967583656\n",
      "Average loss at step 2500 for last 100 steps: 0.00579667568207\n",
      "Average loss at step 2600 for last 100 steps: 0.0059994083643\n",
      "Average loss at step 2700 for last 100 steps: 0.00631940424442\n",
      "Average loss at step 2800 for last 100 steps: 0.00618084788322\n",
      "Average loss at step 2900 for last 100 steps: 0.00672347068787\n",
      "Average loss at step 3000 for last 100 steps: 0.00562068879604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 3100 for last 100 steps: 0.00637260437012\n",
      "Average loss at step 3200 for last 100 steps: 0.00504453897476\n",
      "Average loss at step 3300 for last 100 steps: 0.00582820951939\n",
      "Average loss at step 3400 for last 100 steps: 0.00477123200893\n",
      "Average loss at step 3500 for last 100 steps: 0.00625636398792\n",
      "Average loss at step 3600 for last 100 steps: 0.00615673542023\n",
      "Average loss at step 3700 for last 100 steps: 0.00573322594166\n",
      "Average loss at step 3800 for last 100 steps: 0.00620500802994\n",
      "Average loss at step 3900 for last 100 steps: 0.00579676628113\n",
      "Average loss at step 4000 for last 100 steps: 0.00622150301933\n",
      "Average loss at step 4100 for last 100 steps: 0.00585437595844\n",
      "Average loss at step 4200 for last 100 steps: 0.00556290507317\n",
      "Average loss at step 4300 for last 100 steps: 0.00623350679874\n",
      "Average loss at step 4400 for last 100 steps: 0.00614458084106\n",
      "Average loss at step 4500 for last 100 steps: 0.00663531422615\n",
      "Average loss at step 4600 for last 100 steps: 0.00605977654457\n",
      "Average loss at step 4700 for last 100 steps: 0.00555186092854\n",
      "Average loss at step 4800 for last 100 steps: 0.00569438457489\n",
      "Average loss at step 4900 for last 100 steps: 0.00613916039467\n",
      "Average loss at step 5000 for last 100 steps: 0.006495013237\n",
      "Average loss at step 5100 for last 100 steps: 0.00587741494179\n",
      "Average loss at step 5200 for last 100 steps: 0.00550719022751\n",
      "Average loss at step 5300 for last 100 steps: 0.005979565382\n",
      "Average loss at step 5400 for last 100 steps: 0.00587142765522\n",
      "Average loss at step 5500 for last 100 steps: 0.00627555429935\n",
      "Average loss at step 5600 for last 100 steps: 0.00570376873016\n",
      "Average loss at step 5700 for last 100 steps: 0.00569717466831\n",
      "Average loss at step 5800 for last 100 steps: 0.00592397511005\n",
      "Average loss at step 5900 for last 100 steps: 0.00556457459927\n",
      "Average loss at step 6000 for last 100 steps: 0.00590758919716\n",
      "Average loss at step 6100 for last 100 steps: 0.00652562022209\n",
      "Average loss at step 6200 for last 100 steps: 0.00600785136223\n",
      "Average loss at step 6300 for last 100 steps: 0.00656503558159\n",
      "Average loss at step 6400 for last 100 steps: 0.00593336939812\n",
      "Average loss at step 6500 for last 100 steps: 0.00623823046684\n",
      "Average loss at step 6600 for last 100 steps: 0.00585532963276\n",
      "Average loss at step 6700 for last 100 steps: 0.00621276080608\n",
      "Average loss at step 6800 for last 100 steps: 0.0061459338665\n",
      "Average loss at step 6900 for last 100 steps: 0.00677956402302\n",
      "Average loss at step 7000 for last 100 steps: 0.00571941971779\n",
      "Average loss at step 7100 for last 100 steps: 0.00592525362968\n",
      "Average loss at step 7200 for last 100 steps: 0.00687794208527\n",
      "Average loss at step 7300 for last 100 steps: 0.00588343501091\n",
      "Average loss at step 7400 for last 100 steps: 0.00558083176613\n",
      "Average loss at step 7500 for last 100 steps: 0.00572286605835\n",
      "Average loss at step 7600 for last 100 steps: 0.00522091567516\n",
      "Average loss at step 7700 for last 100 steps: 0.00609445869923\n",
      "Average loss at step 7800 for last 100 steps: 0.00597823381424\n",
      "Average loss at step 7900 for last 100 steps: 0.0054205840826\n",
      "Average loss at step 8000 for last 100 steps: 0.00602796554565\n",
      "Average loss at step 8100 for last 100 steps: 0.00593701660633\n",
      "Average loss at step 8200 for last 100 steps: 0.00616058290005\n",
      "Average loss at step 8300 for last 100 steps: 0.00590146899223\n",
      "Average loss at step 8400 for last 100 steps: 0.00638100504875\n",
      "Average loss at step 8500 for last 100 steps: 0.00630754470825\n",
      "Average loss at step 8600 for last 100 steps: 0.00576487600803\n",
      "Average loss at step 8700 for last 100 steps: 0.00624378681183\n",
      "Average loss at step 8800 for last 100 steps: 0.00578915953636\n",
      "Average loss at step 8900 for last 100 steps: 0.00569982349873\n",
      "Average loss at step 9000 for last 100 steps: 0.00634110271931\n",
      "Average loss at step 9100 for last 100 steps: 0.00595417320728\n",
      "Average loss at step 9200 for last 100 steps: 0.00550957679749\n",
      "Average loss at step 9300 for last 100 steps: 0.0059460657835\n",
      "Average loss at step 9400 for last 100 steps: 0.00542241930962\n",
      "Average loss at step 9500 for last 100 steps: 0.00620699286461\n",
      "Average loss at step 9600 for last 100 steps: 0.00563771307468\n",
      "Average loss at step 9700 for last 100 steps: 0.00591310441494\n",
      "Average loss at step 9800 for last 100 steps: 0.00523865818977\n",
      "Average loss at step 9900 for last 100 steps: 0.00599663734436\n",
      "Average loss at step 10000 for last 100 steps: 0.00626768887043\n",
      "Average loss at step 10100 for last 100 steps: 0.00552816808224\n",
      "Average loss at step 10200 for last 100 steps: 0.00540376782417\n",
      "Average loss at step 10300 for last 100 steps: 0.00604447722435\n",
      "Average loss at step 10400 for last 100 steps: 0.00640091240406\n",
      "Average loss at step 10500 for last 100 steps: 0.00628555774689\n",
      "Average loss at step 10600 for last 100 steps: 0.00532641291618\n",
      "Average loss at step 10700 for last 100 steps: 0.00562696754932\n",
      "Average loss at step 10800 for last 100 steps: 0.00557325959206\n",
      "Average loss at step 10900 for last 100 steps: 0.00656234622002\n",
      "Average loss at step 11000 for last 100 steps: 0.00644236922264\n",
      "Average loss at step 11100 for last 100 steps: 0.00625208079815\n",
      "Average loss at step 11200 for last 100 steps: 0.00573766946793\n",
      "Average loss at step 11300 for last 100 steps: 0.00602509140968\n",
      "Average loss at step 11400 for last 100 steps: 0.00644796490669\n",
      "Average loss at step 11500 for last 100 steps: 0.00562894642353\n",
      "Average loss at step 11600 for last 100 steps: 0.00630412340164\n",
      "Average loss at step 11700 for last 100 steps: 0.00581378817558\n",
      "Average loss at step 11800 for last 100 steps: 0.00576108574867\n",
      "Average loss at step 11900 for last 100 steps: 0.0060203230381\n",
      "Average loss at step 12000 for last 100 steps: 0.00613277673721\n",
      "Average loss at step 12100 for last 100 steps: 0.00592342078686\n",
      "Average loss at step 12200 for last 100 steps: 0.00583319842815\n",
      "Average loss at step 12300 for last 100 steps: 0.00646876692772\n",
      "Average loss at step 12400 for last 100 steps: 0.00655579388142\n",
      "Average loss at step 12500 for last 100 steps: 0.00636032283306\n",
      "Average loss at step 12600 for last 100 steps: 0.00587777495384\n",
      "Average loss at step 12700 for last 100 steps: 0.00605499267578\n",
      "Average loss at step 12800 for last 100 steps: 0.00648006141186\n",
      "Average loss at step 12900 for last 100 steps: 0.0059504109621\n",
      "Average loss at step 13000 for last 100 steps: 0.00582934200764\n",
      "Average loss at step 13100 for last 100 steps: 0.00644476771355\n",
      "Average loss at step 13200 for last 100 steps: 0.00569707274437\n",
      "Average loss at step 13300 for last 100 steps: 0.00600057721138\n",
      "Average loss at step 13400 for last 100 steps: 0.00574954152107\n",
      "Average loss at step 13500 for last 100 steps: 0.00611677289009\n",
      "Average loss at step 13600 for last 100 steps: 0.00610856831074\n",
      "Average loss at step 13700 for last 100 steps: 0.00688118755817\n",
      "Average loss at step 13800 for last 100 steps: 0.00595301151276\n",
      "Average loss at step 13900 for last 100 steps: 0.00559059917927\n",
      "Average loss at step 14000 for last 100 steps: 0.005691395998\n",
      "Average loss at step 14100 for last 100 steps: 0.00680758178234\n",
      "Average loss at step 14200 for last 100 steps: 0.00692801117897\n",
      "Average loss at step 14300 for last 100 steps: 0.00560489714146\n",
      "Average loss at step 14400 for last 100 steps: 0.00610413491726\n",
      "Average loss at step 14500 for last 100 steps: 0.00659849584103\n",
      "Average loss at step 14600 for last 100 steps: 0.0067080527544\n",
      "Average loss at step 14700 for last 100 steps: 0.0055916762352\n",
      "Average loss at step 14800 for last 100 steps: 0.00625757217407\n",
      "Average loss at step 14900 for last 100 steps: 0.00638304650784\n",
      "Average loss at step 15000 for last 100 steps: 0.00590331673622\n",
      "Average loss at step 15100 for last 100 steps: 0.00536339819431\n",
      "Average loss at step 15200 for last 100 steps: 0.0056044113636\n",
      "Average loss at step 15300 for last 100 steps: 0.00636186003685\n",
      "Average loss at step 15400 for last 100 steps: 0.00595844984055\n",
      "Average loss at step 15500 for last 100 steps: 0.0051308375597\n",
      "Average loss at step 15600 for last 100 steps: 0.00604704618454\n",
      "Average loss at step 15700 for last 100 steps: 0.00615020751953\n",
      "Average loss at step 15800 for last 100 steps: 0.00523579776287\n",
      "Average loss at step 15900 for last 100 steps: 0.00618772864342\n",
      "Average loss at step 16000 for last 100 steps: 0.0063915348053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 16100 for last 100 steps: 0.00630837023258\n",
      "Average loss at step 16200 for last 100 steps: 0.00658595442772\n",
      "Average loss at step 16300 for last 100 steps: 0.00594988167286\n",
      "Average loss at step 16400 for last 100 steps: 0.0057230836153\n",
      "Average loss at step 16500 for last 100 steps: 0.00566336929798\n",
      "Average loss at step 16600 for last 100 steps: 0.00576584637165\n",
      "Average loss at step 16700 for last 100 steps: 0.00590969204903\n",
      "Average loss at step 16800 for last 100 steps: 0.00569022536278\n",
      "Average loss at step 16900 for last 100 steps: 0.005865239501\n",
      "Average loss at step 17000 for last 100 steps: 0.00560348510742\n",
      "Average loss at step 17100 for last 100 steps: 0.00626504123211\n",
      "Average loss at step 17200 for last 100 steps: 0.00611851811409\n",
      "Average loss at step 17300 for last 100 steps: 0.00551904857159\n",
      "Average loss at step 17400 for last 100 steps: 0.00599371731281\n",
      "Average loss at step 17500 for last 100 steps: 0.00498709976673\n",
      "Average loss at step 17600 for last 100 steps: 0.00561298489571\n",
      "Average loss at step 17700 for last 100 steps: 0.00578794360161\n",
      "Average loss at step 17800 for last 100 steps: 0.00598706245422\n",
      "Average loss at step 17900 for last 100 steps: 0.0060312306881\n",
      "Average loss at step 18000 for last 100 steps: 0.00590186774731\n",
      "Average loss at step 18100 for last 100 steps: 0.00536363303661\n",
      "Average loss at step 18200 for last 100 steps: 0.00624838650227\n",
      "Average loss at step 18300 for last 100 steps: 0.00565469861031\n",
      "Average loss at step 18400 for last 100 steps: 0.00631281077862\n",
      "Average loss at step 18500 for last 100 steps: 0.00543365955353\n",
      "Average loss at step 18600 for last 100 steps: 0.00660810768604\n",
      "Average loss at step 18700 for last 100 steps: 0.00538656115532\n",
      "Average loss at step 18800 for last 100 steps: 0.00592704594135\n",
      "Average loss at step 18900 for last 100 steps: 0.00652275383472\n",
      "Average loss at step 19000 for last 100 steps: 0.00582015156746\n",
      "Average loss at step 19100 for last 100 steps: 0.00576152443886\n",
      "Average loss at step 19200 for last 100 steps: 0.00611431241035\n",
      "Average loss at step 19300 for last 100 steps: 0.00554875969887\n",
      "Average loss at step 19400 for last 100 steps: 0.00537800490856\n",
      "Average loss at step 19500 for last 100 steps: 0.00562267422676\n",
      "Average loss at step 19600 for last 100 steps: 0.00661056399345\n",
      "Average loss at step 19700 for last 100 steps: 0.0056074154377\n",
      "Average loss at step 19800 for last 100 steps: 0.00671803176403\n",
      "Average loss at step 19900 for last 100 steps: 0.00630410313606\n",
      "Average loss at step 20000 for last 100 steps: 0.00545000433922\n",
      "Average loss at step 20100 for last 100 steps: 0.00602297782898\n",
      "Average loss at step 20200 for last 100 steps: 0.00570524811745\n",
      "Average loss at step 20300 for last 100 steps: 0.00599377453327\n",
      "Average loss at step 20400 for last 100 steps: 0.00633469879627\n",
      "Average loss at step 20500 for last 100 steps: 0.00539741396904\n",
      "Average loss at step 20600 for last 100 steps: 0.00533821463585\n",
      "Average loss at step 20700 for last 100 steps: 0.00589971423149\n",
      "Average loss at step 20800 for last 100 steps: 0.00685351729393\n",
      "Average loss at step 20900 for last 100 steps: 0.00554353177547\n",
      "Average loss at step 21000 for last 100 steps: 0.0063744789362\n",
      "Average loss at step 21100 for last 100 steps: 0.00557236194611\n",
      "Average loss at step 21200 for last 100 steps: 0.00651228606701\n",
      "Average loss at step 21300 for last 100 steps: 0.00578358948231\n",
      "Average loss at step 21400 for last 100 steps: 0.00610774695873\n",
      "Average loss at step 21500 for last 100 steps: 0.00786532282829\n",
      "Average loss at step 21600 for last 100 steps: 0.00588800132275\n",
      "Average loss at step 21700 for last 100 steps: 0.00584086596966\n",
      "Average loss at step 21800 for last 100 steps: 0.00607620596886\n",
      "Average loss at step 21900 for last 100 steps: 0.00645321071148\n",
      "Average loss at step 22000 for last 100 steps: 0.00624004721642\n",
      "Average loss at step 22100 for last 100 steps: 0.00628667533398\n",
      "Average loss at step 22200 for last 100 steps: 0.00594487190247\n",
      "Average loss at step 22300 for last 100 steps: 0.00621272563934\n",
      "Average loss at step 22400 for last 100 steps: 0.00558081746101\n",
      "Average loss at step 22500 for last 100 steps: 0.00556967914104\n",
      "Average loss at step 22600 for last 100 steps: 0.00609143912792\n",
      "Average loss at step 22700 for last 100 steps: 0.00496117025614\n",
      "Average loss at step 22800 for last 100 steps: 0.00580692768097\n",
      "Average loss at step 22900 for last 100 steps: 0.00589682757854\n",
      "Average loss at step 23000 for last 100 steps: 0.00553540349007\n",
      "Average loss at step 23100 for last 100 steps: 0.00623043775558\n",
      "Average loss at step 23200 for last 100 steps: 0.00511652469635\n",
      "Average loss at step 23300 for last 100 steps: 0.00649606406689\n",
      "Average loss at step 23400 for last 100 steps: 0.00663197398186\n",
      "Average loss at step 23500 for last 100 steps: 0.00659994721413\n",
      "Average loss at step 23600 for last 100 steps: 0.00680387556553\n",
      "Average loss at step 23700 for last 100 steps: 0.00594331145287\n",
      "Average loss at step 23800 for last 100 steps: 0.00589874446392\n",
      "Average loss at step 23900 for last 100 steps: 0.00603400588036\n",
      "Average loss at step 24000 for last 100 steps: 0.00524566531181\n",
      "Average loss at step 24100 for last 100 steps: 0.00578875422478\n",
      "Average loss at step 24200 for last 100 steps: 0.00565299153328\n",
      "Average loss at step 24300 for last 100 steps: 0.00529588520527\n",
      "Average loss at step 24400 for last 100 steps: 0.00613910198212\n",
      "Average loss at step 24500 for last 100 steps: 0.00524838268757\n",
      "Average loss at step 24600 for last 100 steps: 0.00531095981598\n",
      "Average loss at step 24700 for last 100 steps: 0.00567341208458\n",
      "Average loss at step 24800 for last 100 steps: 0.00547967553139\n",
      "Average loss at step 24900 for last 100 steps: 0.00609657466412\n",
      "\n",
      " EPOCH 4\n",
      "Average loss at step 100 for last 100 steps: 0.00616053402424\n",
      "Average loss at step 200 for last 100 steps: 0.00567992985249\n",
      "Average loss at step 300 for last 100 steps: 0.00600524008274\n",
      "Average loss at step 400 for last 100 steps: 0.00547621905804\n",
      "Average loss at step 500 for last 100 steps: 0.00584129214287\n",
      "Average loss at step 600 for last 100 steps: 0.00629019916058\n",
      "Average loss at step 700 for last 100 steps: 0.00573295235634\n",
      "Average loss at step 800 for last 100 steps: 0.0064662706852\n",
      "Average loss at step 900 for last 100 steps: 0.00615119576454\n",
      "Average loss at step 1000 for last 100 steps: 0.00683900952339\n",
      "Average loss at step 1100 for last 100 steps: 0.00567587375641\n",
      "Average loss at step 1200 for last 100 steps: 0.00547112762928\n",
      "Average loss at step 1300 for last 100 steps: 0.00630668997765\n",
      "Average loss at step 1400 for last 100 steps: 0.00644264698029\n",
      "Average loss at step 1500 for last 100 steps: 0.00572663784027\n",
      "Average loss at step 1600 for last 100 steps: 0.00641167521477\n",
      "Average loss at step 1700 for last 100 steps: 0.00591631233692\n",
      "Average loss at step 1800 for last 100 steps: 0.00602372467518\n",
      "Average loss at step 1900 for last 100 steps: 0.00575982391834\n",
      "Average loss at step 2000 for last 100 steps: 0.0061473608017\n",
      "Average loss at step 2100 for last 100 steps: 0.00591752707958\n",
      "Average loss at step 2200 for last 100 steps: 0.00659014582634\n",
      "Average loss at step 2300 for last 100 steps: 0.00605481803417\n",
      "Average loss at step 2400 for last 100 steps: 0.00603023588657\n",
      "Average loss at step 2500 for last 100 steps: 0.00622875809669\n",
      "Average loss at step 2600 for last 100 steps: 0.00597151756287\n",
      "Average loss at step 2700 for last 100 steps: 0.00639665782452\n",
      "Average loss at step 2800 for last 100 steps: 0.00589707791805\n",
      "Average loss at step 2900 for last 100 steps: 0.00617497801781\n",
      "Average loss at step 3000 for last 100 steps: 0.00657617211342\n",
      "Average loss at step 3100 for last 100 steps: 0.00651788592339\n",
      "Average loss at step 3200 for last 100 steps: 0.00579391777515\n",
      "Average loss at step 3300 for last 100 steps: 0.00611931085587\n",
      "Average loss at step 3400 for last 100 steps: 0.00689148247242\n",
      "Average loss at step 3500 for last 100 steps: 0.00585265874863\n",
      "Average loss at step 3600 for last 100 steps: 0.00614915013313\n",
      "Average loss at step 3700 for last 100 steps: 0.00635000705719\n",
      "Average loss at step 3800 for last 100 steps: 0.0055061006546\n",
      "Average loss at step 3900 for last 100 steps: 0.00614463210106\n",
      "Average loss at step 4000 for last 100 steps: 0.00566669225693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4100 for last 100 steps: 0.00646718025208\n",
      "Average loss at step 4200 for last 100 steps: 0.00669670403004\n",
      "Average loss at step 4300 for last 100 steps: 0.00634813606739\n",
      "Average loss at step 4400 for last 100 steps: 0.00601709485054\n",
      "Average loss at step 4500 for last 100 steps: 0.00563186407089\n",
      "Average loss at step 4600 for last 100 steps: 0.0061680996418\n",
      "Average loss at step 4700 for last 100 steps: 0.0060022175312\n",
      "Average loss at step 4800 for last 100 steps: 0.00597814381123\n",
      "Average loss at step 4900 for last 100 steps: 0.00663709878922\n",
      "Average loss at step 5000 for last 100 steps: 0.00642446398735\n",
      "Average loss at step 5100 for last 100 steps: 0.00544119179249\n",
      "Average loss at step 5200 for last 100 steps: 0.00624626517296\n",
      "Average loss at step 5300 for last 100 steps: 0.0066325199604\n",
      "Average loss at step 5400 for last 100 steps: 0.00611548781395\n",
      "Average loss at step 5500 for last 100 steps: 0.00567671298981\n",
      "Average loss at step 5600 for last 100 steps: 0.00598410487175\n",
      "Average loss at step 5700 for last 100 steps: 0.00629895210266\n",
      "Average loss at step 5800 for last 100 steps: 0.00614191055298\n",
      "Average loss at step 5900 for last 100 steps: 0.00539120674133\n",
      "Average loss at step 6000 for last 100 steps: 0.00589670658112\n",
      "Average loss at step 6100 for last 100 steps: 0.00570810616016\n",
      "Average loss at step 6200 for last 100 steps: 0.00673449516296\n",
      "Average loss at step 6300 for last 100 steps: 0.00489054977894\n",
      "Average loss at step 6400 for last 100 steps: 0.00577795147896\n",
      "Average loss at step 6500 for last 100 steps: 0.00580201923847\n",
      "Average loss at step 6600 for last 100 steps: 0.0064597016573\n",
      "Average loss at step 6700 for last 100 steps: 0.00626081287861\n",
      "Average loss at step 6800 for last 100 steps: 0.00569700419903\n",
      "Average loss at step 6900 for last 100 steps: 0.00577935099602\n",
      "Average loss at step 7000 for last 100 steps: 0.00647958636284\n",
      "Average loss at step 7100 for last 100 steps: 0.00597016453743\n",
      "Average loss at step 7200 for last 100 steps: 0.00565790653229\n",
      "Average loss at step 7300 for last 100 steps: 0.00593513786793\n",
      "Average loss at step 7400 for last 100 steps: 0.00597961366177\n",
      "Average loss at step 7500 for last 100 steps: 0.0064831918478\n",
      "Average loss at step 7600 for last 100 steps: 0.00581897199154\n",
      "Average loss at step 7700 for last 100 steps: 0.00576471030712\n",
      "Average loss at step 7800 for last 100 steps: 0.00588116526604\n",
      "Average loss at step 7900 for last 100 steps: 0.00654885947704\n",
      "Average loss at step 8000 for last 100 steps: 0.005870693326\n",
      "Average loss at step 8100 for last 100 steps: 0.00664241552353\n",
      "Average loss at step 8200 for last 100 steps: 0.00638222336769\n",
      "Average loss at step 8300 for last 100 steps: 0.00633715391159\n",
      "Average loss at step 8400 for last 100 steps: 0.00598172843456\n",
      "Average loss at step 8500 for last 100 steps: 0.00591830909252\n",
      "Average loss at step 8600 for last 100 steps: 0.00606360077858\n",
      "Average loss at step 8700 for last 100 steps: 0.00583018243313\n",
      "Average loss at step 8800 for last 100 steps: 0.00604264438152\n",
      "Average loss at step 8900 for last 100 steps: 0.00603817939758\n",
      "Average loss at step 9000 for last 100 steps: 0.00601785123348\n",
      "Average loss at step 9100 for last 100 steps: 0.00538747608662\n",
      "Average loss at step 9200 for last 100 steps: 0.00606475234032\n",
      "Average loss at step 9300 for last 100 steps: 0.00640396296978\n",
      "Average loss at step 9400 for last 100 steps: 0.0055728828907\n",
      "Average loss at step 9500 for last 100 steps: 0.00568157017231\n",
      "Average loss at step 9600 for last 100 steps: 0.00615635871887\n",
      "Average loss at step 9700 for last 100 steps: 0.00581836342812\n",
      "Average loss at step 9800 for last 100 steps: 0.0059396469593\n",
      "Average loss at step 9900 for last 100 steps: 0.00591795444489\n",
      "Average loss at step 10000 for last 100 steps: 0.00511319041252\n",
      "Average loss at step 10100 for last 100 steps: 0.00551661491394\n",
      "Average loss at step 10200 for last 100 steps: 0.00604995310307\n",
      "Average loss at step 10300 for last 100 steps: 0.00591981649399\n",
      "Average loss at step 10400 for last 100 steps: 0.00552838206291\n",
      "Average loss at step 10500 for last 100 steps: 0.00640023231506\n",
      "Average loss at step 10600 for last 100 steps: 0.0056786262989\n",
      "Average loss at step 10700 for last 100 steps: 0.00537652492523\n",
      "Average loss at step 10800 for last 100 steps: 0.00605681240559\n",
      "Average loss at step 10900 for last 100 steps: 0.00607659459114\n",
      "Average loss at step 11000 for last 100 steps: 0.00681137979031\n",
      "Average loss at step 11100 for last 100 steps: 0.00640909314156\n",
      "Average loss at step 11200 for last 100 steps: 0.00640293598175\n",
      "Average loss at step 11300 for last 100 steps: 0.00621426701546\n",
      "Average loss at step 11400 for last 100 steps: 0.00571398258209\n",
      "Average loss at step 11500 for last 100 steps: 0.00526592373848\n",
      "Average loss at step 11600 for last 100 steps: 0.00681563079357\n",
      "Average loss at step 11700 for last 100 steps: 0.0054869812727\n",
      "Average loss at step 11800 for last 100 steps: 0.00595355153084\n",
      "Average loss at step 11900 for last 100 steps: 0.00589379251003\n",
      "Average loss at step 12000 for last 100 steps: 0.00555094122887\n",
      "Average loss at step 12100 for last 100 steps: 0.00597662806511\n",
      "Average loss at step 12200 for last 100 steps: 0.00547146141529\n",
      "Average loss at step 12300 for last 100 steps: 0.00557521224022\n",
      "Average loss at step 12400 for last 100 steps: 0.00578427910805\n",
      "Average loss at step 12500 for last 100 steps: 0.00663238883018\n",
      "Average loss at step 12600 for last 100 steps: 0.00602994799614\n",
      "Average loss at step 12700 for last 100 steps: 0.00624477744102\n",
      "Average loss at step 12800 for last 100 steps: 0.00599986732006\n",
      "Average loss at step 12900 for last 100 steps: 0.00599982142448\n",
      "Average loss at step 13000 for last 100 steps: 0.00605603516102\n",
      "Average loss at step 13100 for last 100 steps: 0.00650772869587\n",
      "Average loss at step 13200 for last 100 steps: 0.00627591252327\n",
      "Average loss at step 13300 for last 100 steps: 0.0056237244606\n",
      "Average loss at step 13400 for last 100 steps: 0.00601745307446\n",
      "Average loss at step 13500 for last 100 steps: 0.00689253866673\n",
      "Average loss at step 13600 for last 100 steps: 0.0066657435894\n",
      "Average loss at step 13700 for last 100 steps: 0.00625018239021\n",
      "Average loss at step 13800 for last 100 steps: 0.00638070464134\n",
      "Average loss at step 13900 for last 100 steps: 0.00636670470238\n",
      "Average loss at step 14000 for last 100 steps: 0.00594256699085\n",
      "Average loss at step 14100 for last 100 steps: 0.00596855342388\n",
      "Average loss at step 14200 for last 100 steps: 0.00587858855724\n",
      "Average loss at step 14300 for last 100 steps: 0.00618437170982\n",
      "Average loss at step 14400 for last 100 steps: 0.00569037556648\n",
      "Average loss at step 14500 for last 100 steps: 0.00614151120186\n",
      "Average loss at step 14600 for last 100 steps: 0.00616895794868\n",
      "Average loss at step 14700 for last 100 steps: 0.00556068778038\n",
      "Average loss at step 14800 for last 100 steps: 0.00651534557343\n",
      "Average loss at step 14900 for last 100 steps: 0.00576865315437\n",
      "Average loss at step 15000 for last 100 steps: 0.00615327060223\n",
      "Average loss at step 15100 for last 100 steps: 0.00566363334656\n",
      "Average loss at step 15200 for last 100 steps: 0.00679047703743\n",
      "Average loss at step 15300 for last 100 steps: 0.00662913560867\n",
      "Average loss at step 15400 for last 100 steps: 0.00493990808725\n",
      "Average loss at step 15500 for last 100 steps: 0.00531884968281\n",
      "Average loss at step 15600 for last 100 steps: 0.00594167113304\n",
      "Average loss at step 15700 for last 100 steps: 0.0057924079895\n",
      "Average loss at step 15800 for last 100 steps: 0.00616176426411\n",
      "Average loss at step 15900 for last 100 steps: 0.00562028586864\n",
      "Average loss at step 16000 for last 100 steps: 0.00588950932026\n",
      "Average loss at step 16100 for last 100 steps: 0.00582090973854\n",
      "Average loss at step 16200 for last 100 steps: 0.00633854568005\n",
      "Average loss at step 16300 for last 100 steps: 0.00574626326561\n",
      "Average loss at step 16400 for last 100 steps: 0.00541439831257\n",
      "Average loss at step 16500 for last 100 steps: 0.00639286696911\n",
      "Average loss at step 16600 for last 100 steps: 0.00586947798729\n",
      "Average loss at step 16700 for last 100 steps: 0.00526910126209\n",
      "Average loss at step 16800 for last 100 steps: 0.00588019728661\n",
      "Average loss at step 16900 for last 100 steps: 0.00554878294468\n",
      "Average loss at step 17000 for last 100 steps: 0.00571241497993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 17100 for last 100 steps: 0.00602366328239\n",
      "Average loss at step 17200 for last 100 steps: 0.00551492214203\n",
      "Average loss at step 17300 for last 100 steps: 0.00659654319286\n",
      "Average loss at step 17400 for last 100 steps: 0.00488594055176\n",
      "Average loss at step 17500 for last 100 steps: 0.00577364325523\n",
      "Average loss at step 17600 for last 100 steps: 0.00589491426945\n",
      "Average loss at step 17700 for last 100 steps: 0.00581309974194\n",
      "Average loss at step 17800 for last 100 steps: 0.00654812037945\n",
      "Average loss at step 17900 for last 100 steps: 0.00570166885853\n",
      "Average loss at step 18000 for last 100 steps: 0.00642610669136\n",
      "Average loss at step 18100 for last 100 steps: 0.00547873497009\n",
      "Average loss at step 18200 for last 100 steps: 0.00511740922928\n",
      "Average loss at step 18300 for last 100 steps: 0.00596631348133\n",
      "Average loss at step 18400 for last 100 steps: 0.00614884734154\n",
      "Average loss at step 18500 for last 100 steps: 0.006718557477\n",
      "Average loss at step 18600 for last 100 steps: 0.00662065386772\n",
      "Average loss at step 18700 for last 100 steps: 0.00661382555962\n",
      "Average loss at step 18800 for last 100 steps: 0.00616313099861\n",
      "Average loss at step 18900 for last 100 steps: 0.0061709010601\n",
      "Average loss at step 19000 for last 100 steps: 0.0063571625948\n",
      "Average loss at step 19100 for last 100 steps: 0.00581246554852\n",
      "Average loss at step 19200 for last 100 steps: 0.00569808840752\n",
      "Average loss at step 19300 for last 100 steps: 0.00621074080467\n",
      "Average loss at step 19400 for last 100 steps: 0.00651727020741\n",
      "Average loss at step 19500 for last 100 steps: 0.00687266886234\n",
      "Average loss at step 19600 for last 100 steps: 0.00624750614166\n",
      "Average loss at step 19700 for last 100 steps: 0.00613447010517\n",
      "Average loss at step 19800 for last 100 steps: 0.00555823504925\n",
      "Average loss at step 19900 for last 100 steps: 0.00651056647301\n",
      "Average loss at step 20000 for last 100 steps: 0.00573346793652\n",
      "Average loss at step 20100 for last 100 steps: 0.00509464561939\n",
      "Average loss at step 20200 for last 100 steps: 0.00573727250099\n",
      "Average loss at step 20300 for last 100 steps: 0.00673594653606\n",
      "Average loss at step 20400 for last 100 steps: 0.00586763978004\n",
      "Average loss at step 20500 for last 100 steps: 0.0057884645462\n",
      "Average loss at step 20600 for last 100 steps: 0.00656086802483\n",
      "Average loss at step 20700 for last 100 steps: 0.00668035924435\n",
      "Average loss at step 20800 for last 100 steps: 0.00564765572548\n",
      "Average loss at step 20900 for last 100 steps: 0.00593251049519\n",
      "Average loss at step 21000 for last 100 steps: 0.00555675625801\n",
      "Average loss at step 21100 for last 100 steps: 0.00603540420532\n",
      "Average loss at step 21200 for last 100 steps: 0.00610403716564\n",
      "Average loss at step 21300 for last 100 steps: 0.00637686312199\n",
      "Average loss at step 21400 for last 100 steps: 0.00634714245796\n",
      "Average loss at step 21500 for last 100 steps: 0.00591654896736\n",
      "Average loss at step 21600 for last 100 steps: 0.00640598595142\n",
      "Average loss at step 21700 for last 100 steps: 0.00626471459866\n",
      "Average loss at step 21800 for last 100 steps: 0.00552660167217\n",
      "Average loss at step 21900 for last 100 steps: 0.00557891368866\n",
      "Average loss at step 22000 for last 100 steps: 0.0062791800499\n",
      "Average loss at step 22100 for last 100 steps: 0.00650993227959\n",
      "Average loss at step 22200 for last 100 steps: 0.00627893149853\n",
      "Average loss at step 22300 for last 100 steps: 0.00586210846901\n",
      "Average loss at step 22400 for last 100 steps: 0.00552055537701\n",
      "Average loss at step 22500 for last 100 steps: 0.00600212931633\n",
      "Average loss at step 22600 for last 100 steps: 0.00578430056572\n",
      "Average loss at step 22700 for last 100 steps: 0.00620741546154\n",
      "Average loss at step 22800 for last 100 steps: 0.00536736428738\n",
      "Average loss at step 22900 for last 100 steps: 0.0057950335741\n",
      "Average loss at step 23000 for last 100 steps: 0.00611398458481\n",
      "Average loss at step 23100 for last 100 steps: 0.00605188369751\n",
      "Average loss at step 23200 for last 100 steps: 0.00602944850922\n",
      "Average loss at step 23300 for last 100 steps: 0.00572595179081\n",
      "Average loss at step 23400 for last 100 steps: 0.00575155079365\n",
      "Average loss at step 23500 for last 100 steps: 0.00587364077568\n",
      "Average loss at step 23600 for last 100 steps: 0.00550047874451\n",
      "Average loss at step 23700 for last 100 steps: 0.00649990737438\n",
      "Average loss at step 23800 for last 100 steps: 0.0063796633482\n",
      "Average loss at step 23900 for last 100 steps: 0.00657314419746\n",
      "Average loss at step 24000 for last 100 steps: 0.00570120513439\n",
      "Average loss at step 24100 for last 100 steps: 0.00655194759369\n",
      "Average loss at step 24200 for last 100 steps: 0.00601934611797\n",
      "Average loss at step 24300 for last 100 steps: 0.00614416480064\n",
      "Average loss at step 24400 for last 100 steps: 0.00606899440289\n",
      "Average loss at step 24500 for last 100 steps: 0.00526889562607\n",
      "Average loss at step 24600 for last 100 steps: 0.00584587931633\n",
      "Average loss at step 24700 for last 100 steps: 0.00543467998505\n",
      "Average loss at step 24800 for last 100 steps: 0.00647070765495\n",
      "Average loss at step 24900 for last 100 steps: 0.00552648186684\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD6CAYAAABOIFvoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDMMwjMX/A013RtsrR0jFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "num_classes = 2\n",
    "num_steps = 10\n",
    "state_size = 4\n",
    "learning_rate = 0.2\n",
    "\n",
    "\n",
    "def gen_data(size = 1000000):\n",
    "    \"\"\"\n",
    "        生成数据:\n",
    "        输入数据X：在时间t，Xt的值有50%的概率为1，50%的概率为0；\n",
    "        输出数据Y：在实践t，Yt的值有50%的概率为1，50%的概率为0，除此之外，如果`Xt-3 == 1`，Yt为1的概率增加50%， 如果`Xt-8 == 1`，则Yt为1的概率减少25%， 如果上述两个条件同时满足，则Yt为1的概率为75%。\n",
    "    \"\"\"\n",
    "    X = np.array(np.random.choice(2,size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threhold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threhold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threhold -= 0.25\n",
    "        if np.random.rand() > threhold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X,np.array(Y)\n",
    "\n",
    "\n",
    "def gen_batch(raw_data,batch_size,num_steps):\n",
    "\n",
    "    raw_x,raw_y = raw_data\n",
    "    data_x = raw_x.reshape(-1,batch_size,num_steps)\n",
    "    data_y = raw_y.reshape(-1,batch_size,num_steps)\n",
    "    for i in range(data_x.shape[0]):\n",
    "        yield (data_x[i],data_y[i])\n",
    "\n",
    "\n",
    "def gen_epochs(n):\n",
    "    '''这里的n就是训练过程中用的epoch，即在样本规模上循环的次数'''\n",
    "    for i in range(n):\n",
    "        yield(gen_batch(gen_data(),batch_size,num_steps))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.int32,[batch_size,num_steps],name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32,[batch_size,num_steps],name='output_placeholder')\n",
    "\n",
    "init_state = tf.zeros([batch_size,state_size])\n",
    "\n",
    "rnn_inputs = tf.one_hot(x,num_classes)\n",
    "\n",
    "\"\"\"\n",
    "tf.unstack()　　\n",
    "将给定的R维张量拆分成R-1维张量\n",
    "将value根据axis分解成num个张量，返回的值是list类型，如果没有指定num则根据axis推断出！\n",
    "但是dynamic不需要unstack，直接输入[batch_size,nums_step,num_classes]输入即可\n",
    "\"\"\"\n",
    "#rnn_inputs = tf.unstack(x_one_hot,axis=1)\n",
    "\n",
    "cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "#输出时[batch_size,num_steps,state_size]\n",
    "rnn_outputs,final_state = tf.nn.dynamic_rnn(cell,rnn_inputs,initial_state=init_state)\n",
    "\n",
    "with tf.variable_scope(\"softmax\"):\n",
    "    W = tf.get_variable(\"W\",[state_size,num_classes])\n",
    "    b = tf.get_variable(\"b\",[num_classes],initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "\n",
    "logits = tf.reshape(tf.matmul(tf.reshape(rnn_outputs,[-1,state_size]),W)+b,[batch_size,num_steps,num_classes])\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=predictions)\n",
    "\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "\n",
    "def train_network(num_epochs,num_steps,state_size=4,verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for idx,epoch in enumerate(gen_epochs(num_epochs)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size,state_size))\n",
    "            if verbose:\n",
    "                print(\"\\n EPOCH\",idx)\n",
    "            for step,(X,Y) in enumerate(epoch):\n",
    "                tr_losses,training_loss_,training_state,_ ,_= sess.run([losses,total_loss,final_state,train_step,rnn_outputs],\n",
    "                                                                     feed_dict={x:X,y:Y,init_state:training_state})\n",
    "                #print(rnn_outputs.shape)\n",
    "                training_loss += training_loss_\n",
    "\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\",step,\"for last 100 steps:\",training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                training_loss = 0\n",
    "\n",
    "\n",
    "        return training_losses\n",
    "\n",
    "training_losses = train_network(5,num_steps)\n",
    "plt.plot(training_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
